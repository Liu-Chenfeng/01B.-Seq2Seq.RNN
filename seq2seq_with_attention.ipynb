{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2efa50-07f8-4d71-8eac-5101c219108e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8576635d-0a2f-436c-972b-fc953bd25fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的库    Импорт требуемых библиотек\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95c757a-3cd9-48a2-aac4-b4988357e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用 GPU 加速    Вызов GPU-ускорения\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31f553-2862-4ff2-a516-c53fd4948133",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3471441-9168-4d34-a1fe-f96e6ad4315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径    Путь к файлу\n",
    "file_path = r\"C:\\Users\\lcf14\\Desktop\\homework\\Machine_Learning_appli\\有俄语-英语对应句 - 2024-10-31.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026c0cfe-d09b-42bd-a2ac-b305ce9e7c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id_rus                                           text_rus  id_eng  \\\n",
      "0     243  Один раз в жизни я делаю хорошее дело... И оно...    3257   \n",
      "1    5409                      Давайте что-нибудь попробуем!    1276   \n",
      "2    5410                               Мне пора идти спать.    1277   \n",
      "3    5411                                    Что ты делаешь?   16492   \n",
      "4    5411                                    Что ты делаешь?  511884   \n",
      "\n",
      "                                            text_eng  \n",
      "0  For once in my life I'm doing a good deed... A...  \n",
      "1                               Let's try something.  \n",
      "2                             I have to go to sleep.  \n",
      "3                                What are you doing?  \n",
      "4                                  What do you make?  \n"
     ]
    }
   ],
   "source": [
    "# 读取文件并设置列名，忽略有问题的行    Чтение файла и установка имен столбцов, пропуская строки с ошибками\n",
    "data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"id_rus\", \"text_rus\", \"id_eng\", \"text_eng\"], on_bad_lines='skip')\n",
    "#data = data[:100]  # 仅选择前5000行数据\n",
    "\n",
    "# 输出前几行查看数据    Вывод первых нескольких строк для просмотра данных\n",
    "print(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa21425-65f6-477e-a13f-018d55733e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id_rus                                           text_rus  id_eng  \\\n",
      "0     243  Один раз в жизни я делаю хорошее дело И оно бе...    3257   \n",
      "1    5409                        Давайте чтонибудь попробуем    1276   \n",
      "2    5410                                Мне пора идти спать    1277   \n",
      "3    5411                                     Что ты делаешь   16492   \n",
      "4    5411                                     Что ты делаешь  511884   \n",
      "\n",
      "                                            text_eng  \n",
      "0  For once in my life Im doing a good deed And i...  \n",
      "1                                 Lets try something  \n",
      "2                              I have to go to sleep  \n",
      "3                                 What are you doing  \n",
      "4                                   What do you make  \n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数，用于去除文本中的标点符号    Определение функции для удаления пунктуации из текста\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))  # 使用translate方法删除标点符号    Использование метода translate для удаления пунктуации\n",
    "\n",
    "# 对俄语文本列应用去除标点符号的函数    Применение функции удаления пунктуации к столбцу с русским текстом\n",
    "data['text_rus'] = data['text_rus'].apply(remove_punctuation)\n",
    "\n",
    "# 对英语文本列应用去除标点符号的函数    Применение функции удаления пунктуации к столбцу с английским текстом\n",
    "data['text_eng'] = data['text_eng'].apply(remove_punctuation)\n",
    "\n",
    "# 打印数据框中俄语和英语文本的前几行    Вывод первых строк текста на русском и английском языках\n",
    "print(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3cbd5e-3398-42ca-b2c8-e8c7cae396b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个GPT2的分词器    Инициализация токенизатора GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 添加特殊标记，包括起始、结束和填充标记    Добавление специальных токенов: начало, конец и заполнение\n",
    "tokenizer.add_special_tokens({\n",
    "    \"bos_token\": \"<BOS>\",  # 起始标记    Токен начала предложения\n",
    "    \"eos_token\": \"<EOS>\",  # 结束标记    Токен конца предложения\n",
    "    \"pad_token\": \"<PAD>\"   # 填充标记    Токен заполнения\n",
    "})\n",
    "\n",
    "# 将填充标记的ID设为结束标记的ID    Установка идентификатора токена заполнения таким же, как у токена конца предложения\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8048888e-f7af-4800-836f-a743534b4267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs tensor([[  140,   252, 43666,  ...,   141,   227, 15166],\n",
      "        [  140,   242, 16142,  ..., 16843, 43108, 50258],\n",
      "        [  140,   250, 22177,  ..., 50258, 50258, 50258],\n",
      "        ...,\n",
      "        [  140,   107, 12466,  ...,   111, 43666, 16142],\n",
      "        [  140,   107, 12466,  ...,   111, 43666, 16142],\n",
      "        [  140,   107, 12466,  ..., 50258, 50258, 50258]])\n",
      "outputs tensor([[50257,  1890,  1752,  ..., 50258, 50258, 50258],\n",
      "        [50257,    43,  1039,  ..., 50258, 50258, 50258],\n",
      "        [50257,    40,   423,  ..., 50258, 50258, 50258],\n",
      "        ...,\n",
      "        [50257,    40,   760,  ..., 50258, 50258, 50258],\n",
      "        [50257,    40,   760,  ..., 50258, 50258, 50258],\n",
      "        [50257,    40, 17666,  ..., 50258, 50258, 50258]])\n",
      "Input tensor shape: torch.Size([735543, 32])\n",
      "Output tensor shape: torch.Size([735543, 32])\n",
      "Mask tensor shape: torch.Size([735543, 32])\n",
      "Predicted Output tensor shape: torch.Size([735543, 32])\n"
     ]
    }
   ],
   "source": [
    "# 定义数据集生成函数    Определение функции для генерации датасета\n",
    "def Sentences_Dataset(sentence_pairs, tokenizer, max_len=32):\n",
    "    inputs = []  # 输入张量列表    Список тензоров для входов\n",
    "    outputs = []  # 输出张量列表    Список тензоров для выходов\n",
    "    predicted_outputs = []  # 预测输出张量列表    Список тензоров для предсказанных выходов\n",
    "    masks = []  # 掩码张量列表    Список тензоров для масок\n",
    "\n",
    "    # 使用 values 属性快速访问数据    Использование свойства values для быстрого доступа к данным\n",
    "    for i, row in enumerate(sentence_pairs.values):\n",
    "        # 对输入（俄语）和输出（英语）进行分词    Токенизация входных данных (русский) и выходных данных (английский)\n",
    "        input_tokens = tokenizer(row[1], max_length=max_len, padding='max_length', truncation=True, add_special_tokens=True)['input_ids']\n",
    "        output_tokens = tokenizer(row[3], max_length=max_len - 1, padding='max_length', truncation=True, add_special_tokens=True)['input_ids']\n",
    "\n",
    "        # 在输出序列前加上 BOS token    Добавление BOS-токена в начало выходной последовательности\n",
    "        output_tokens_with_bos = [tokenizer.bos_token_id] + output_tokens\n",
    "        # 在预测输出序列末尾加上 EOS token    Добавление EOS-токена в конец предсказанной выходной последовательности\n",
    "        predicted_output_tokens = output_tokens + [tokenizer.eos_token_id]\n",
    "\n",
    "        # 将 tokens 添加到列表中    Добавление токенов в списки\n",
    "        inputs.append(torch.tensor(input_tokens))\n",
    "        outputs.append(torch.tensor(output_tokens_with_bos))\n",
    "        predicted_outputs.append(torch.tensor(predicted_output_tokens))  # 预测输出是没有 BOS token 的版本    Предсказанная выходная последовательность не содержит BOS-токена\n",
    "\n",
    "        # 生成 mask，标记 pad 的位置为 0    Генерация маски, помечая места заполнения (pad) нулями\n",
    "        masks.append(torch.tensor([1 if token != tokenizer.pad_token_id else 0 for token in output_tokens_with_bos]))\n",
    "\n",
    "    # 使用 torch.stack 将列表转换为 tensor    Преобразование списков в тензоры с помощью torch.stack\n",
    "    inputs = torch.stack(inputs)\n",
    "    outputs = torch.stack(outputs)\n",
    "    predicted_outputs = torch.stack(predicted_outputs)\n",
    "    masks = torch.stack(masks)\n",
    "\n",
    "    return (inputs, outputs), predicted_outputs, masks\n",
    "\n",
    "\n",
    "# 使用自定义数据集函数生成数据    Генерация данных с помощью функции пользовательского датасета\n",
    "(inputs, outputs), predicted_output, masks = Sentences_Dataset(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']], tokenizer)\n",
    "print(\"Inputs\", inputs)\n",
    "print(\"outputs\", outputs)\n",
    "print(\"Input tensor shape:\", inputs.shape)       # 输入张量的形状    Форма входного тензора\n",
    "print(\"Output tensor shape:\", outputs.shape)     # 输出张量的形状    Форма выходного тензора\n",
    "print(\"Mask tensor shape:\", masks.shape)         # 掩码张量的形状    Форма тензора маски\n",
    "print(\"Predicted Output tensor shape:\", predicted_output.shape)  # 预测输出张量的形状    Форма тензора предсказанного выхода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157c1e6d-6d85-4168-b934-ae9163ecfdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([64, 32])\n",
      "Output IDs shape: torch.Size([64, 32])\n",
      "Predicted Output IDs shape: torch.Size([64, 32])\n",
      "Attention mask shape: torch.Size([64, 32])\n",
      "input_ids: tensor([  140,   255, 20375, 15166,   220, 21727, 16142, 43108, 15166, 16843,\n",
      "        12466,   111, 30143, 35072,   140,   109, 25443,   118, 15166, 16843,\n",
      "        12466,   122,   140,   115, 16843, 21169, 15166, 12466,   110, 12466,\n",
      "          107,   140])\n",
      "output_ids: tensor([50257,  1212,   318,   262, 25420, 13546,   286,  2869, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258])\n",
      "Predicted Output IDs: tensor([ 1212,   318,   262, 25420, 13546,   286,  2869, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258])\n",
      "attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 定义自定义数据集类    Определение класса пользовательского датасета\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, sentence_pairs, tokenizer, max_len=32):\n",
    "        self.sentence_pairs = sentence_pairs  # 存储句子对数据    Хранение пар предложений\n",
    "        self.tokenizer = tokenizer  # 存储分词器    Хранение токенизатора\n",
    "        self.max_len = max_len  # 最大序列长度    Максимальная длина последовательности\n",
    "\n",
    "        # 调用外部的 create_dataset 函数生成数据    Вызов внешней функции create_dataset для генерации данных\n",
    "        (self.inputs, self.outputs), self.predicted_outputs, self.masks = Sentences_Dataset(sentence_pairs, tokenizer, max_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)  # 返回数据集的大小    Возвращение размера датасета\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {  # 返回指定索引的数据    Возвращение данных по указанному индексу\n",
    "            'input_ids': self.inputs[idx],  # 输入 ID    Входные ID\n",
    "            'output_ids': self.outputs[idx],  # 输出 ID    Выходные ID\n",
    "            'predicted_output_ids': self.predicted_outputs[idx],  # 预测输出 ID    Предсказанные выходные ID\n",
    "            'attention_mask': self.masks[idx]  # 注意力掩码    Маска внимания\n",
    "        }\n",
    "\n",
    "\n",
    "# 创建数据集对象    Создание объекта датасета\n",
    "translation_dataset = TranslationDataset(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']], tokenizer)\n",
    "\n",
    "# 创建数据加载器    Создание загрузчика данных\n",
    "batch_size = 64  # 根据显存大小调整批量大小    Настройка размера батча в зависимости от доступной памяти\n",
    "data_loader = DataLoader(translation_dataset, batch_size=batch_size, shuffle=True)  # 创建数据加载器对象    Создание объекта загрузчика данных\n",
    "\n",
    "# 检查数据加载器    Проверка загрузчика данных\n",
    "for batch in data_loader:\n",
    "    print(\"Input IDs shape:\", batch['input_ids'].shape)  # 打印输入 ID 的形状    Вывод формы входных ID\n",
    "    print(\"Output IDs shape:\", batch['output_ids'].shape)  # 打印输出 ID 的形状    Вывод формы выходных ID\n",
    "    print(\"Predicted Output IDs shape:\", batch['predicted_output_ids'].shape)  # 打印预测输出 ID 的形状    Вывод формы предсказанных выходных ID\n",
    "    print(\"Attention mask shape:\", batch['attention_mask'].shape)  # 打印注意力掩码的形状    Вывод формы маски внимания\n",
    "    break  # 只查看第一个批次    Просмотр только первого батча\n",
    "\n",
    "# 打印第一个样本的输入、输出、预测输出和注意力掩码    \n",
    "# Вывод входных, выходных, предсказанных выходных данных и маски внимания для первого примера\n",
    "for batch in data_loader:\n",
    "    print(\"input_ids:\", batch['input_ids'][0])  # 打印第一个样本的输入 IDs    Вывод входных ID первого примера\n",
    "    print(\"output_ids:\", batch['output_ids'][0])  # 打印第一个样本的输出 IDs    Вывод выходных ID первого примера\n",
    "    print(\"Predicted Output IDs:\", batch['predicted_output_ids'][0])  # 打印第一个样本的预测输出 IDs    Вывод предсказанных выходных ID первого примера\n",
    "    print(\"attention_mask:\", batch['attention_mask'][0])  # 打印第一个样本的注意力掩码    Вывод маски внимания первого примера\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912cf49e-49f6-46d3-ba93-6d2bf7e30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tokenizer_with_dataloader(data_loader, tokenizer, num_samples=5):\n",
    "    # 获取一个批次数据    Получение одного пакета данных\n",
    "    batch = next(iter(data_loader))\n",
    "    \n",
    "    # 提取 input_ids 和 output_ids    Извлечение input_ids и output_ids\n",
    "    input_ids_batch = batch['input_ids']  # [batch_size, seq_len] 批次大小和序列长度    Размер пакета и длина последовательности\n",
    "    output_ids_batch = batch['output_ids']  # [batch_size, seq_len] 批次大小和序列长度    Размер пакета и длина последовательности\n",
    "    \n",
    "    # 转为列表以便逐条处理    Преобразование в список для построчной обработки\n",
    "    input_ids_list = input_ids_batch.tolist()\n",
    "    output_ids_list = output_ids_batch.tolist()\n",
    "    \n",
    "    # 遍历样本并解码    Перебор примеров и декодирование\n",
    "    for i, (input_ids, output_ids) in enumerate(zip(input_ids_list, output_ids_list)):\n",
    "        # 使用 tokenizer 解码    Декодирование с использованием токенизатора\n",
    "        decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        decoded_output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # 打印对比结果    Вывод результата сравнения\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\"  Original Input IDs: {input_ids}\")  # 原始输入 ID    Исходные входные идентификаторы\n",
    "        print(f\"  Decoded Input Text: {decoded_input}\")  # 解码后的输入文本    Декодированный входной текст\n",
    "        print(f\"  Original Output IDs: {output_ids}\")  # 原始输出 ID    Исходные выходные идентификаторы\n",
    "        print(f\"  Decoded Output Text: {decoded_output}\")  # 解码后的输出文本    Декодированный выходной текст\n",
    "        print()\n",
    "        \n",
    "        # 控制显示样本数量    Ограничение количества отображаемых примеров\n",
    "        if i + 1 >= num_samples:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444f2d41-1477-41c6-9ae7-2c8b6807c20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "  Original Input IDs: [140, 94, 15166, 25443, 109, 141, 231, 18849, 12466, 95, 25443, 120, 35072, 12466, 122, 220, 20375, 25443, 120, 220, 141, 229, 20375, 15166, 12466, 123, 21169, 15166, 18849, 21727, 141, 227]\n",
      "  Decoded Input Text: Сообщи Тому о том что происх\n",
      "  Original Output IDs: [50257, 5756, 4186, 760, 45038, 5836, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Output Text: Let Tom know whats happening\n",
      "\n",
      "Sample 2:\n",
      "  Original Input IDs: [140, 241, 43666, 16843, 12466, 122, 22177, 12466, 121, 16142, 35072, 141, 229, 18849, 30143, 21727, 40623, 12466, 110, 25443, 112, 18849, 20375, 45367, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Input Text: Где он научился водить\n",
      "  Original Output IDs: [50257, 8496, 750, 339, 2193, 284, 3708, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Output Text: Where did he learn to drive\n",
      "\n",
      "Sample 3:\n",
      "  Original Input IDs: [140, 253, 15166, 141, 229, 16843, 43108, 35072, 12466, 110, 45035, 12466, 110, 21727, 16843, 140, 111, 43666, 16142, 12466, 122, 21727, 20375, 16142, 38857, 30143, 40623, 16843, 20375, 16843, 12466, 112]\n",
      "  Decoded Input Text: Почему вы всегда оставляете д\n",
      "  Original Output IDs: [50257, 5195, 389, 345, 1464, 4305, 262, 3420, 1280, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Output Text: Why are you always leaving the door open\n",
      "\n",
      "Sample 4:\n",
      "  Original Input IDs: [140, 243, 140, 111, 15166, 12466, 122, 20375, 38857, 16843, 20375, 12466, 121, 16843, 12466, 109, 45035, 30143, 220, 20375, 15166, 141, 229, 22177, 45035, 43108, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Input Text: Его ответ не был точным\n",
      "  Original Output IDs: [50257, 1544, 42547, 1577, 257, 7141, 3280, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Output Text: He didnt give a precise answer\n",
      "\n",
      "Sample 5:\n",
      "  Original Input IDs: [140, 107, 12466, 119, 141, 236, 140, 109, 30143, 141, 236, 220, 21727, 38857, 25443, 117, 220, 40623, 140, 115, 45035, 31583, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Input Text: Я люблю свой язык\n",
      "  Original Output IDs: [50257, 40, 1842, 616, 6868, 3303, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]\n",
      "  Decoded Output Text: I love my native language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 检查分词器的工作情况    Проверка работы токенизатора\n",
    "check_tokenizer_with_dataloader(data_loader, tokenizer, num_samples=5)  # 使用5个样本检查    Проверка на 5 примерах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a3326-b42a-4fb7-a9be-3e4422a7965f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc31205e-f6a8-4142-a99d-620b52255f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器类，负责处理输入序列    Класс кодировщика, который обрабатывает входные последовательности\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        # 嵌入层，将输入词索引映射到向量    Слой эмбеддингов для отображения индексов слов в вектора\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        # 双向 GRU，用于提取上下文特征    Двунаправленный GRU для извлечения контекстных признаков\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
    "        # 将编码器的隐藏状态映射到解码器隐藏状态初值    Преобразование скрытых состояний кодировщика в начальное скрытое состояние декодера\n",
    "        self.fc_hidden_init = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)  # 添加 Dropout，防止过拟合    Добавление Dropout для предотвращения переобучения\n",
    "\n",
    "    def forward(self, source_sequence):\n",
    "        # 将序列转置为 (src_len, batch_size) 格式    Транспонирование последовательности в формат (src_len, batch_size)\n",
    "        source_sequence = source_sequence.transpose(0, 1)\n",
    "        # 嵌入并添加 Dropout，结果转置为 (src_len, batch_size, embedding_dim)    Эмбеддинг с добавлением Dropout, транспонируется в (src_len, batch_size, embedding_dim)\n",
    "        embedded_sequence = self.dropout(self.embedding(source_sequence)).transpose(0, 1)\n",
    "        # 输入嵌入到 GRU，得到输出和隐藏状态    Подача эмбеддингов в GRU, получение выходов и скрытых состояний\n",
    "        encoder_outputs, encoder_hidden_states = self.rnn(embedded_sequence)\n",
    "        # 合并双向隐藏状态，映射到解码器初始隐藏状态    Объединение двунаправленных скрытых состояний для начального состояния декодера\n",
    "        decoder_initial_hidden = torch.tanh(\n",
    "            self.fc_hidden_init(torch.cat((encoder_hidden_states[-2, :, :], encoder_hidden_states[-1, :, :]), dim=1))\n",
    "        )\n",
    "        return encoder_outputs, decoder_initial_hidden\n",
    "\n",
    "\n",
    "# 注意力机制类，计算解码器隐藏状态与编码器输出的相关性    Класс механизма внимания, вычисляющий зависимость между скрытыми состояниями декодера и выходами кодировщика\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        # 解码器隐藏状态投影到编码器空间    Проекция скрытых состояний декодера в пространство кодировщика\n",
    "        self.decoder_projection = nn.Linear(decoder_hidden_dim, encoder_hidden_dim * 2)\n",
    "        self.scale = 1.0 / (encoder_hidden_dim * 2) ** 0.5  # 缩放因子，防止梯度消失    Коэффициент масштабирования для предотвращения затухания градиентов\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        # 解码器隐藏状态：[batch_size, decoder_hidden_dim]    Скрытое состояние декодера: [batch_size, decoder_hidden_dim]\n",
    "        # 编码器输出：[src_len, batch_size, encoder_hidden_dim * 2]    Выход кодировщика: [src_len, batch_size, encoder_hidden_dim * 2]\n",
    "        src_len = encoder_outputs.shape[0]  # 获取源序列长度    Получение длины входной последовательности\n",
    "\n",
    "        # 将解码器隐藏状态投影到编码器空间    Проекция скрытых состояний декодера в пространство кодировщика\n",
    "        decoder_hidden = self.decoder_projection(decoder_hidden)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # 添加时间维度：[batch_size, 1, encoder_hidden_dim * 2]\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # 转置编码器输出：[batch_size, src_len, encoder_hidden_dim * 2]\n",
    "        # 计算注意力得分    Вычисление весов внимания\n",
    "        attention_scores = torch.bmm(decoder_hidden, encoder_outputs.transpose(1, 2)) * self.scale\n",
    "        # 对注意力得分进行 softmax 归一化    Нормализация весов внимания с помощью softmax\n",
    "        attention_weights = F.softmax(attention_scores.squeeze(1), dim=1)\n",
    "        return attention_weights\n",
    "\n",
    "\n",
    "# 解码器类，负责生成目标序列    Класс декодера для генерации целевых последовательностей\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_vocab_size = output_vocab_size  # 目标词汇表大小    Размер словаря целевых токенов\n",
    "        self.attention = attention  # 注意力机制模块    Модуль механизма внимания\n",
    "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)  # 嵌入层    Слой эмбеддингов\n",
    "        # 解码器 GRU 输入包括上下文向量和目标嵌入    Вход GRU декодера включает контекстный вектор и эмбеддинги целевых токенов\n",
    "        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)\n",
    "        # 输出全连接层，用于生成目标词分布    Полносвязный слой для генерации распределения целевых токенов\n",
    "        self.fc_out = nn.Linear((encoder_hidden_dim * 2) + decoder_hidden_dim + embedding_dim, output_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout 层    Слой Dropout\n",
    "\n",
    "    def forward(self, target_token, decoder_hidden, encoder_outputs):\n",
    "        target_token = target_token.unsqueeze(1)  # 添加时间维度：[batch_size, 1]    Добавление временного измерения: [batch_size, 1]\n",
    "        # 嵌入目标 token，并应用 Dropout    Эмбеддинг целевых токенов с применением Dropout\n",
    "        embedded_target = self.dropout(self.embedding(target_token)).transpose(0, 1)\n",
    "        # 计算注意力权重    Вычисление весов внимания\n",
    "        attention_weights = self.attention(decoder_hidden, encoder_outputs).unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # 转置编码器输出    Транспонирование выходов кодировщика\n",
    "        # 计算上下文向量    Вычисление контекстного вектора\n",
    "        context_vector = torch.bmm(attention_weights, encoder_outputs).transpose(0, 1)\n",
    "        # 将目标嵌入和上下文向量拼接作为 GRU 输入    Конкатенация эмбеддингов целевых токенов и контекстного вектора для входа в GRU\n",
    "        rnn_input = torch.cat((embedded_target, context_vector), dim=2)\n",
    "        rnn_output, hidden_state = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))  # 通过 GRU 计算    Вычисление с помощью GRU\n",
    "        rnn_output = rnn_output.squeeze(0)  # 去掉时间维度    Удаление временного измерения\n",
    "        embedded_target = embedded_target.squeeze(0)  # 去掉时间维度    Удаление временного измерения\n",
    "        context_vector = context_vector.squeeze(0)  # 去掉时间维度    Удаление временного измерения\n",
    "        # 计算输出预测    Вычисление предсказаний\n",
    "        predictions = self.fc_out(torch.cat((rnn_output, context_vector, embedded_target), dim=1))\n",
    "        return predictions, hidden_state.squeeze(0), attention_weights.squeeze(1)\n",
    "\n",
    "\n",
    "# Seq2Seq 模型类，结合编码器、解码器和注意力机制    Класс Seq2Seq модели, объединяющий кодировщик, декодер и механизм внимания\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder  # 编码器    Кодировщик\n",
    "        self.decoder = decoder  # 解码器    Декодер\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source_sequence, target_sequence, teacher_forcing_ratio=0.9):\n",
    "        batch_size = source_sequence.shape[1]  # 批量大小    Размер батча\n",
    "        target_len = target_sequence.shape[0]  # 目标序列长度    Длина целевой последовательности\n",
    "        target_vocab_size = self.decoder.output_vocab_size  # 目标词汇表大小    Размер словаря целевых токенов\n",
    "        # 初始化输出张量    Инициализация тензора для выходов\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "        # 通过编码器计算编码器输出和初始解码器隐藏状态    Получение выходов кодировщика и начального состояния декодера\n",
    "        encoder_outputs, decoder_hidden = self.encoder(source_sequence)\n",
    "        # 解码器的初始输入为 <BOS>（开始标记）    Начальный вход декодера - токен <BOS> (начало последовательности)\n",
    "        decoder_input = target_sequence[0, :]\n",
    "        for t in range(1, target_len):\n",
    "            # 通过解码器预测下一个 token    Предсказание следующего токена с помощью декодера\n",
    "            output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            outputs[t] = output  # 保存当前时间步的输出    Сохранение выхода текущего временного шага\n",
    "            top1 = output.argmax(1)  # 获取概率最高的 token    Выбор токена с максимальной вероятностью\n",
    "            # 决定是否使用教师强制    Решение о применении teacher forcing\n",
    "            decoder_input = target_sequence[t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a44bd11c-b1f0-4d88-92f0-066a1dcf5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(50260, 256)\n",
      "    (rnn): GRU(256, 256, bidirectional=True)\n",
      "    (fc_hidden_init): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (decoder_projection): Linear(in_features=256, out_features=512, bias=True)\n",
      "    )\n",
      "    (embedding): Embedding(50260, 256)\n",
      "    (rnn): GRU(768, 256)\n",
      "    (fc_out): Linear(in_features=1024, out_features=50260, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义输入和输出词汇表大小    Определение размеров словаря входной и выходной последовательностей\n",
    "input_vocab_size = len(tokenizer.get_vocab())  # 输入词汇表大小    Размер словаря входных токенов\n",
    "output_vocab_size = len(tokenizer.get_vocab())  # 输出词汇表大小    Размер словаря выходных токенов\n",
    "\n",
    "# 定义编码器和解码器的嵌入维度    Определение размерности эмбеддингов кодировщика и декодера\n",
    "encoder_embedding_dim = 256  # 编码器嵌入层的维度    Размерность эмбеддингов кодировщика\n",
    "decoder_embedding_dim = 256  # 解码器嵌入层的维度    Размерность эмбеддингов декодера\n",
    "\n",
    "# 定义编码器和解码器的隐藏状态维度    Определение размерности скрытых состояний кодировщика и декодера\n",
    "encoder_hidden_dim = 256  # 编码器隐藏层的维度    Размерность скрытых состояний кодировщика\n",
    "decoder_hidden_dim = 256  # 解码器隐藏层的维度    Размерность скрытых состояний декодера\n",
    "\n",
    "# 定义 Dropout 概率，用于防止过拟合    Определение вероятности Dropout для предотвращения переобучения\n",
    "encoder_dropout = 0.5  # 编码器的 Dropout 概率    Вероятность Dropout кодировщика\n",
    "decoder_dropout = 0.5  # 解码器的 Dropout 概率    Вероятность Dropout декодера\n",
    "\n",
    "# 实例化注意力机制模块    Инициализация модуля механизма внимания\n",
    "attention = Attention(encoder_hidden_dim, decoder_hidden_dim)  # 创建注意力对象    Создание объекта механизма внимания\n",
    "\n",
    "# 初始化编码器    Инициализация кодировщика\n",
    "encoder = Encoder(\n",
    "    input_vocab_size=input_vocab_size,  # 输入词汇表的大小    Размер словаря входных токенов\n",
    "    embedding_dim=encoder_embedding_dim,  # 嵌入层的维度    Размерность эмбеддингов\n",
    "    encoder_hidden_dim=encoder_hidden_dim,  # 隐藏状态的维度    Размерность скрытых состояний\n",
    "    decoder_hidden_dim=decoder_hidden_dim,  # 解码器隐藏状态的维度，用于初始化解码器    Размерность скрытых состояний декодера для инициализации\n",
    "    dropout=encoder_dropout  # Dropout 概率    Вероятность Dropout\n",
    ")\n",
    "\n",
    "# 初始化解码器    Инициализация декодера\n",
    "decoder = Decoder(\n",
    "    output_vocab_size=output_vocab_size,  # 输出词汇表的大小    Размер словаря выходных токенов\n",
    "    embedding_dim=decoder_embedding_dim,  # 嵌入层的维度    Размерность эмбеддингов\n",
    "    encoder_hidden_dim=encoder_hidden_dim,  # 编码器隐藏状态的维度，用于上下文向量    Размерность скрытых состояний кодировщика для контекстного вектора\n",
    "    decoder_hidden_dim=decoder_hidden_dim,  # 解码器隐藏状态的维度    Размерность скрытых состояний декодера\n",
    "    dropout=decoder_dropout,  # Dropout 概率    Вероятность Dropout\n",
    "    attention=attention  # 注意力机制模块    Модуль механизма внимания\n",
    ")\n",
    "\n",
    "# 检查是否有 GPU 可用，并设置设备    Проверка наличия GPU и настройка устройства\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 如果有 GPU 使用 GPU，否则使用 CPU    Использование GPU, если доступен, иначе CPU\n",
    "\n",
    "# 初始化 Seq2Seq 模型    Инициализация Seq2Seq модели\n",
    "seq2seq_model_attention = Seq2Seq(encoder, decoder, device).to(device)  # 将编码器和解码器传递给模型，并将其加载到设备上    Передача кодировщика и декодера в модель и перенос на устройство\n",
    "\n",
    "# 打印模型结构    Печать структуры модели\n",
    "print(seq2seq_model_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138ab13-891e-44bb-984d-0342bba8b540",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "563a0fb2-965b-4c40-be91-87455aabbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # 导入垃圾回收模块    Импорт модуля для управления сбором мусора\n",
    "\n",
    "# 清除缓存和显存的函数    Функция для очистки кэша и видеопамяти\n",
    "def clear_cache():\n",
    "    gc.collect()  # 清理 Python 的垃圾内存    Очистка мусора в Python\n",
    "    if torch.cuda.is_available():  # 如果 GPU 可用    Если доступен GPU\n",
    "        torch.cuda.empty_cache()  # 清理 GPU 的缓存    Очистка кэша GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "139db5dc-96ab-4554-964c-706e86b7b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数    Определение функции обучения\n",
    "def train(seq2seq_model_attention, data_loader, optimizer, criterion, device, num_epochs, pad_token_id=50258):\n",
    "    # 将模型设置为训练模式    Перевод модели в режим обучения\n",
    "    seq2seq_model_attention.train()  \n",
    "    for epoch in range(num_epochs):\n",
    "        clear_cache()  # 清除缓存和显存    Очистка кэша и видеопамяти\n",
    "        total_epoch_loss = 0  # 累积每个 epoch 的总损失    Суммарные потери за эпоху\n",
    "        num_correct_tokens = 0  # 累积正确预测的 token 数    Суммарное количество правильно предсказанных токенов\n",
    "        num_total_tokens = 0  # 累积总的 token 数    Суммарное количество токенов\n",
    "\n",
    "        # 遍历数据加载器的每个批次    Итерация по батчам из загрузчика данных\n",
    "        for batch in tqdm(data_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\"):\n",
    "            # 获取输入和输出的 ID 并将它们传递到设备上    Извлечение ID входных и выходных последовательностей и их перенос на устройство\n",
    "            input_ids = batch['input_ids'].to(device)  # 输入 ID    ID входных токенов\n",
    "            output_ids = batch['output_ids'].to(device)  # 输出 ID    ID выходных токенов\n",
    "            \n",
    "            # 动态调整教师强制比例    Динамическое изменение коэффициента teacher forcing\n",
    "            teacher_forcing_ratio = max(0.5, 1 - epoch * 0.1)  # 最低值为 0.5    Минимальное значение 0.5\n",
    "            # 前向传播获取模型预测    Прямой проход для получения предсказаний модели\n",
    "            predictions = seq2seq_model_attention(input_ids, output_ids, teacher_forcing_ratio=0.9)\n",
    "            \n",
    "            # 将目标序列调整为 1D    Приведение целевых последовательностей к одномерному виду\n",
    "            target_ids = output_ids[:, 1:].contiguous().view(-1)  # 去掉 <BOS> 标记    Удаление токена <BOS>\n",
    "            predictions = predictions[:, 1:].contiguous().view(-1, predictions.shape[-1])  # 调整预测的形状    Преобразование формы предсказаний\n",
    "            \n",
    "            # 计算当前批次的损失    Вычисление потерь для текущего батча\n",
    "            loss = criterion(predictions, target_ids)\n",
    "            \n",
    "            optimizer.zero_grad()  # 清零优化器的梯度    Обнуление градиентов оптимизатора\n",
    "   \n",
    "            loss.backward()  # 反向传播计算梯度    Обратное распространение для расчета градиентов\n",
    "\n",
    "            # 裁剪梯度以避免梯度爆炸    Ограничение градиентов для предотвращения их взрывного роста\n",
    "            torch.nn.utils.clip_grad_norm_(seq2seq_model_attention.parameters(), max_norm=1.0)\n",
    "    \n",
    "            optimizer.step()  # 更新模型参数    Обновление параметров модели\n",
    "  \n",
    "            total_epoch_loss += loss.item()  # 累积损失    Суммирование потерь\n",
    "            \n",
    "            # 计算非 PAD token 的掩码    Создание маски для токенов, не являющихся PAD\n",
    "            non_pad_mask = target_ids != 50258  # 忽略 PAD token    Игнорирование токенов PAD\n",
    "            predicted_tokens = predictions.argmax(dim=1)  # 获取预测的 token    Получение предсказанных токенов\n",
    "            # 计算正确预测的 token 数量    Подсчет правильно предсказанных токенов\n",
    "            num_correct_tokens += (predicted_tokens == target_ids).masked_select(non_pad_mask).sum().item()\n",
    "            # 累积非 PAD token 的总数量    Подсчет общего количества токенов, не являющихся PAD\n",
    "            num_total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "        # 计算当前 epoch 的平均损失和准确率    Вычисление средней потери и точности за текущую эпоху\n",
    "        epoch_loss = total_epoch_loss / len(data_loader)  # 平均损失    Средняя потеря\n",
    "        epoch_accuracy = num_correct_tokens / num_total_tokens  # 平均准确率    Средняя точность\n",
    "\n",
    "        # 打印 epoch 的损失和准确率    Вывод потерь и точности за эпоху\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecc1942-0492-45bd-9f75-8a478febc777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/6]: 100%|█████████████████████████████████████████████████████████████| 11493/11493 [2:34:46<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6] - Loss: 6.5875, Accuracy: 0.0634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/6]: 100%|█████████████████████████████████████████████████████████████| 11493/11493 [2:24:03<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/6] - Loss: 9.6336, Accuracy: 0.0271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/6]: 100%|█████████████████████████████████████████████████████████████| 11493/11493 [2:17:32<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6] - Loss: 11.2188, Accuracy: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/6]: 100%|█████████████████████████████████████████████████████████████| 11493/11493 [2:19:05<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6] - Loss: 11.7274, Accuracy: 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/6]: 100%|█████████████████████████████████████████████████████████████| 11493/11493 [2:37:09<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6] - Loss: 10.5979, Accuracy: 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/6]: 100%|█████████████████████████████████████████████████████████████| 11493/11493 [3:28:05<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6] - Loss: 10.2495, Accuracy: 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数，忽略 padding token 的损失计算\n",
    "# Определяем функцию потерь, игнорируя padding токены\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=50258).to(device)  # 设置到指定设备 / Переносим на указанное устройство\n",
    "\n",
    "# 定义优化器，使用 Adam 优化器，学习率为 0.01\n",
    "# Определяем оптимизатор, используем Adam с learning rate = 0.002\n",
    "optimizer = optim.Adam(seq2seq_model_attention.parameters(), lr=0.002)\n",
    "\n",
    "# 训练模型  Обучение модели\n",
    "train(seq2seq_model_attention, data_loader, optimizer, criterion, device, num_epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134ab24-5dc1-4794-b5db-4d1b87ebfc21",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f79545b7-b710-4a1a-b4fe-a1fc77907621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文本列表 - Пример списка текстов\n",
    "sample_texts = [\n",
    "    \"Давайте что-нибудь попробуем!\",  # 让我们试试吧！\n",
    "    \"Мне пора идти спать.\",           # 我该睡觉了。\n",
    "    \"Что ты делаешь?\",                # 你在做什么？\n",
    "    \"Что ты делаешь?\",                # 你在做什么？\n",
    "    \"Собака бегает по траве\",         # 狗在草地上奔跑\n",
    "    \"Небо и море - все синее.\"        # 天空和海洋全都是蓝色\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b56eab2a-f380-439d-a853-cf91f4c92d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义翻译函数，输入文本，分词器，带注意力机制的 Seq2Seq 模型，最大翻译长度\n",
    "# Определяем функцию перевода, принимая текст, токенизатор, Seq2Seq модель с вниманием и максимальную длину перевода\n",
    "def translate(text, tokenizer, seq2seq_model_attention, max_len=24):\n",
    "    seq2seq_model_attention.eval()  # 设置模型为评估模式  Устанавливаем модель в режим оценки\n",
    "    seq2seq_model_attention.to(device)  # 将模型加载到指定设备  Переносим модель на заданное устройство\n",
    "\n",
    "    # 将输入文本编码为张量并转移到设备\n",
    "    # Кодируем входной текст в тензор и переносим на устройство\n",
    "    source_sequence = tokenizer.encode(text, return_tensors='pt').squeeze(0).to(device)  # [source_len]\n",
    "    source_sequence = source_sequence.unsqueeze(1)  # 为批处理添加维度  Добавляем размерность для батча: [source_len, 1]\n",
    "    \n",
    "    # 初始化编码器\n",
    "    # Инициализируем энкодер\n",
    "    encoder_outputs, decoder_hidden = seq2seq_model_attention.encoder(source_sequence)\n",
    "    \n",
    "    # 初始化解码器输入为 <BOS> 标记\n",
    "    # Инициализируем вход декодера как токен <BOS>\n",
    "    sos_token = tokenizer.bos_token_id  # 获取 <BOS> 标记的 ID  Получаем ID для токена <BOS>\n",
    "    eos_token = tokenizer.eos_token_id  # 获取 <EOS> 标记的 ID  Получаем ID для токена <EOS>\n",
    "    decoder_input = torch.tensor([sos_token], device=device)  # 解码器的输入为 <BOS> 标记  Вход декодера - токен <BOS>\n",
    "    \n",
    "    # 存储翻译结果\n",
    "    # Храним результат перевода\n",
    "    translated_tokens = []\n",
    "    \n",
    "    # 在最大长度内进行解码\n",
    "    # Декодируем до максимальной длины\n",
    "    for _ in range(max_len):\n",
    "        # 解码一步\n",
    "        # Шаг декодирования\n",
    "        predictions, decoder_hidden, attention_weights = seq2seq_model_attention.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        predictions[0][translated_tokens] -= 1.0  # 降低已生成词的概率\n",
    "        #print(f\"Step {_}:\")\n",
    "        #print(f\"Attention Weights: {attention_weights}\")\n",
    "        #print(f\"predictions: {predictions}\")\n",
    "        # 获取概率最高的 token\n",
    "        # Получаем токен с максимальной вероятностью\n",
    "        top1 = predictions.argmax(1).item()  # 获取预测的最高概率的索引  Получаем индекс с наибольшей вероятностью\n",
    "        \n",
    "        # 保存生成的 token\n",
    "        # Сохраняем сгенерированный токен\n",
    "        translated_tokens.append(top1)\n",
    "        \n",
    "        # 如果遇到 <EOS> 标记，则停止生成\n",
    "        # Прекращаем генерацию, если встретился токен <EOS>\n",
    "        if top1 == eos_token:\n",
    "            break\n",
    "        \n",
    "        # 下一步的解码器输入\n",
    "        # Ввод для следующего шага декодирования\n",
    "        decoder_input = torch.tensor([top1], device=device)  # 输入解码器为预测的 token  Ввод для декодера - это предсказанный токен\n",
    "    \n",
    "    # 使用 tokenizer.decode 将 token ID 转换为句子\n",
    "    # Преобразуем ID токенов в предложение с помощью decode\n",
    "    translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # 清理翻译结果\n",
    "    # Очищаем результат перевода\n",
    "    def clean_translation(translation):\n",
    "        translation = re.sub(r'([!?.])\\1+', r'\\1', translation)  # 合并重复的标点符号  Объединяем повторяющиеся знаки препинания\n",
    "        return translation.strip()  # 去除首尾空格  Убираем пробелы в начале и в конце\n",
    "\n",
    "    return clean_translation(translated_sentence)  # 返回清理后的翻译结果  Возвращаем очищенный перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aec4a8a-c930-44b6-b496-aa3d4ba08951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Давайте что-нибудь попробуем!\n",
      "Translated: started number color beautyes owe restaurant mother front whole owe restaurant owe restaurant card owe restaurant paint owe restaurant dead you saveFind\n",
      "\n",
      "Original: Мне пора идти спать.\n",
      "Translated: isUnfortunately enteringothsUnfortunately entering irresponsibleUnfortunately melThWillThWillTh frontHaveThWillThWillThWillThWill\n",
      "\n",
      "Original: Что ты делаешь?\n",
      "Translated: traveled suggest held suggest smoke suggest suggest speed suggest suggest sw suggest suggest suggest suggest suggest suggest suggest suggestriage suggest suggest shoeanni\n",
      "\n",
      "Original: Что ты делаешь?\n",
      "Translated: traveled suggest held suggest smoke suggest suggest speed suggest suggest sw suggest suggest suggest suggest suggest suggest suggest suggestriage suggest suggest shoeanni\n",
      "\n",
      "Original: Собака бегает по траве\n",
      "Translated: swurry conservative lookedter bet roughly exam suggest advise bet realityurry conservative looked conservative looked conservative looked conservative looked conservative looked conservative\n",
      "\n",
      "Original: Небо и море - все синее.\n",
      "Translated: exaggerNot 90Al finished modernJust dentistSome growNotReadNotAl that stupidNot promptListen mad guarantee parentsNotWho\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clear_cache()  # 清理缓存  Очистка кэша\n",
    "\n",
    "# 使用模型对每个文本进行翻译\n",
    "# Переводим каждый текст с помощью модели\n",
    "for text in sample_texts:\n",
    "    translation = translate(text, tokenizer, seq2seq_model_attention)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translated: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d69c174-25b9-41f5-8c4b-2f29bf9dd826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e389ecd-d51d-4c99-8734-da57d22c6d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cc99f-9e58-47cf-8aa9-f46aa7356b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
