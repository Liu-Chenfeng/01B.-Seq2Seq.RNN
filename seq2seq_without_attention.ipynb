{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ba683a-916e-4dad-b11f-3a850f4020b6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d387be-b82e-438d-902a-d4c97afcbf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的库    Импорт требуемых библиотек\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from transformers import DataCollatorWithPadding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dc0418-4f8e-4f50-bf7f-527183e2be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用 GPU 加速    Вызов GPU-ускорения\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6adc08-be21-4a6a-81a8-cef04bbdcbab",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b421745-3904-4600-bc79-01481cee8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件路径    Путь к файлу\n",
    "file_path = r\"C:\\Users\\lcf14\\Desktop\\homework\\Machine_Learning_appli\\有俄语-英语对应句 - 2024-10-31.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf10fb8d-9964-4cc0-86b3-3b06c459c8f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 读取文件并设置列名，忽略有问题的行    Чтение файла и установка имен столбцов, пропуская строки с ошибками\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_rus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_rus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_eng\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_eng\u001b[39m\u001b[38;5;124m\"\u001b[39m], on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m5000\u001b[39m]  \u001b[38;5;66;03m# 仅选择前10000行数据\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 输出前几行查看数据    Вывод первых нескольких строк для просмотра данных\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# 读取文件并设置列名，忽略有问题的行    Чтение файла и установка имен столбцов, пропуская строки с ошибками\n",
    "data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"id_rus\", \"text_rus\", \"id_eng\", \"text_eng\"], on_bad_lines='skip')\n",
    "data = data[:5000]  # 仅选择前10000行数据\n",
    "\n",
    "# 输出前几行查看数据    Вывод первых нескольких строк для просмотра данных\n",
    "print(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442c75af-e323-428f-a734-094f49836e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Tokenizer(transformers.GPT2Tokenizer):\\n\\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\\n        if token_ids_1 is None:\\n            return [self.bos_token_id, *token_ids_0, self.eos_token_id]\\n\\n        return [self.bos_token_id, *token_ids_0, self.bos_token_id, *token_ids_1, self.eos_token_id]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class Tokenizer(transformers.GPT2Tokenizer):\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return [self.bos_token_id, *token_ids_0, self.eos_token_id]\n",
    "\n",
    "        return [self.bos_token_id, *token_ids_0, self.bos_token_id, *token_ids_1, self.eos_token_id]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8ea83fb-b7d7-4cd4-ac20-d692c47b672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个GPT2的分词器    Инициализация токенизатора GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 添加特殊标记，包括起始、结束和填充标记    Добавление специальных токенов: начало, конец и заполнение\n",
    "tokenizer.add_special_tokens({\n",
    "    \"bos_token\": \"<BOS>\",  # 起始标记    Токен начала предложения\n",
    "    \"eos_token\": \"<EOS>\",  # 结束标记    Токен конца предложения\n",
    "    \"pad_token\": \"<PAD>\"   # 填充标记    Токен заполнения\n",
    "})\n",
    "\n",
    "# 将填充标记的ID设为结束标记的ID    Установка идентификатора токена заполнения таким же, как у токена конца предложения\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b3574a-da32-4a4a-9a75-6623fbe103de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([735543, 32])\n",
      "Output tensor shape: torch.Size([735543, 32])\n",
      "Mask tensor shape: torch.Size([735543, 32])\n",
      "Predicted Output tensor shape: torch.Size([735543, 32])\n"
     ]
    }
   ],
   "source": [
    "# 定义数据集生成函数    Определение функции для генерации датасета\n",
    "def Sentences_Dataset(sentence_pairs, tokenizer, max_len=24):\n",
    "    inputs = []  # 输入张量列表    Список тензоров для входов\n",
    "    outputs = []  # 输出张量列表    Список тензоров для выходов\n",
    "    predicted_outputs = []  # 预测输出张量列表    Список тензоров для предсказанных выходов\n",
    "    masks = []  # 掩码张量列表    Список тензоров для масок\n",
    "\n",
    "    # 使用 values 属性快速访问数据    Использование свойства values для быстрого доступа к данным\n",
    "    for i, row in enumerate(sentence_pairs.values):\n",
    "        # 对输入（俄语）和输出（英语）进行分词    Токенизация входных данных (русский) и выходных данных (английский)\n",
    "        input_tokens = tokenizer(row[1], max_length=max_len, padding='max_length', truncation=True, add_special_tokens=True)['input_ids']\n",
    "        output_tokens = tokenizer(row[3], max_length=max_len - 1, padding='max_length', truncation=True, add_special_tokens=True)['input_ids']\n",
    "\n",
    "        # 在输出序列前加上 BOS token    Добавление BOS-токена в начало выходной последовательности\n",
    "        output_tokens_with_bos = [tokenizer.bos_token_id] + output_tokens\n",
    "        # 在预测输出序列末尾加上 EOS token    Добавление EOS-токена в конец предсказанной выходной последовательности\n",
    "        predicted_output_tokens = output_tokens + [tokenizer.eos_token_id]\n",
    "\n",
    "        # 将 tokens 添加到列表中    Добавление токенов в списки\n",
    "        inputs.append(torch.tensor(input_tokens))\n",
    "        outputs.append(torch.tensor(output_tokens_with_bos))\n",
    "        predicted_outputs.append(torch.tensor(predicted_output_tokens))  # 预测输出是没有 BOS token 的版本    Предсказанная выходная последовательность не содержит BOS-токена\n",
    "\n",
    "        # 生成 mask，标记 pad 的位置为 0    Генерация маски, помечая места заполнения (pad) нулями\n",
    "        masks.append(torch.tensor([1 if token != tokenizer.pad_token_id else 0 for token in output_tokens_with_bos]))\n",
    "\n",
    "    # 使用 torch.stack 将列表转换为 tensor    Преобразование списков в тензоры с помощью torch.stack\n",
    "    inputs = torch.stack(inputs)\n",
    "    outputs = torch.stack(outputs)\n",
    "    predicted_outputs = torch.stack(predicted_outputs)\n",
    "    masks = torch.stack(masks)\n",
    "\n",
    "    return (inputs, outputs), predicted_outputs, masks\n",
    "\n",
    "\n",
    "# 使用自定义数据集函数生成数据    Генерация данных с помощью функции пользовательского датасета\n",
    "(inputs, outputs), predicted_output, masks = Sentences_Dataset(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']], tokenizer)\n",
    "print(\"Input tensor shape:\", inputs.shape)       # 输入张量的形状    Форма входного тензора\n",
    "print(\"Output tensor shape:\", outputs.shape)     # 输出张量的形状    Форма выходного тензора\n",
    "print(\"Mask tensor shape:\", masks.shape)         # 掩码张量的形状    Форма тензора маски\n",
    "print(\"Predicted Output tensor shape:\", predicted_output.shape)  # 预测输出张量的形状    Форма тензора предсказанного выхода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c885ef-48c4-4d08-b292-729753d4cd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([256, 32])\n",
      "Output IDs shape: torch.Size([256, 32])\n",
      "Predicted Output IDs shape: torch.Size([256, 32])\n",
      "Attention mask shape: torch.Size([256, 32])\n",
      "input_ids: tensor([  140,    94, 43666, 16843, 30143, 16142,   140,   117,   220, 20375,\n",
      "        15166,    11, 12466,   122,   220,   141,   229,   141,   239, 43108,\n",
      "        12466,    95, 25443,   120, 12466,   123, 21169, 15166, 21727, 18849,\n",
      "        20375,    13])\n",
      "output_ids: tensor([50257,  5211,   644,  4186,  7893,    13, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258])\n",
      "Predicted Output IDs: tensor([ 5211,   644,  4186,  7893,    13, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
      "        50258, 50258])\n",
      "attention_mask: tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 定义自定义数据集类    Определение класса пользовательского датасета\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, sentence_pairs, tokenizer, max_len=24):\n",
    "        self.sentence_pairs = sentence_pairs  # 存储句子对数据    Хранение пар предложений\n",
    "        self.tokenizer = tokenizer  # 存储分词器    Хранение токенизатора\n",
    "        self.max_len = max_len  # 最大序列长度    Максимальная длина последовательности\n",
    "\n",
    "        # 调用外部的 create_dataset 函数生成数据    Вызов внешней функции create_dataset для генерации данных\n",
    "        (self.inputs, self.outputs), self.predicted_outputs, self.masks = Sentences_Dataset(sentence_pairs, tokenizer, max_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)  # 返回数据集的大小    Возвращение размера датасета\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {  # 返回指定索引的数据    Возвращение данных по указанному индексу\n",
    "            'input_ids': self.inputs[idx],  # 输入 ID    Входные ID\n",
    "            'output_ids': self.outputs[idx],  # 输出 ID    Выходные ID\n",
    "            'predicted_output_ids': self.predicted_outputs[idx],  # 预测输出 ID    Предсказанные выходные ID\n",
    "            'attention_mask': self.masks[idx]  # 注意力掩码    Маска внимания\n",
    "        }\n",
    "\n",
    "\n",
    "# 创建数据集对象    Создание объекта датасета\n",
    "translation_dataset = TranslationDataset(data[['id_rus', 'text_rus', 'id_eng', 'text_eng']], tokenizer)\n",
    "\n",
    "# 创建数据加载器    Создание загрузчика данных\n",
    "batch_size = 256  # 根据显存大小调整批量大小    Настройка размера батча в зависимости от доступной памяти\n",
    "data_loader = DataLoader(translation_dataset, batch_size=batch_size, shuffle=True)  # 创建数据加载器对象    Создание объекта загрузчика данных\n",
    "\n",
    "# 检查数据加载器    Проверка загрузчика данных\n",
    "for batch in data_loader:\n",
    "    print(\"Input IDs shape:\", batch['input_ids'].shape)  # 打印输入 ID 的形状    Вывод формы входных ID\n",
    "    print(\"Output IDs shape:\", batch['output_ids'].shape)  # 打印输出 ID 的形状    Вывод формы выходных ID\n",
    "    print(\"Predicted Output IDs shape:\", batch['predicted_output_ids'].shape)  # 打印预测输出 ID 的形状    Вывод формы предсказанных выходных ID\n",
    "    print(\"Attention mask shape:\", batch['attention_mask'].shape)  # 打印注意力掩码的形状    Вывод формы маски внимания\n",
    "    break  # 只查看第一个批次    Просмотр только первого батча\n",
    "\n",
    "# 打印第一个样本的输入、输出、预测输出和注意力掩码    \n",
    "# Вывод входных, выходных, предсказанных выходных данных и маски внимания для первого примера\n",
    "for batch in data_loader:\n",
    "    print(\"input_ids:\", batch['input_ids'][0])  # 打印第一个样本的输入 IDs    Вывод входных ID первого примера\n",
    "    print(\"output_ids:\", batch['output_ids'][0])  # 打印第一个样本的输出 IDs    Вывод выходных ID первого примера\n",
    "    print(\"Predicted Output IDs:\", batch['predicted_output_ids'][0])  # 打印第一个样本的预测输出 IDs    Вывод предсказанных выходных ID первого примера\n",
    "    print(\"attention_mask:\", batch['attention_mask'][0])  # 打印第一个样本的注意力掩码    Вывод маски внимания первого примера\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400b158-530e-49bf-b458-09937295b90c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95dbb6b6-2e83-4d15-90c3-b52be6dd62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器类    Определение класса кодировщика\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, units, n_tokens, n_stacks=1, bidirectional=False, dropout_rate=0.3, cell_type=nn.GRU):\n",
    "        # 初始化编码器    Инициализация кодировщика\n",
    "        super(Encoder, self).__init__()\n",
    "        # 定义嵌入层，用于将输入序列转为嵌入向量    Определение слоя внедрения для преобразования входной последовательности в вектор внедрения\n",
    "        self.embedding = nn.Embedding(n_tokens, units)\n",
    "        # 定义 RNN 层（GRU），根据是否双向设置隐藏单元的数量    Определение слоя RNN (GRU), с настройкой количества скрытых единиц в зависимости от того, является ли он двунаправленным\n",
    "        self.rnn = cell_type(units, units, num_layers=n_stacks, bidirectional=bidirectional, dropout=dropout_rate, batch_first=True)\n",
    "        # 保存 RNN 是否为双向的    Хранение информации о том, является ли RNN двунаправленным\n",
    "        self.bidirectional = bidirectional\n",
    "        # 保存隐藏单元的数量    Хранение количества скрытых единиц\n",
    "        self.units = units\n",
    "        # 在 Encoder 和 Decoder 的 __init__ 方法中添加嵌入层初始化    Инициализация весов слоя внедрения в методах __init__ Encoder и Decoder\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # 获取输入序列，将其转为嵌入向量    Преобразование входной последовательности в вектор внедрения\n",
    "        embedded = self.embedding(input_seq)  # (batch_size, seq_len, units)    (размер_пакета, длина_последовательности, количество_единиц)\n",
    "        # 将嵌入向量输入到 RNN 层    Ввод вектора внедрения в слой RNN\n",
    "        encoder_outputs, hidden = self.rnn(embedded)\n",
    "\n",
    "        # 如果 RNN 是双向的，将最后一层的前向和后向隐藏状态拼接    \n",
    "        # Если RNN двунаправленный, объединение скрытых состояний из последнего слоя по направлению вперед и назад\n",
    "        if self.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)  # 拼接最后一层的前向和后向隐藏状态    Объединение последних скрытых состояний из обоих направлений\n",
    "        else:\n",
    "            hidden = hidden[-1].unsqueeze(0)  # 如果是单向，只取最后一层的隐藏状态    Если однонаправленный, взять только скрытое состояние из последнего слоя\n",
    "\n",
    "        hidden = hidden[:, :, :self.units]  # 确保hidden维度与解码器一致    Обеспечение соответствия размерности hidden с декодером\n",
    "        # 确保返回的隐藏状态形状符合解码器的输入需求    \n",
    "        # Обеспечение того, чтобы возвращаемая форма скрытого состояния соответствовала требованиям для входа в декодер\n",
    "        return encoder_outputs, hidden[:, :, :self.units]  # 将hidden的维度匹配到解码器的输入形状    Приведение размерности hidden к форме, необходимой для декодера\n",
    "\n",
    "\n",
    "# 定义解码器类    Определение класса декодера\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, units, n_labels, attention_dim, embedding_dim, cell_type=nn.GRU):\n",
    "        # 初始化解码器    Инициализация декодера\n",
    "        super(Decoder, self).__init__()\n",
    "        # 定义嵌入层，用于将解码器输入转换为嵌入向量    Определение слоя внедрения для преобразования входа в декодер в вектор внедрения\n",
    "        self.embedding = nn.Embedding(n_labels, embedding_dim)\n",
    "        # 定义 RNN 层，用于处理嵌入向量和上下文向量    Определение слоя RNN для обработки вектора внедрения и контекстного вектора\n",
    "        self.rnn = cell_type(embedding_dim, units, batch_first=True)\n",
    "        # 定义输出层，用于生成解码器的最终预测    Определение выходного слоя для генерации окончательного предсказания декодера\n",
    "        self.fc_out = nn.Linear(units, n_labels)\n",
    "        # 初始化嵌入层权重    Инициализация весов слоя внедрения\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, decoder_input, hidden):\n",
    "        # 获取解码器输入的嵌入向量    Получение вектора внедрения для входа в декодер\n",
    "        decoder_input_embedded = self.embedding(decoder_input)\n",
    "        # 如果 hidden 是 None，初始化为零张量    Если hidden равно None, инициализировать его как нулевой тензор\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(1, decoder_input.size(0), self.rnn.hidden_size, device=decoder_input.device)\n",
    "        # 确保隐藏状态是连续的    Обеспечение того, чтобы скрытое состояние было непрерывным\n",
    "        hidden = hidden.contiguous()\n",
    "        # 将嵌入向量输入到 RNN 层    Ввод вектора внедрения в слой RNN\n",
    "        output, hidden = self.rnn(decoder_input_embedded, hidden)\n",
    "        # 将 RNN 的输出传入全连接层生成预测    Пропуск выходных данных RNN через полносвязный слой для получения предсказания\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # 返回预测结果和新的隐藏状态    Возвращение предсказания и нового скрытого состояния\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "# 定义序列到序列模型类    Определение класса Seq2Seq модели\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.encoder = encoder  # 设置编码器    Установка кодировщика\n",
    "        self.decoder = decoder  # 设置解码器    Установка декодера\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden = self.encoder(encoder_input)  # 编码输入序列    Кодирование входной последовательности\n",
    "         # 初始化输出序列    Инициализация выходной последовательности\n",
    "        outputs = torch.zeros(decoder_input.size(0), decoder_input.size(1), self.decoder.fc_out.out_features).to(decoder_input.device)\n",
    "\n",
    "        # 设置解码器的第一个输入标记为 <BOS>    Установка первого входного маркера для декодера как <BOS>\n",
    "        input_token = decoder_input[:, 0].unsqueeze(1)\n",
    "\n",
    "        # 逐时间步遍历目标序列长度    Проход по целевой последовательности по временным шагам\n",
    "        for t in range(1, decoder_input.size(1)):\n",
    "            # 解码器前向传播    Прямой проход декодера\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "            outputs[:, t, :] = output  # 存储当前时间步的输出    Сохранение выходных данных на текущем временном шаге\n",
    "\n",
    "            # 使用教师强制决定下一个输入    Применение принудительного обучения для выбора следующего входа\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)  # 获取预测标记    Получение предсказанного маркера\n",
    "            input_token = decoder_input[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs  # 返回解码器的所有输出    Возвращение всех выходных данных декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "069885ec-5474-401e-9bb6-70cbe3d27219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqModel(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(50260, 256)\n",
      "    (rnn): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(50260, 256)\n",
      "    (rnn): GRU(256, 256, batch_first=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=50260, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义模型的参数    Определение параметров модели\n",
    "units = 256  # 隐藏单元的数量    Количество скрытых единиц\n",
    "n_tokens = len(tokenizer.get_vocab())  # 输入词典大小    Размер словаря для входных данных\n",
    "n_labels = len(tokenizer.get_vocab())  # 输出词典大小    Размер словаря для выходных данных\n",
    "n_stacks = 3  # RNN 层数    Количество слоев RNN\n",
    "\n",
    "# 初始化编码器，传入词典大小、隐藏单元数量、层数等参数    \n",
    "# Инициализация кодировщика с передачей размера словаря, количества скрытых единиц, количества слоев и других параметров\n",
    "encoder = Encoder(\n",
    "    units=units,  # 隐藏单元数量    Количество скрытых единиц\n",
    "    n_tokens=n_tokens,  # 输入词典大小    Размер словаря для входных данных\n",
    "    n_stacks=n_stacks,  # RNN 层数    Количество слоев RNN\n",
    "    bidirectional=True,  # 是否双向    Является ли кодировщик двунаправленным\n",
    "    cell_type=nn.GRU  # RNN 类型    Тип RNN\n",
    ")\n",
    "\n",
    "# 初始化解码器，传入隐藏单元数量、输出词典大小、层数等参数    \n",
    "# Инициализация декодера с передачей количества скрытых единиц, размера выходного словаря и других параметров\n",
    "decoder = Decoder(\n",
    "    units=units,  # 隐藏单元数量    Количество скрытых единиц\n",
    "    n_labels=n_labels,  # 输出词典大小    Размер выходного словаря\n",
    "    attention_dim=units,    # 注意力层的维度    Размерность слоя внимания\n",
    "    embedding_dim=units,    # 嵌入层的维度    Размерность слоя внедрения\n",
    "    cell_type=nn.GRU       # RNN 的类型    Тип RNN\n",
    ")\n",
    "\n",
    "# 初始化 Seq2SeqModel，将编码器和解码器实例传入    Инициализация модели Seq2Seq, передавая экземпляры кодировщика и декодера\n",
    "seq2seq_model = Seq2SeqModel(\n",
    "    encoder=encoder,  # 传入编码器    Передача кодировщика\n",
    "    decoder=decoder   # 传入解码器    Передача декодера\n",
    ")\n",
    "\n",
    "# 打印模型结构    Печать структуры модели\n",
    "print(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a42ce4c-a69f-4f01-8296-0ffe53337f14",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690b80ce-35b5-42db-ab98-6ba2f22a1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # 导入垃圾回收模块    Импорт модуля для управления сбором мусора\n",
    "\n",
    "# 清除缓存和显存的函数    Функция для очистки кэша и видеопамяти\n",
    "def clear_cache():\n",
    "    gc.collect()  # 清理 Python 的垃圾内存    Очистка мусора в Python\n",
    "    if torch.cuda.is_available():  # 如果 GPU 可用    Если доступен GPU\n",
    "        torch.cuda.empty_cache()  # 清理 GPU 的缓存    Очистка кэша GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "662cff4e-aff7-4b68-9b60-82ddfec61300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次运行训练代码前调用此函数    Вызываем эту функцию перед запуском каждого этапа тренировки\n",
    "clear_cache()  # 清理缓存    Очистка кэша\n",
    "\n",
    "# 定义模型的训练函数    Определение функции тренировки модели\n",
    "def train_model(data_loader, model, criterion, optimizer, num_epochs, teacher_forcing_ratio, decay_factor=0.8):\n",
    "    model.train()  # 设置模型为训练模式    Перевод модели в режим тренировки\n",
    "\n",
    "    # 开始迭代训练    Начало итерации тренировки\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # 总损失    Общая потеря\n",
    "        total_correct = 0  # 总正确预测    Общее количество правильных предсказаний\n",
    "        total_tokens = 0  # 总标记数量    Общее количество токенов\n",
    "\n",
    "        # 每个 epoch 后将教师强制比率逐渐降低    Уменьшаем коэффициент принудительного учителя после каждого этапа\n",
    "        current_teacher_forcing_ratio = teacher_forcing_ratio * (decay_factor ** epoch)\n",
    "\n",
    "        for batch in tqdm(data_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\"):  # 使用 tqdm 显示进度条\n",
    "            input_ids = batch['input_ids'].to(device)  # 输入序列    Входные последовательности\n",
    "            output_ids = batch['output_ids'].to(device)  # 输出序列    Выходные последовательности\n",
    "            attention_mask = batch['attention_mask'].to(device)  # 注意力掩码    Маска внимания\n",
    "\n",
    "            optimizer.zero_grad()  # 清除梯度    Очистка градиентов\n",
    "\n",
    "            # 通过模型进行前向传播    Прямое распространение через модель\n",
    "            predictions = model(input_ids, output_ids, current_teacher_forcing_ratio)\n",
    "\n",
    "            # 计算损失    Вычисление потерь\n",
    "            loss = criterion(predictions.view(-1, predictions.size(-1)), output_ids.view(-1))\n",
    "            loss.backward()  # 反向传播    Обратное распространение\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # 梯度裁剪    Обрезка градиента\n",
    "            optimizer.step()  # 更新模型参数    Обновление параметров модели\n",
    "\n",
    "            # 累积损失    Накопление потерь\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算准确率    Вычисление точности\n",
    "            predicted_tokens = predictions.argmax(-1)  # 获取每个时间步的预测标记    Получение предсказанных токенов на каждом шаге времени\n",
    "            correct = ((predicted_tokens == output_ids) * attention_mask).float().sum()  # 计算正确预测    Вычисление правильных предсказаний\n",
    "            total_correct += correct.item()\n",
    "            total_tokens += attention_mask.sum().item()\n",
    "\n",
    "        # 清理每个 epoch 后的缓存    Очистка кэша после каждого этапа\n",
    "        clear_cache()\n",
    "\n",
    "        # 计算并输出每个 epoch 的损失和准确率    Вычисление и вывод потерь и точности после каждого этапа\n",
    "        epoch_loss = total_loss / len(data_loader)  # 计算当前 epoch 的平均损失    Вычисление средней потери на этапе\n",
    "        epoch_accuracy = total_correct / total_tokens  # 计算当前 epoch 的准确率    Вычисление точности на этапе\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\\n Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4c3cadc-f7f8-4c28-aa8b-1a2501843d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(50260, 256)\n",
       "    (rnn): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(50260, 256)\n",
       "    (rnn): GRU(256, 256, batch_first=True)\n",
       "    (fc_out): Linear(in_features=256, out_features=50260, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设定训练参数    Установка параметров тренировки\n",
    "num_epochs = 2  # 训练的 epoch 数    Количество эпох тренировки\n",
    "learning_rate = 0.001  # 学习率    Коэффициент обучения\n",
    "teacher_forcing_ratio = 0.8  # 教师强制的比率    Коэффициент принудительного учителя\n",
    "\n",
    "# 损失函数和优化器    Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=50258)  # 使用交叉熵损失函数，忽略 padding 标记    Использование функции потерь кросс-энтропии с игнорированием токенов паддинга\n",
    "optimizer1 = optim.Adam(seq2seq_model.parameters(), lr=learning_rate)  # 使用 Adam 优化器    Использование оптимизатора Adam\n",
    "\n",
    "# 将模型移动到 GPU    Перенос модели на GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 检查是否可以使用 GPU    Проверка, доступен ли GPU\n",
    "seq2seq_model.to(device)  # 将模型加载到选择的设备    Перенос модели на выбранное устройство (GPU или CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ededee7-2162-4543-8d66-0f0796f7c8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: 100%|███████████████████████████████████████████████████████████████| 2874/2874 [6:45:59<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      " Loss: 5.0818, Accuracy: 0.2502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: 100%|███████████████████████████████████████████████████████████████| 2874/2874 [6:24:28<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n",
      " Loss: 4.0391, Accuracy: 0.3974\n"
     ]
    }
   ],
   "source": [
    "# 设定训练参数    Установка параметров тренировки\n",
    "num_epochs = 2  # 训练的 epoch 数    Количество эпох тренировки\n",
    "learning_rate = 0.001  # 学习率    Коэффициент обучения\n",
    "teacher_forcing_ratio = 0.8  # 教师强制的比率    Коэффициент принудительного учителя\n",
    "\n",
    "# 损失函数和优化器    Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=50258)  # 使用交叉熵损失函数，忽略 padding 标记    Использование функции потерь кросс-энтропии с игнорированием токенов паддинга\n",
    "optimizer1 = optim.Adam(seq2seq_model.parameters(), lr=learning_rate)  # 使用 Adam 优化器    Использование оптимизатора Adam\n",
    "\n",
    "# 将模型移动到 GPU    Перенос модели на GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 检查是否可以使用 GPU    Проверка, доступен ли GPU\n",
    "seq2seq_model.to(device)  # 将模型加载到选择的设备    Перенос модели на выбранное устройство (GPU или CPU)\n",
    "\n",
    "# 开始训练模型    Начало тренировки модели\n",
    "train_model(data_loader, seq2seq_model, criterion, optimizer1, num_epochs, teacher_forcing_ratio, decay_factor=0.8)  # 调用训练函数    Вызов функции тренировки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7d8b9-2874-471e-93ae-7ace824a16d5",
   "metadata": {},
   "source": [
    "# Add attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a644e71-cac4-4849-a7ea-0a60d82566e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器类    Определение класса кодировщика\n",
    "class EncoderWithAttention(nn.Module):\n",
    "    def __init__(self, units, n_tokens, n_stacks=1, bidirectional=False, dropout_rate=0.3, cell_type=nn.GRU, padding_idx=50258):\n",
    "        # 初始化编码器    Инициализация кодировщика\n",
    "        super(EncoderWithAttention, self).__init__()\n",
    "        # 定义嵌入层，用于将输入序列转为嵌入向量    Определение слоя встраивания для преобразования входных последовательностей в векторные представления\n",
    "        self.embedding = nn.Embedding(n_tokens, units, padding_idx=padding_idx)\n",
    "        # 定义 RNN 层（GRU），根据是否双向设置隐藏单元的数量    Определение RNN слоя (GRU), количество скрытых единиц зависит от направления (двусторонний или нет)\n",
    "        self.rnn = cell_type(units, units, num_layers=n_stacks, bidirectional=bidirectional, dropout=dropout_rate, batch_first=True)\n",
    "        # 保存 RNN 是否为双向的    Сохранение информации о том, является ли RNN двусторонним\n",
    "        self.bidirectional = bidirectional\n",
    "        # 保存隐藏单元的数量    Сохранение количества скрытых единиц\n",
    "        self.units = units\n",
    "        # 在 Encoder 和 Decoder 的 __init__ 方法中添加嵌入层初始化    Инициализация весов слоя встраивания в методах __init__ кодировщика и декодировщика\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # 获取输入序列，将其转为嵌入向量    Преобразование входной последовательности в векторные представления\n",
    "        embedded = self.embedding(input_seq)  # (batch_size, seq_len, units)\n",
    "        # 将嵌入向量输入到 RNN 层    Пропуск векторных представлений через RNN слой\n",
    "        encoder_outputs, hidden = self.rnn(embedded)\n",
    "        if encoder_outputs.size(-1) != hidden.size(-1):\n",
    "            encoder_outputs = encoder_outputs[:, :, :hidden.size(-1)]\n",
    "        \n",
    "        # 如果 RNN 是双向的，将最后一层的前向和后向隐藏状态拼接    Если RNN двусторонний, соединяем последние скрытые состояния прямого и обратного направлений\n",
    "        if self.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1).unsqueeze(0)\n",
    "        else:\n",
    "            hidden = hidden[-1].unsqueeze(0)\n",
    "\n",
    "        hidden = hidden[:, :, :self.units]  # 确保hidden维度与解码器一致    Обеспечиваем соответствие размеров hidden для декодера\n",
    "        # 确保返回的隐藏状态形状符合解码器的输入需求    Обеспечиваем, чтобы возвращаемое скрытое состояние соответствовало требованиям декодера\n",
    "        return encoder_outputs, hidden[:, :, :self.units]  # 返回编码器输出和隐藏状态    Возвращаем выводы кодировщика и скрытое состояние\n",
    "\n",
    "\n",
    "# 定义注意力机制类    Определение класса механизма внимания\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def dot_score(self, hidden_state, encoder_states):\n",
    "        # 计算注意力分数    Вычисление оценок внимания\n",
    "        return torch.sum(hidden_state * encoder_states, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # 计算注意力权重    Вычисление весов внимания\n",
    "        attention_scores = self.dot_score(hidden, encoder_outputs)\n",
    "        attention_scores = attention_scores.t()\n",
    "\n",
    "        # 使用掩码，避免网络关注 <pad> tokens    Применение маски, чтобы избежать внимания к <pad> токенам\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, -1e5)\n",
    "\n",
    "        # 返回注意力分数的softmax    Возвращаем softmax от оценок внимания\n",
    "        return F.softmax(attention_scores, dim=1).unsqueeze(1)\n",
    "\n",
    "\n",
    "# 定义解码器类    Определение класса декодировщика\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, units, n_labels, attention_dim, embedding_dim, cell_type=nn.GRU):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        \n",
    "        # 定义嵌入层    Определение слоя встраивания\n",
    "        self.embedding = nn.Embedding(n_labels, embedding_dim)\n",
    "        # 定义RNN层（GRU）    Определение RNN слоя (GRU)\n",
    "        self.rnn = cell_type(embedding_dim, units, batch_first=True)\n",
    "        # 定义输出层    Определение слоя для выхода\n",
    "        self.fc_out = nn.Linear(units * 2, n_labels)\n",
    "        \n",
    "        # 初始化嵌入层的权重    Инициализация весов слоя встраивания\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # 初始化注意力机制    Инициализация механизма внимания\n",
    "        self.attn = Attention(units)  # 初始化注意力机制    Инициализация механизма внимания\n",
    "        # 定义上下文向量的线性转换层    Определение линейного преобразования контекстного вектора\n",
    "        self.context_linear = nn.Linear(units, units)\n",
    "\n",
    "    def forward(self, decoder_input, hidden, encoder_outputs, mask):\n",
    "        # 获取解码器输入并嵌入    Получаем входные данные для декодера и преобразуем их в векторное представление\n",
    "        decoder_input_embedded = self.embedding(decoder_input)\n",
    "        \n",
    "        hidden = hidden.contiguous()  # 确保隐藏状态是连续的    Обеспечиваем, чтобы скрытое состояние было непрерывным\n",
    "        output, hidden = self.rnn(decoder_input_embedded, hidden)\n",
    "        \n",
    "        # 计算注意力权重    Вычисление весов внимания\n",
    "        attention_weights = self.attn(output, encoder_outputs, mask)  # [batch_size, 1, seq_len]\n",
    "        \n",
    "        # 计算上下文向量    Вычисление контекстного вектора\n",
    "        context = encoder_outputs.transpose(1, 2).bmm(attention_weights.permute(2, 0, 1))\n",
    "        \n",
    "        # 将上下文向量 squeeze 以匹配 output    Сжимаем контекстный вектор для соответствия output\n",
    "        context = context.squeeze(2)  # [batch_size, hidden_dim]\n",
    "        output = output.squeeze(1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # 连接 output 和 context 并生成预测    Объединяем output и context и генерируем предсказания\n",
    "        concat_input = torch.cat((output, context), 1)\n",
    "        prediction = self.fc_out(concat_input)\n",
    "\n",
    "        return prediction, hidden  # 返回预测和隐藏状态    Возвращаем предсказания и скрытое состояние\n",
    "\n",
    "\n",
    "# 定义序列到序列模型类    Определение класса Seq2Seq модели\n",
    "class Seq2SeqModelWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqModelWithAttention, self).__init__()\n",
    "        self.encoder = encoder  # 编码器    Кодировщик\n",
    "        self.decoder = decoder  # 解码器    Декодировщик\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing_ratio):\n",
    "        encoder_outputs, hidden = self.encoder(encoder_input)  # 编码输入序列    Кодируем входную последовательность\n",
    "    \n",
    "        # 先检查 padding_idx 是否是一个 tensor 类型    Сначала проверяем, является ли padding_idx тензором\n",
    "        padding_idx = torch.tensor(self.encoder.embedding.padding_idx, dtype=torch.long, device=encoder_input.device)\n",
    "    \n",
    "        # 获取 padding_idx 并创建掩码    Получаем padding_idx и создаем маску\n",
    "        mask = (encoder_input != padding_idx)  # 比较是否为 padding_idx    Проверяем, является ли токен padding_idx\n",
    "        mask = mask.float()  # 转换为浮动类型的张量    Преобразуем в тензор с плавающей точкой\n",
    "        mask = mask.to(encoder_input.device)  # 确保掩码在相同的设备上    Обеспечиваем, чтобы маска была на том же устройстве\n",
    "        mask = mask.permute(1, 0)  # 转置为 (seq_len, batch_size) 以匹配注意力层的要求    Переводим маску в (seq_len, batch_size), чтобы соответствовать требованиям слоя внимания\n",
    "    \n",
    "        outputs = torch.zeros(decoder_input.size(0), decoder_input.size(1), self.decoder.fc_out.out_features).to(decoder_input.device)  # 初始化输出序列    Инициализируем выходную последовательность\n",
    "    \n",
    "        input_token = decoder_input[:, 0].unsqueeze(1)  # 设置解码器的第一个输入标记为 <BOS>    Устанавливаем первый токен для декодера как <BOS>\n",
    "    \n",
    "        for t in range(1, decoder_input.size(1)):\n",
    "            output, hidden = self.decoder(input_token, hidden, encoder_outputs, mask)  # 解码器输出    Выход декодера\n",
    "            outputs[:, t, :] = output  # 存储当前时间步的输出    Сохраняем выход на текущем шаге времени\n",
    "    \n",
    "            # 使用教师强制决定下一个输入    Используем принудительное обучение для выбора следующего входа\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)  # 获取预测标记    Получаем предсказанный токен\n",
    "            input_token = decoder_input[:, t].unsqueeze(1) if teacher_force else top1  # 使用教师强制或预测作为下一个输入    Используем принудительное обучение или предсказание как следующий вход\n",
    "    \n",
    "        return outputs  # 返回解码器的所有输出    Возвращаем все выходы декодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad6b17fb-9295-4866-a433-f1f6df38db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqModelWithAttention(\n",
      "  (encoder): EncoderWithAttention(\n",
      "    (embedding): Embedding(50260, 256, padding_idx=50258)\n",
      "    (rnn): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): DecoderWithAttention(\n",
      "    (embedding): Embedding(50260, 256)\n",
      "    (rnn): GRU(256, 256, batch_first=True)\n",
      "    (fc_out): Linear(in_features=512, out_features=50260, bias=True)\n",
      "    (attn): Attention()\n",
      "    (context_linear): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 设置超参数    Установка гиперпараметров\n",
    "units = 256  # 隐藏单元的数量    Количество скрытых единиц\n",
    "n_tokens = len(tokenizer.get_vocab())  # 输入词典大小    Размер словаря для входных данных\n",
    "n_labels = len(tokenizer.get_vocab())  # 输出词典大小    Размер словаря для выходных данных\n",
    "n_stacks = 3  # 编码器的层数    Количество слоев в кодировщике\n",
    "\n",
    "# 初始化编码器，传入词典大小、隐藏单元数量、层数等参数    \n",
    "# Инициализация кодировщика с параметрами размерности словаря, скрытых единиц и слоев\n",
    "EncoderWithAttention = EncoderWithAttention(\n",
    "    units=units,  # 隐藏单元的数量    Количество скрытых единиц\n",
    "    n_tokens=n_tokens,  # 输入词典大小    Размер словаря для входных данных\n",
    "    n_stacks=n_stacks,  # 编码器层数    Количество слоев в кодировщике\n",
    "    bidirectional=True,  # 是否双向    Используется ли двусторонний RNN\n",
    "    cell_type=nn.GRU,  # RNN的类型    Тип RNN (GRU)\n",
    "    padding_idx=50258  # 填充标记的索引    Индекс токена для дополнения\n",
    ")\n",
    "\n",
    "# 初始化解码器，传入隐藏单元数量、输出词典大小、层数等参数    \n",
    "# Инициализация декодировщика с параметрами скрытых единиц, размерности словаря и слоев\n",
    "# 设定 attention_dim 和 embedding_dim 与 units 一致    Устанавливаем attention_dim и embedding_dim равными units\n",
    "DecoderWithAttention = DecoderWithAttention(\n",
    "    units=units,  # 隐藏单元的数量    Количество скрытых единиц\n",
    "    n_labels=n_labels,  # 输出词典大小    Размер словаря для выходных данных\n",
    "    attention_dim=units,  # 注意力层的维度    Размерность слоя внимания\n",
    "    embedding_dim=units,  # 嵌入层的维度    Размерность слоя встраивания\n",
    "    cell_type=nn.GRU  # RNN 的类型    Тип RNN (GRU)\n",
    ")\n",
    "\n",
    "# 初始化 seq2seq_model_WithAttention，将编码器和解码器实例传入    \n",
    "# Инициализация модели seq2seq_model_WithAttention, передаем экземпляры кодировщика и декодировщика\n",
    "seq2seq_model_WithAttention = Seq2SeqModelWithAttention(\n",
    "    encoder=EncoderWithAttention,  # 编码器    Кодировщик\n",
    "    decoder=DecoderWithAttention  # 解码器    Декодировщик\n",
    ")\n",
    "\n",
    "# 打印模型结构    Вывод структуры модели\n",
    "print(seq2seq_model_WithAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9c3eb2-3703-442a-ba80-968b53d1bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型的训练函数    Определение функции обучения модели\n",
    "def train_model_with_attention(data_loader, model, criterion, optimizer, num_epochs, teacher_forcing_ratio, decay_factor=0.8):\n",
    "    model.train()  # 设置模型为训练模式    Устанавливаем модель в режим обучения\n",
    "\n",
    "    # 开始迭代训练    Начало цикла обучения\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0  # 初始化总损失    Инициализация общей потери\n",
    "        total_correct = 0  # 初始化总正确预测数    Инициализация общего количества правильных предсказаний\n",
    "        total_tokens = 0  # 初始化总标记数    Инициализация общего числа токенов\n",
    "\n",
    "        # 每个 epoch 后将教师强制比率逐渐降低    Понижение коэффициента принудительного обучения после каждого эпоха\n",
    "        current_teacher_forcing_ratio = teacher_forcing_ratio * (decay_factor ** epoch)\n",
    "\n",
    "        # 遍历数据加载器中的每个批次    Перебор каждого батча в загрузчике данных\n",
    "        for batch in tqdm(data_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\"):\n",
    "            input_ids = batch['input_ids'].to(device)  # 输入序列（来源于数据加载器）    Входные последовательности (из загрузчика данных)\n",
    "            output_ids = batch['output_ids'].to(device)  # 输出序列（真实目标序列）    Выходные последовательности (истинные метки)\n",
    "            attention_mask = batch['attention_mask'].to(device)  # 注意力掩码（用于忽略 padding 部分）    Массив маски внимания (для игнорирования паддинга)\n",
    "\n",
    "            optimizer.zero_grad()  # 清除上一轮的梯度    Обнуляем градиенты с предыдущего шага\n",
    "\n",
    "            # 通过模型进行前向传播，生成预测    Прогоняем данные через модель для получения предсказаний\n",
    "            predictions = model(input_ids, output_ids, current_teacher_forcing_ratio)\n",
    "\n",
    "            # 计算损失函数：将预测的 logits 展平，然后计算与真实标签的交叉熵损失    Вычисление потерь: преобразуем предсказанные логи и вычисляем кросс-энтропию\n",
    "            loss = criterion(predictions.view(-1, predictions.size(-1)), output_ids.view(-1))\n",
    "            loss.backward()  # 反向传播计算梯度    Обратное распространение для вычисления градиентов\n",
    "\n",
    "            # 梯度裁剪 Обрезка градиентов\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # 防止梯度爆炸    Чтобы избежать взрыва градиентов\n",
    "\n",
    "            optimizer.step()  # 更新模型参数    Обновление параметров модели\n",
    "\n",
    "            # 累积损失    Накопление потерь\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 计算准确率    Вычисление точности\n",
    "            predicted_tokens = predictions.argmax(-1)  # 获取每个时间步的预测标记    Получаем предсказания для каждого временного шага\n",
    "            correct = ((predicted_tokens == output_ids) * attention_mask).float().sum()  # 计算正确预测的标记数    Подсчитываем количество правильных предсказанных токенов\n",
    "            total_correct += correct.item()  # 累加正确的标记数    Накопление правильных предсказаний\n",
    "            total_tokens += attention_mask.sum().item()  # 累加有效标记的总数（忽略 padding）    Накопление общего числа токенов (игнорируя паддинг)\n",
    "\n",
    "        # 每个 epoch 结束后清理缓存    Очистка кэша после каждого эпоха\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 计算并输出每个 epoch 的平均损失和准确率    Вычисление и вывод средней потери и точности для каждого эпоха\n",
    "        epoch_loss = total_loss / len(data_loader)  # 计算平均损失    Вычисление средней потери\n",
    "        epoch_accuracy = total_correct / total_tokens  # 计算准确率    Вычисление точности\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\\n Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "970cbd7a-d465-48b2-8323-b0ef371ecb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqModelWithAttention(\n",
       "  (encoder): EncoderWithAttention(\n",
       "    (embedding): Embedding(50260, 256, padding_idx=50258)\n",
       "    (rnn): GRU(256, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (decoder): DecoderWithAttention(\n",
       "    (embedding): Embedding(50260, 256)\n",
       "    (rnn): GRU(256, 256, batch_first=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=50260, bias=True)\n",
       "    (attn): Attention()\n",
       "    (context_linear): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 损失函数和优化器    Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=50258)  # 定义交叉熵损失函数，忽略 padding 的 token    Определение функции потерь кросс-энтропии, игнорируя токены паддинга\n",
    "optimizer2 = optim.Adam(seq2seq_model_WithAttention.parameters(), lr=learning_rate)  # 使用 Adam 优化器    Использование оптимизатора Adam\n",
    "\n",
    "# 将模型转入设备    Перемещаем модель на устройство (GPU или CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 判断是否有可用的 GPU，如果有则使用 GPU，否则使用 CPU    Проверяем, доступен ли GPU, если да, используем его, иначе используем CPU\n",
    "seq2seq_model_WithAttention.to(device)  # 将模型转移到指定的设备    Перемещаем модель на выбранное устройство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d06fc7-65aa-4493-bed0-4330aa73b2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]: 100%|███████████████████████████████████████████████████████████████| 2874/2874 [7:48:15<00:00,  9.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      " Loss: 5.2387, Accuracy: 0.2427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]: 100%|███████████████████████████████████████████████████████████████| 2874/2874 [6:41:01<00:00,  8.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2]\n",
      " Loss: 4.4261, Accuracy: 0.3493\n"
     ]
    }
   ],
   "source": [
    "# 损失函数和优化器    Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=50258)  # 定义交叉熵损失函数，忽略 padding 的 token    Определение функции потерь кросс-энтропии, игнорируя токены паддинга\n",
    "optimizer2 = optim.Adam(seq2seq_model_WithAttention.parameters(), lr=learning_rate)  # 使用 Adam 优化器    Использование оптимизатора Adam\n",
    "\n",
    "# 将模型转入设备    Перемещаем модель на устройство (GPU или CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 判断是否有可用的 GPU，如果有则使用 GPU，否则使用 CPU    Проверяем, доступен ли GPU, если да, используем его, иначе используем CPU\n",
    "seq2seq_model_WithAttention.to(device)  # 将模型转移到指定的设备    Перемещаем модель на выбранное устройство\n",
    "\n",
    "# 每次运行训练代码前调用此函数    Вызываем эту функцию перед запуском обучения\n",
    "clear_cache()  # 清理缓存    Очистка кэша\n",
    "\n",
    "# 训练模型    Обучаем модель\n",
    "train_model_with_attention(data_loader, seq2seq_model_WithAttention, criterion, optimizer2, num_epochs, teacher_forcing_ratio, decay_factor=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219673e-cba6-48e4-9283-2b21389a2c8f",
   "metadata": {},
   "source": [
    "# Continue Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca3ea35b-e7f2-408f-841c-5c6bd361b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer saved.\n"
     ]
    }
   ],
   "source": [
    "# 保存 seq2seq_model 和优化器的状态\n",
    "torch.save({\n",
    "    'model_state_dict': seq2seq_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer1.state_dict(),\n",
    "}, 'seq2seq_model.pth')\n",
    "\n",
    "print(\"Model and optimizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0412b723-44ee-4157-81b8-15806d354117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]: 100%|███████████████████████████████████████████████████████████████████████| 2874/2874 [7:29:04<00:00,  9.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n",
      " Loss: 3.3028, Accuracy: 0.4971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]: 100%|███████████████████████████████████████████████████████████████████████| 2874/2874 [6:05:30<00:00,  7.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]\n",
      " Loss: 3.1887, Accuracy: 0.5162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]: 100%|███████████████████████████████████████████████████████████████████████| 2874/2874 [7:36:58<00:00,  9.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]\n",
      " Loss: 3.1551, Accuracy: 0.5198\n",
      "Model loaded and training resumed.\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和优化器的状态\n",
    "checkpoint = torch.load('seq2seq_model.pth')\n",
    "seq2seq_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer1.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# 将模型移动到设备\n",
    "seq2seq_model.to(device)\n",
    "\n",
    "# 继续训练\n",
    "num_epochs_plus = 3\n",
    "train_model(data_loader, seq2seq_model, criterion, optimizer1, num_epochs_plus, teacher_forcing_ratio, decay_factor=0.8)\n",
    "\n",
    "print(\"Model loaded and training resumed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84630ca3-fac5-4857-8556-d46b77ffc8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqWithAttention model and optimizer saved.\n"
     ]
    }
   ],
   "source": [
    "# 保存 seq2seq_model_WithAttention 和优化器的状态\n",
    "torch.save({\n",
    "    'model_state_dict': seq2seq_model_WithAttention.state_dict(),\n",
    "    'optimizer_state_dict': optimizer2.state_dict(),\n",
    "}, 'seq2seq_model_with_attention.pth')\n",
    "\n",
    "print(\"Seq2SeqWithAttention model and optimizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "736e1e59-3d25-4b61-bb03-f6c1b2a01ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]: 100%|███████████████████████████████████████████████████████████████████████| 2874/2874 [8:25:04<00:00, 10.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n",
      " Loss: 3.6719, Accuracy: 0.4420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]: 100%|███████████████████████████████████████████████████████████████████████| 2874/2874 [8:34:29<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3]\n",
      " Loss: 3.4385, Accuracy: 0.4763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]: 100%|███████████████████████████████████████████████████████████████████████| 2874/2874 [7:47:11<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3]\n",
      " Loss: 3.3380, Accuracy: 0.4895\n",
      "Seq2SeqWithAttention model loaded and training resumed.\n"
     ]
    }
   ],
   "source": [
    "# 加载 seq2seq_model_WithAttention 和优化器的状态\n",
    "checkpoint_attention = torch.load('seq2seq_model_with_attention.pth')\n",
    "seq2seq_model_WithAttention.load_state_dict(checkpoint_attention['model_state_dict'])\n",
    "optimizer2.load_state_dict(checkpoint_attention['optimizer_state_dict'])\n",
    "\n",
    "# 将模型移动到设备\n",
    "seq2seq_model_WithAttention.to(device)\n",
    "\n",
    "# 继续训练\n",
    "train_model(data_loader, seq2seq_model_WithAttention, criterion, optimizer2, num_epochs_plus, teacher_forcing_ratio, decay_factor=0.8)\n",
    "\n",
    "print(\"Seq2SeqWithAttention model loaded and training resumed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bdd9952-a91b-4a59-8134-3f78a72f71d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq model and optimizer saved after two trainings.\n"
     ]
    }
   ],
   "source": [
    "# 保存 seq2seq_model 和 optimizer1 的状态\n",
    "torch.save({\n",
    "    'model_state_dict': seq2seq_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer1.state_dict(),\n",
    "}, 'seq2seq_model2.pth')\n",
    "\n",
    "print(\"Seq2Seq model and optimizer saved after two trainings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c74af91-b0cc-414d-838e-ba6a14ffffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqWithAttention model and optimizer saved after two trainings.\n"
     ]
    }
   ],
   "source": [
    "# 保存 seq2seq_model_WithAttention 和 optimizer2 的状态\n",
    "torch.save({\n",
    "    'model_state_dict': seq2seq_model_WithAttention.state_dict(),\n",
    "    'optimizer_state_dict': optimizer2.state_dict(),\n",
    "}, 'seq2seq_model_with_attention2.pth')\n",
    "\n",
    "print(\"Seq2SeqWithAttention model and optimizer saved after two trainings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdbab493-2359-4c8e-b07f-e54bebefea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Seq2SeqModel\n",
    "checkpoint = torch.load('seq2seq_model2.pth')  # 加载保存的检查点\n",
    "seq2seq_model = Seq2SeqModel(encoder=encoder, decoder=decoder)  # 将实例化的 encoder 和 decoder 传入模型\n",
    "# 加载模型权重\n",
    "seq2seq_model.load_state_dict(checkpoint['model_state_dict'])  # 加载保存的模型权重\n",
    "seq2seq_model.eval()  # 切换到评估模式\n",
    "# 加载优化器状态\n",
    "optimizer1.load_state_dict(checkpoint['optimizer_state_dict'])  # 恢复优化器的状态\n",
    "\n",
    "\n",
    "# 加载 Seq2SeqAttentionModel\n",
    "checkpoint_attention = torch.load('seq2seq_model_with_attention2.pth')  # 加载保存的检查点\n",
    "seq2seq_attention_model = Seq2SeqModelWithAttention(encoder=EncoderWithAttention, decoder=DecoderWithAttention)\n",
    "# 加载模型权重\n",
    "seq2seq_attention_model.load_state_dict(checkpoint_attention['model_state_dict'])  # 加载保存的模型权重\n",
    "seq2seq_attention_model.eval()  # 切换到评估模式\n",
    "# 加载优化器状态\n",
    "optimizer2.load_state_dict(checkpoint_attention['optimizer_state_dict'])  # 恢复优化器的状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8727a-a18a-4b42-b967-37825da48f16",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b5aff5a3-9f85-4c2a-a659-2ecfde38858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例文本列表 - Пример списка текстов\n",
    "sample_texts = [\n",
    "    \"Давайте что-нибудь попробуем!\",  # 让我们试试吧！\n",
    "    \"Мне пора идти спать.\",           # 我该睡觉了。\n",
    "    \"Что ты делаешь?\",                # 你在做什么？\n",
    "    \"Что ты делаешь?\",                # 你在做什么？\n",
    "    \"Собака бегает по траве\",         # 狗在草地上奔跑\n",
    "    \"Небо и море - все синее.\"        # 天空和海洋全都是蓝色\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6854626b-64e6-4655-9fc5-f5829855be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(\n",
    "    text: str,  # 输入文本 - Входной текст\n",
    "    tokenizer: GPT2Tokenizer,  # 自定义分词器 - Индивидуальный токенизатор\n",
    "    model: torch.nn.Module,  # 已训练的模型 - Обученная модель\n",
    "    max_len: int = 48,  # 最大输出长度 - Максимальная длина вывода\n",
    "    confidence_threshold: float = 0.3,  # 置信度阈值 - Порог уверенности\n",
    "    low_confidence_limit: int = 15  # 连续低置信度 token 数限制 - Ограничение для токенов с низкой уверенностью\n",
    ") -> str:\n",
    "    '''使用模型预测文本的翻译 - Предсказать перевод текста с помощью модели'''\n",
    "    \n",
    "    # 将输入文本转换为模型输入格式 - Преобразуем текст в формат, подходящий для модели\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 设置模型为评估模式 - Установка модели в режим оценки\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # 关闭梯度计算，不影响模型权重 - Отключение расчета градиентов\n",
    "        # 初始化输出 ID 序列，以 <BOS> 作为起始 token - Инициализация выходной последовательности ID, начиная с токена <BOS>\n",
    "        output_ids = torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long, device=device)\n",
    "\n",
    "        # 连续低置信度 token 计数器 - Счетчик подряд идущих токенов с низкой уверенностью\n",
    "        low_confidence_count = 0\n",
    "\n",
    "        for _ in range(max_len):  # 限制生成最大长度为 max_len - Ограничиваем максимальную длину генерации\n",
    "            if isinstance(model, Seq2SeqModel):  # 如果是 Seq2SeqModel 模型 - Если это модель Seq2SeqModel\n",
    "                # 对于 Seq2SeqModel, 只需要 input_ids 和 output_ids - Для Seq2SeqModel нужно только input_ids и output_ids\n",
    "                outputs = model(input_ids, output_ids)\n",
    "            else:  # 对于带有注意力机制的 Seq2SeqModelWithAttention 模型 - Для Seq2SeqModelWithAttention\n",
    "                # 对于带有注意力机制的模型，传入 teacher_forcing_ratio 参数 - Для модели с вниманием, передаем параметр teacher_forcing_ratio\n",
    "                teacher_forcing_ratio = 0  # 可以根据需要修改这个值 - Можем изменить это значение по необходимости\n",
    "                outputs = model(input_ids, output_ids, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "            # 获取最后一个 token 的预测分布 - Получаем распределение вероятностей для последнего токена\n",
    "            next_token_logits = outputs[:, -1, :]\n",
    "\n",
    "            # 应用 softmax 获取分布概率 - Применяем softmax для получения распределения вероятностей\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # 获取概率最高的 token 和对应的概率 - Получаем токен с наивысшей вероятностью и его вероятность\n",
    "            next_token_prob, next_token_id = next_token_probs.max(dim=-1)\n",
    "\n",
    "            # 检查置信度是否低于阈值 - Проверяем, ниже ли уверенность порогового значения\n",
    "            if next_token_prob.item() < confidence_threshold:\n",
    "                low_confidence_count += 1\n",
    "                # 如果连续低置信度 token 数达到限制，则停止生成 - Останавливаем генерацию, если количество низкоуверенных токенов достигает предела\n",
    "                if low_confidence_count >= low_confidence_limit:\n",
    "                    break\n",
    "            else:\n",
    "                low_confidence_count = 0  # 重置低置信度计数器 - Сброс счетчика низкой уверенности\n",
    "\n",
    "            # 将预测的 token 追加到输出序列 - Добавляем предсказанный токен в выходную последовательность\n",
    "            output_ids = torch.cat([output_ids, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # 如果生成了 <EOS>，则停止生成 - Останавливаем генерацию, если был сгенерирован <EOS>\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # 使用分词器将生成的 ID 序列解码为文本 - Декодируем сгенерированную последовательность ID в текст с помощью токенизатора\n",
    "    translation = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "    # 后处理：去除多余的标点符号 - Обработка после перевода: удаляем лишние знаки препинания\n",
    "    def clean_translation(translation):\n",
    "        translation = re.sub(r'([!?.])\\1+', r'\\1', translation)  # 去掉重复的标点符号 - Убираем повторяющиеся знаки препинания\n",
    "        return translation\n",
    "    \n",
    "    cleaned_translation = clean_translation(translation)  # 清理翻译结果 - Очищаем результат перевода\n",
    "\n",
    "    return cleaned_translation  # 返回翻译 - Возвращаем перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "266c9684-d7be-4a99-a21e-eb3024dd02a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Давайте что-нибудь попробуем!\n",
      "Translation: !Let's's try something.\n",
      "Original: Мне пора идти спать.\n",
      "Translation: !I time to to go bed early early.\n",
      "Original: Что ты делаешь?\n",
      "Translation: !What are you do doing?.\".\".\".\".\".\n",
      "Original: Что ты делаешь?\n",
      "Translation: !What do you you? doing?.\".\".\".\".\".\"\n",
      "Original: Собака бегает по траве\n",
      "Translation: !The dog dog is is running.\n",
      "Original: Небо и море - все синее.\n",
      "Translation: !The sky is, blue the red blue one.\n"
     ]
    }
   ],
   "source": [
    "# 对每个文本进行翻译并打印结果 - Переводим каждый текст и выводим результат\n",
    "for text in sample_texts:\n",
    "    translation = translate(text, tokenizer, seq2seq_model)  # 使用模型进行翻译 - Переводим текст с помощью модели\n",
    "    print(f\"Original: {text}\")  # 原始文本 - Оригинальный текст\n",
    "    print(f\"Translation: {translation}\")  # 翻译文本 - Перевод текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "92528e7b-4a82-42d7-b52f-7e6287522c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateAtt(text, tokenizer, model, max_len=50, confidence_threshold=0.5, low_confidence_limit=5):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    output_ids = torch.full((input_ids.size(0), 1), tokenizer.bos_token_id).to(device)\n",
    "\n",
    "    translation = []\n",
    "    teacher_forcing_ratio = 0.5\n",
    "\n",
    "    for t in range(max_len):\n",
    "        model.eval()\n",
    "\n",
    "        if isinstance(model, Seq2SeqModelWithAttention):\n",
    "            outputs = model(input_ids, output_ids, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, attention_weights = outputs\n",
    "            else:\n",
    "                attention_weights = None\n",
    "            # 打印 attention_weights 和 outputs 来检查它们是否有变化\n",
    "            print(f\"Step {t}:\")\n",
    "            print(f\"Attention Weights: {attention_weights}\")\n",
    "            print(f\"Outputs: {outputs}\")\n",
    "        else:\n",
    "            outputs = model(input_ids, output_ids)\n",
    "            attention_weights = None\n",
    "\n",
    "        predicted_token = outputs[:, -1, :].argmax(dim=-1)\n",
    "        translation.append(predicted_token.item())\n",
    "\n",
    "        if predicted_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        output_ids = torch.cat((output_ids, predicted_token.unsqueeze(-1)), dim=1)\n",
    "\n",
    "    translated_text = tokenizer.decode(translation, skip_special_tokens=True)\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9c0c48d1-473f-46b5-8a37-4b650e89d0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Step 1:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 2:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 3:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         [ 419.8440, -117.9332, -351.3718,  ..., -250.2015, -257.5630,\n",
      "          -250.9971]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 4:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         [ 420.4836, -117.4440, -351.5691,  ..., -249.6613, -257.1068,\n",
      "          -250.4766],\n",
      "         [ 421.8637, -116.2344, -352.1594,  ..., -249.9984, -257.3076,\n",
      "          -250.7017]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 5:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         [ 420.4836, -117.4440, -351.5691,  ..., -249.6613, -257.1068,\n",
      "          -250.4766],\n",
      "         [ 421.8637, -116.2344, -352.1594,  ..., -249.9984, -257.3076,\n",
      "          -250.7017],\n",
      "         [ 422.3801, -116.2594, -352.4301,  ..., -250.2451, -257.5122,\n",
      "          -250.8690]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 6:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 421.8137, -116.8228, -352.3792,  ..., -250.5636, -257.8265,\n",
      "          -251.2237],\n",
      "         [ 422.3827, -116.6865, -352.6020,  ..., -250.8468, -258.0964,\n",
      "          -251.4066],\n",
      "         [ 422.4230, -116.9420, -352.8069,  ..., -250.6358, -257.9179,\n",
      "          -251.1907]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 7:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 422.3801, -116.2594, -352.4301,  ..., -250.2451, -257.5122,\n",
      "          -250.8690],\n",
      "         [ 422.2615, -116.4844, -352.6870,  ..., -250.3753, -257.6618,\n",
      "          -250.9989],\n",
      "         [ 421.9858, -116.4133, -352.8596,  ..., -250.5361, -257.8482,\n",
      "          -251.2110]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 8:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 422.2615, -116.4844, -352.6870,  ..., -250.3753, -257.6618,\n",
      "          -250.9989],\n",
      "         [ 421.9858, -116.4133, -352.8596,  ..., -250.5361, -257.8482,\n",
      "          -251.2110],\n",
      "         [ 421.6536, -116.2112, -352.9798,  ..., -250.7551, -258.0759,\n",
      "          -251.4853]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 9:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 422.2595, -116.8495, -352.9898,  ..., -250.6192, -257.9550,\n",
      "          -251.2401],\n",
      "         [ 421.9606, -116.6013, -353.1485,  ..., -250.8153, -258.1835,\n",
      "          -251.5090],\n",
      "         [ 421.6356, -116.3517, -353.2763,  ..., -251.0387, -258.4120,\n",
      "          -251.7861]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 10:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 421.6536, -116.2112, -352.9798,  ..., -250.7551, -258.0759,\n",
      "          -251.4853],\n",
      "         [ 421.3083, -116.0084, -353.0630,  ..., -250.9643, -258.2814,\n",
      "          -251.7390],\n",
      "         [ 420.9917, -115.8377, -353.1152,  ..., -251.1481, -258.4586,\n",
      "          -251.9586]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 11:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 421.6356, -116.3517, -353.2763,  ..., -251.0387, -258.4120,\n",
      "          -251.7861],\n",
      "         [ 421.3384, -116.1589, -353.3564,  ..., -251.2332, -258.6015,\n",
      "          -252.0200],\n",
      "         [ 421.0783, -116.0192, -353.3958,  ..., -251.3967, -258.7579,\n",
      "          -252.2144]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 12:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 421.3384, -116.1589, -353.3564,  ..., -251.2332, -258.6015,\n",
      "          -252.0200],\n",
      "         [ 421.0783, -116.0192, -353.3958,  ..., -251.3967, -258.7579,\n",
      "          -252.2144],\n",
      "         [ 420.8506, -115.9192, -353.4056,  ..., -251.5311, -258.8835,\n",
      "          -252.3731]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 13:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 421.0783, -116.0192, -353.3958,  ..., -251.3967, -258.7579,\n",
      "          -252.2144],\n",
      "         [ 420.8506, -115.9192, -353.4056,  ..., -251.5311, -258.8835,\n",
      "          -252.3731],\n",
      "         [ 420.6505, -115.8462, -353.3944,  ..., -251.6386, -258.9801,\n",
      "          -252.4991]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 14:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.4809, -115.6037, -353.1491,  ..., -251.4353, -258.7292,\n",
      "          -252.2970],\n",
      "         [ 420.2823, -115.5278, -353.1422,  ..., -251.5406, -258.8234,\n",
      "          -252.4192],\n",
      "         [ 420.1192, -115.4707, -353.1264,  ..., -251.6260, -258.8960,\n",
      "          -252.5172]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 15:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.2823, -115.5278, -353.1422,  ..., -251.5406, -258.8234,\n",
      "          -252.4192],\n",
      "         [ 420.1192, -115.4707, -353.1264,  ..., -251.6260, -258.8960,\n",
      "          -252.5172],\n",
      "         [ 419.9914, -115.4283, -353.1069,  ..., -251.6974, -258.9530,\n",
      "          -252.5973]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 16:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.4765, -115.7913, -353.3696,  ..., -251.7235, -259.0518,\n",
      "          -252.5981],\n",
      "         [ 420.3296, -115.7497, -353.3372,  ..., -251.7918, -259.1052,\n",
      "          -252.6767],\n",
      "         [ 420.2109, -115.7186, -353.3026,  ..., -251.8490, -259.1460,\n",
      "          -252.7414]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 17:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.3296, -115.7497, -353.3372,  ..., -251.7918, -259.1052,\n",
      "          -252.6767],\n",
      "         [ 420.2109, -115.7186, -353.3026,  ..., -251.8490, -259.1460,\n",
      "          -252.7414],\n",
      "         [ 420.1208, -115.6959, -353.2699,  ..., -251.8993, -259.1792,\n",
      "          -252.7965]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 18:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 419.8976, -115.3973, -353.0880,  ..., -251.7594, -258.9996,\n",
      "          -252.6646],\n",
      "         [ 419.8350, -115.3752, -353.0724,  ..., -251.8152, -259.0396,\n",
      "          -252.7228],\n",
      "         [ 419.7992, -115.3602, -353.0619,  ..., -251.8668, -259.0752,\n",
      "          -252.7744]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 19:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.1208, -115.6959, -353.2699,  ..., -251.8993, -259.1792,\n",
      "          -252.7965],\n",
      "         [ 420.0579, -115.6800, -353.2417,  ..., -251.9455, -259.2080,\n",
      "          -252.8455],\n",
      "         [ 420.0195, -115.6695, -353.2194,  ..., -251.9893, -259.2345,\n",
      "          -252.8901]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 20:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 419.7992, -115.3602, -353.0619,  ..., -251.8668, -259.0752,\n",
      "          -252.7744],\n",
      "         [ 419.7849, -115.3508, -353.0570,  ..., -251.9152, -259.1079,\n",
      "          -252.8209],\n",
      "         [ 419.7869, -115.3461, -353.0573,  ..., -251.9608, -259.1385,\n",
      "          -252.8631]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 21:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.0195, -115.6695, -353.2194,  ..., -251.9893, -259.2345,\n",
      "          -252.8901],\n",
      "         [ 420.0016, -115.6634, -353.2032,  ..., -252.0313, -259.2597,\n",
      "          -252.9315],\n",
      "         [ 419.9999, -115.6609, -353.1932,  ..., -252.0719, -259.2843,\n",
      "          -252.9701]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 22:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 419.7869, -115.3461, -353.0573,  ..., -251.9608, -259.1385,\n",
      "          -252.8631],\n",
      "         [ 419.8004, -115.3450, -353.0622,  ..., -252.0038, -259.1671,\n",
      "          -252.9015],\n",
      "         [ 419.8216, -115.3471, -353.0706,  ..., -252.0439, -259.1937,\n",
      "          -252.9364]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 23:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 419.9999, -115.6609, -353.1932,  ..., -252.0719, -259.2843,\n",
      "          -252.9701],\n",
      "         [ 420.0103, -115.6613, -353.1885,  ..., -252.1110, -259.3081,\n",
      "          -253.0062],\n",
      "         [ 420.0291, -115.6641, -353.1883,  ..., -252.1484, -259.3312,\n",
      "          -253.0399]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 24:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 419.8216, -115.3471, -353.0706,  ..., -252.0439, -259.1937,\n",
      "          -252.9364],\n",
      "         [ 419.8475, -115.3517, -353.0816,  ..., -252.0813, -259.2186,\n",
      "          -252.9679],\n",
      "         [ 419.8758, -115.3586, -353.0945,  ..., -252.1159, -259.2417,\n",
      "          -252.9965]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 25:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.0291, -115.6641, -353.1883,  ..., -252.1484, -259.3312,\n",
      "          -253.0399],\n",
      "         [ 420.0534, -115.6691, -353.1918,  ..., -252.1839, -259.3534,\n",
      "          -253.0710],\n",
      "         [ 420.0808, -115.6757, -353.1982,  ..., -252.2173, -259.3745,\n",
      "          -253.0997]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 26:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.0534, -115.6691, -353.1918,  ..., -252.1839, -259.3534,\n",
      "          -253.0710],\n",
      "         [ 420.0808, -115.6757, -353.1982,  ..., -252.2173, -259.3745,\n",
      "          -253.0997],\n",
      "         [ 420.1097, -115.6838, -353.2068,  ..., -252.2484, -259.3944,\n",
      "          -253.1258]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 27:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 419.9050, -115.3672, -353.1085,  ..., -252.1477, -259.2629,\n",
      "          -253.0221],\n",
      "         [ 419.9342, -115.3774, -353.1231,  ..., -252.1768, -259.2823,\n",
      "          -253.0449],\n",
      "         [ 419.9626, -115.3888, -353.1378,  ..., -252.2032, -259.3000,\n",
      "          -253.0653]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 28:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.1097, -115.6838, -353.2068,  ..., -252.2484, -259.3944,\n",
      "          -253.1258],\n",
      "         [ 420.1387, -115.6932, -353.2168,  ..., -252.2774, -259.4129,\n",
      "          -253.1496],\n",
      "         [ 420.1671, -115.7036, -353.2280,  ..., -252.3040, -259.4301,\n",
      "          -253.1709]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 29:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 419.9626, -115.3888, -353.1378,  ..., -252.2032, -259.3000,\n",
      "          -253.0653],\n",
      "         [ 419.9898, -115.4012, -353.1526,  ..., -252.2272, -259.3161,\n",
      "          -253.0834],\n",
      "         [ 420.0156, -115.4144, -353.1668,  ..., -252.2489, -259.3306,\n",
      "          -253.0994]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 30:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.1671, -115.7036, -353.2280,  ..., -252.3040, -259.4301,\n",
      "          -253.1709],\n",
      "         [ 420.1943, -115.7149, -353.2397,  ..., -252.3284, -259.4459,\n",
      "          -253.1901],\n",
      "         [ 420.2200, -115.7269, -353.2518,  ..., -252.3505, -259.4603,\n",
      "          -253.2072]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 31:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.1943, -115.7149, -353.2397,  ..., -252.3284, -259.4459,\n",
      "          -253.1901],\n",
      "         [ 420.2200, -115.7269, -353.2518,  ..., -252.3505, -259.4603,\n",
      "          -253.2072],\n",
      "         [ 420.2440, -115.7393, -353.2638,  ..., -252.3707, -259.4733,\n",
      "          -253.2224]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 32:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.0400, -115.4281, -353.1806,  ..., -252.2684, -259.3437,\n",
      "          -253.1135],\n",
      "         [ 420.0627, -115.4422, -353.1938,  ..., -252.2859, -259.3554,\n",
      "          -253.1259],\n",
      "         [ 420.0840, -115.4566, -353.2065,  ..., -252.3015, -259.3659,\n",
      "          -253.1366]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 33:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.2440, -115.7393, -353.2638,  ..., -252.3707, -259.4733,\n",
      "          -253.2224],\n",
      "         [ 420.2662, -115.7522, -353.2757,  ..., -252.3888, -259.4851,\n",
      "          -253.2359],\n",
      "         [ 420.2866, -115.7653, -353.2874,  ..., -252.4052, -259.4957,\n",
      "          -253.2477]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 34:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.2662, -115.7522, -353.2757,  ..., -252.3888, -259.4851,\n",
      "          -253.2359],\n",
      "         [ 420.2866, -115.7653, -353.2874,  ..., -252.4052, -259.4957,\n",
      "          -253.2477],\n",
      "         [ 420.3054, -115.7786, -353.2986,  ..., -252.4197, -259.5051,\n",
      "          -253.2579]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 35:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.2866, -115.7653, -353.2874,  ..., -252.4052, -259.4957,\n",
      "          -253.2477],\n",
      "         [ 420.3054, -115.7786, -353.2986,  ..., -252.4197, -259.5051,\n",
      "          -253.2579],\n",
      "         [ 420.3225, -115.7919, -353.3094,  ..., -252.4328, -259.5135,\n",
      "          -253.2669]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 36:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.3054, -115.7786, -353.2986,  ..., -252.4197, -259.5051,\n",
      "          -253.2579],\n",
      "         [ 420.3225, -115.7919, -353.3094,  ..., -252.4328, -259.5135,\n",
      "          -253.2669],\n",
      "         [ 420.3381, -115.8052, -353.3197,  ..., -252.4444, -259.5209,\n",
      "          -253.2747]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 37:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.1392, -115.5004, -353.2401,  ..., -252.3387, -259.3907,\n",
      "          -253.1608],\n",
      "         [ 420.1551, -115.5150, -353.2500,  ..., -252.3483, -259.3970,\n",
      "          -253.1666],\n",
      "         [ 420.1697, -115.5294, -353.2593,  ..., -252.3567, -259.4026,\n",
      "          -253.1714]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 38:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.3381, -115.8052, -353.3197,  ..., -252.4444, -259.5209,\n",
      "          -253.2747],\n",
      "         [ 420.3522, -115.8184, -353.3296,  ..., -252.4547, -259.5273,\n",
      "          -253.2813],\n",
      "         [ 420.3651, -115.8315, -353.3388,  ..., -252.4639, -259.5331,\n",
      "          -253.2870]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 39:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.1697, -115.5294, -353.2593,  ..., -252.3567, -259.4026,\n",
      "          -253.1714],\n",
      "         [ 420.1835, -115.5436, -353.2680,  ..., -252.3640, -259.4074,\n",
      "          -253.1755],\n",
      "         [ 420.1962, -115.5576, -353.2761,  ..., -252.3704, -259.4115,\n",
      "          -253.1788]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 40:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.1835, -115.5436, -353.2680,  ..., -252.3640, -259.4074,\n",
      "          -253.1755],\n",
      "         [ 420.1962, -115.5576, -353.2761,  ..., -252.3704, -259.4115,\n",
      "          -253.1788],\n",
      "         [ 420.2080, -115.5714, -353.2838,  ..., -252.3759, -259.4151,\n",
      "          -253.1814]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 41:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.1962, -115.5576, -353.2761,  ..., -252.3704, -259.4115,\n",
      "          -253.1788],\n",
      "         [ 420.2080, -115.5714, -353.2838,  ..., -252.3759, -259.4151,\n",
      "          -253.1814],\n",
      "         [ 420.2191, -115.5850, -353.2909,  ..., -252.3806, -259.4181,\n",
      "          -253.1835]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 42:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.3873, -115.8571, -353.3561,  ..., -252.4790, -259.5424,\n",
      "          -253.2959],\n",
      "         [ 420.3969, -115.8696, -353.3640,  ..., -252.4853, -259.5461,\n",
      "          -253.2993],\n",
      "         [ 420.4056, -115.8819, -353.3715,  ..., -252.4907, -259.5493,\n",
      "          -253.3021]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 43:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.2191, -115.5850, -353.2909,  ..., -252.3806, -259.4181,\n",
      "          -253.1835],\n",
      "         [ 420.2295, -115.5985, -353.2976,  ..., -252.3846, -259.4207,\n",
      "          -253.1850],\n",
      "         [ 420.2390, -115.6116, -353.3039,  ..., -252.3880, -259.4227,\n",
      "          -253.1861]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 44:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.2295, -115.5985, -353.2976,  ..., -252.3846, -259.4207,\n",
      "          -253.1850],\n",
      "         [ 420.2390, -115.6116, -353.3039,  ..., -252.3880, -259.4227,\n",
      "          -253.1861],\n",
      "         [ 420.2481, -115.6246, -353.3098,  ..., -252.3908, -259.4245,\n",
      "          -253.1868]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 45:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.2390, -115.6116, -353.3039,  ..., -252.3880, -259.4227,\n",
      "          -253.1861],\n",
      "         [ 420.2481, -115.6246, -353.3098,  ..., -252.3908, -259.4245,\n",
      "          -253.1868],\n",
      "         [ 420.2566, -115.6374, -353.3154,  ..., -252.3932, -259.4259,\n",
      "          -253.1872]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 46:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.4207, -115.9056, -353.3853,  ..., -252.4997, -259.5544,\n",
      "          -253.3063],\n",
      "         [ 420.4272, -115.9171, -353.3917,  ..., -252.5033, -259.5563,\n",
      "          -253.3077],\n",
      "         [ 420.4331, -115.9284, -353.3977,  ..., -252.5065, -259.5580,\n",
      "          -253.3089]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 47:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.2566, -115.6374, -353.3154,  ..., -252.3932, -259.4259,\n",
      "          -253.1872],\n",
      "         [ 420.2646, -115.6500, -353.3207,  ..., -252.3951, -259.4270,\n",
      "          -253.1872],\n",
      "         [ 420.2721, -115.6624, -353.3257,  ..., -252.3967, -259.4279,\n",
      "          -253.1871]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 48:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 418.5793, -118.4888, -349.5076,  ..., -249.2327, -256.4819,\n",
      "          -250.3589],\n",
      "         ...,\n",
      "         [ 420.2646, -115.6500, -353.3207,  ..., -252.3951, -259.4270,\n",
      "          -253.1872],\n",
      "         [ 420.2721, -115.6624, -353.3257,  ..., -252.3967, -259.4279,\n",
      "          -253.1871],\n",
      "         [ 420.2792, -115.6747, -353.3304,  ..., -252.3979, -259.4286,\n",
      "          -253.1867]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 49:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 416.0692, -118.8889, -346.5327,  ..., -243.5838, -250.9626,\n",
      "          -244.5615],\n",
      "         [ 417.6671, -118.8771, -349.0438,  ..., -250.1590, -257.3128,\n",
      "          -250.8667],\n",
      "         ...,\n",
      "         [ 420.4385, -115.9394, -353.4036,  ..., -252.5092, -259.5594,\n",
      "          -253.3097],\n",
      "         [ 420.4434, -115.9502, -353.4091,  ..., -252.5116, -259.5605,\n",
      "          -253.3103],\n",
      "         [ 420.4479, -115.9608, -353.4144,  ..., -252.5137, -259.5615,\n",
      "          -253.3107]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Original: Давайте что-нибудь попробуем!\n",
      "Translation: ! we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we\n",
      "Step 0:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Step 1:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 2:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 3:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         [ 263.4984,  -96.7211, -247.0112,  ..., -177.4973, -187.1900,\n",
      "          -182.3282]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 4:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         [ 263.1150,  -96.3456, -246.9761,  ..., -177.5352, -187.1570,\n",
      "          -182.1516],\n",
      "         [ 264.0253,  -97.0034, -247.5088,  ..., -177.0582, -186.6388,\n",
      "          -181.8925]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 5:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         [ 263.1150,  -96.3456, -246.9761,  ..., -177.5352, -187.1570,\n",
      "          -182.1516],\n",
      "         [ 264.0253,  -97.0034, -247.5088,  ..., -177.0582, -186.6388,\n",
      "          -181.8925],\n",
      "         [ 265.6664,  -96.8067, -247.9886,  ..., -177.6940, -187.1953,\n",
      "          -182.6038]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 6:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 265.0946,  -96.3958, -247.7091,  ..., -177.4883, -187.0578,\n",
      "          -182.4544],\n",
      "         [ 266.4637,  -96.2328, -248.0819,  ..., -177.8114, -187.2894,\n",
      "          -182.8254],\n",
      "         [ 267.9514,  -95.8374, -248.0494,  ..., -177.5601, -186.9035,\n",
      "          -182.5748]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 7:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 266.4637,  -96.2328, -248.0819,  ..., -177.8114, -187.2894,\n",
      "          -182.8254],\n",
      "         [ 267.9514,  -95.8374, -248.0494,  ..., -177.5601, -186.9035,\n",
      "          -182.5748],\n",
      "         [ 269.3342,  -95.4596, -247.9736,  ..., -177.0524, -186.2887,\n",
      "          -182.0603]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 8:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 267.0134,  -96.4412, -248.0252,  ..., -177.8670, -187.2589,\n",
      "          -182.8058],\n",
      "         [ 268.2747,  -95.9446, -247.8542,  ..., -177.3937, -186.6414,\n",
      "          -182.3293],\n",
      "         [ 269.3832,  -95.5465, -247.8780,  ..., -176.9263, -186.1220,\n",
      "          -181.8961]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 9:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 268.2747,  -95.9446, -247.8542,  ..., -177.3937, -186.6414,\n",
      "          -182.3293],\n",
      "         [ 269.3832,  -95.5465, -247.8780,  ..., -176.9263, -186.1220,\n",
      "          -181.8961],\n",
      "         [ 270.0394,  -95.3459, -248.0723,  ..., -176.5625, -185.7936,\n",
      "          -181.5755]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 10:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.3305,  -95.1166, -248.0866,  ..., -176.6385, -185.8843,\n",
      "          -181.6850],\n",
      "         [ 270.6709,  -95.0055, -248.2413,  ..., -176.2423, -185.5564,\n",
      "          -181.3135],\n",
      "         [ 270.6243,  -94.9517, -248.2746,  ..., -175.8305, -185.2221,\n",
      "          -180.9143]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 11:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.6709,  -95.0055, -248.2413,  ..., -176.2423, -185.5564,\n",
      "          -181.3135],\n",
      "         [ 270.6243,  -94.9517, -248.2746,  ..., -175.8305, -185.2221,\n",
      "          -180.9143],\n",
      "         [ 270.4825,  -94.8553, -248.1690,  ..., -175.4039, -184.8537,\n",
      "          -180.5009]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 12:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.2403,  -95.3117, -248.2501,  ..., -176.2193, -185.5232,\n",
      "          -181.2509],\n",
      "         [ 270.2482,  -95.2595, -248.3128,  ..., -175.8776, -185.2517,\n",
      "          -180.9200],\n",
      "         [ 270.1869,  -95.1602, -248.2600,  ..., -175.5435, -184.9660,\n",
      "          -180.5939]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 13:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.2482,  -95.2595, -248.3128,  ..., -175.8776, -185.2517,\n",
      "          -180.9200],\n",
      "         [ 270.1869,  -95.1602, -248.2600,  ..., -175.5435, -184.9660,\n",
      "          -180.5939],\n",
      "         [ 270.0834,  -95.0226, -248.1385,  ..., -175.2323, -184.6799,\n",
      "          -180.2860]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 14:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.1869,  -95.1602, -248.2600,  ..., -175.5435, -184.9660,\n",
      "          -180.5939],\n",
      "         [ 270.0834,  -95.0226, -248.1385,  ..., -175.2323, -184.6799,\n",
      "          -180.2860],\n",
      "         [ 269.9462,  -94.8426, -247.9978,  ..., -174.9557, -184.4128,\n",
      "          -180.0086]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 15:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.1604,  -94.5595, -247.8058,  ..., -174.6662, -184.1536,\n",
      "          -179.7738],\n",
      "         [ 270.0063,  -94.3585, -247.6521,  ..., -174.4025, -183.8893,\n",
      "          -179.5090],\n",
      "         [ 269.8731,  -94.1562, -247.5312,  ..., -174.2035, -183.6882,\n",
      "          -179.3080]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 16:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.0063,  -94.3585, -247.6521,  ..., -174.4025, -183.8893,\n",
      "          -179.5090],\n",
      "         [ 269.8731,  -94.1562, -247.5312,  ..., -174.2035, -183.6882,\n",
      "          -179.3080],\n",
      "         [ 269.7807,  -93.9691, -247.4375,  ..., -174.0588, -183.5418,\n",
      "          -179.1620]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 17:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 269.8731,  -94.1562, -247.5312,  ..., -174.2035, -183.6882,\n",
      "          -179.3080],\n",
      "         [ 269.7807,  -93.9691, -247.4375,  ..., -174.0588, -183.5418,\n",
      "          -179.1620],\n",
      "         [ 269.7416,  -93.7988, -247.3664,  ..., -173.9574, -183.4397,\n",
      "          -179.0611]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 18:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 269.7807,  -93.9691, -247.4375,  ..., -174.0588, -183.5418,\n",
      "          -179.1620],\n",
      "         [ 269.7416,  -93.7988, -247.3664,  ..., -173.9574, -183.4397,\n",
      "          -179.0611],\n",
      "         [ 269.7523,  -93.6428, -247.3123,  ..., -173.8855, -183.3682,\n",
      "          -178.9919]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 19:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 269.7416,  -93.7988, -247.3664,  ..., -173.9574, -183.4397,\n",
      "          -179.0611],\n",
      "         [ 269.7523,  -93.6428, -247.3123,  ..., -173.8855, -183.3682,\n",
      "          -178.9919],\n",
      "         [ 269.7988,  -93.4984, -247.2697,  ..., -173.8300, -183.3141,\n",
      "          -178.9411]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 20:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 269.3832,  -94.0671, -247.5641,  ..., -174.2903, -183.7601,\n",
      "          -179.3320],\n",
      "         [ 269.3732,  -93.9103, -247.5046,  ..., -174.2304, -183.7016,\n",
      "          -179.2714],\n",
      "         [ 269.4180,  -93.7647, -247.4611,  ..., -174.1926, -183.6641,\n",
      "          -179.2349]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 21:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 269.7988,  -93.4984, -247.2697,  ..., -173.8300, -183.3141,\n",
      "          -178.9411],\n",
      "         [ 269.8663,  -93.3637, -247.2341,  ..., -173.7828, -183.2690,\n",
      "          -178.9002],\n",
      "         [ 269.9442,  -93.2374, -247.2032,  ..., -173.7400, -183.2288,\n",
      "          -178.8646]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 22:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 269.4180,  -93.7647, -247.4611,  ..., -174.1926, -183.6641,\n",
      "          -179.2349],\n",
      "         [ 269.4932,  -93.6283, -247.4279,  ..., -174.1638, -183.6349,\n",
      "          -179.2092],\n",
      "         [ 269.5807,  -93.5001, -247.4007,  ..., -174.1370, -183.6072,\n",
      "          -179.1867]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 23:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 269.4932,  -93.6283, -247.4279,  ..., -174.1638, -183.6349,\n",
      "          -179.2092],\n",
      "         [ 269.5807,  -93.5001, -247.4007,  ..., -174.1370, -183.6072,\n",
      "          -179.1867],\n",
      "         [ 269.6703,  -93.3796, -247.3767,  ..., -174.1095, -183.5785,\n",
      "          -179.1644]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 24:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.0260,  -93.1187, -247.1753,  ..., -173.7001, -183.1915,\n",
      "          -178.8322],\n",
      "         [ 270.1080,  -93.0070, -247.1498,  ..., -173.6625, -183.1565,\n",
      "          -178.8022],\n",
      "         [ 270.1879,  -92.9020, -247.1261,  ..., -173.6268, -183.1234,\n",
      "          -178.7737]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 25:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 269.6703,  -93.3796, -247.3767,  ..., -174.1095, -183.5785,\n",
      "          -179.1644],\n",
      "         [ 269.7575,  -93.2663, -247.3546,  ..., -174.0808, -183.5485,\n",
      "          -179.1411],\n",
      "         [ 269.8404,  -93.1599, -247.3337,  ..., -174.0513, -183.5178,\n",
      "          -179.1171]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 26:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.1879,  -92.9020, -247.1261,  ..., -173.6268, -183.1234,\n",
      "          -178.7737],\n",
      "         [ 270.2643,  -92.8035, -247.1039,  ..., -173.5927, -183.0921,\n",
      "          -178.7465],\n",
      "         [ 270.3363,  -92.7115, -247.0832,  ..., -173.5601, -183.0624,\n",
      "          -178.7202]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 27:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 269.8404,  -93.1599, -247.3337,  ..., -174.0513, -183.5178,\n",
      "          -179.1171],\n",
      "         [ 269.9181,  -93.0600, -247.3136,  ..., -174.0215, -183.4869,\n",
      "          -179.0923],\n",
      "         [ 269.9905,  -92.9662, -247.2943,  ..., -173.9915, -183.4563,\n",
      "          -179.0671]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 28:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 269.9181,  -93.0600, -247.3136,  ..., -174.0215, -183.4869,\n",
      "          -179.0923],\n",
      "         [ 269.9905,  -92.9662, -247.2943,  ..., -173.9915, -183.4563,\n",
      "          -179.0671],\n",
      "         [ 270.0578,  -92.8783, -247.2757,  ..., -173.9615, -183.4261,\n",
      "          -179.0415]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 29:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.4038,  -92.6259, -247.0638,  ..., -173.5288, -183.0342,\n",
      "          -178.6947],\n",
      "         [ 270.4669,  -92.5466, -247.0458,  ..., -173.4991, -183.0075,\n",
      "          -178.6702],\n",
      "         [ 270.5257,  -92.4733, -247.0293,  ..., -173.4710, -182.9825,\n",
      "          -178.6467]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 30:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.0578,  -92.8783, -247.2757,  ..., -173.9615, -183.4261,\n",
      "          -179.0415],\n",
      "         [ 270.1202,  -92.7961, -247.2579,  ..., -173.9317, -183.3965,\n",
      "          -179.0156],\n",
      "         [ 270.1784,  -92.7192, -247.2410,  ..., -173.9021, -183.3675,\n",
      "          -178.9896]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 31:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.5257,  -92.4733, -247.0293,  ..., -173.4710, -182.9825,\n",
      "          -178.6467],\n",
      "         [ 270.5807,  -92.4057, -247.0143,  ..., -173.4446, -182.9590,\n",
      "          -178.6245],\n",
      "         [ 270.6323,  -92.3434, -247.0008,  ..., -173.4200, -182.9374,\n",
      "          -178.6036]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 32:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.1784,  -92.7192, -247.2410,  ..., -173.9021, -183.3675,\n",
      "          -178.9896],\n",
      "         [ 270.2329,  -92.6473, -247.2250,  ..., -173.8729, -183.3393,\n",
      "          -178.9636],\n",
      "         [ 270.2843,  -92.5802, -247.2100,  ..., -173.8441, -183.3118,\n",
      "          -178.9379]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 33:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.6323,  -92.3434, -247.0008,  ..., -173.4200, -182.9374,\n",
      "          -178.6036],\n",
      "         [ 270.6807,  -92.2859, -246.9886,  ..., -173.3973, -182.9174,\n",
      "          -178.5842],\n",
      "         [ 270.7262,  -92.2329, -246.9778,  ..., -173.3764, -182.8991,\n",
      "          -178.5661]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 34:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.6807,  -92.2859, -246.9886,  ..., -173.3973, -182.9174,\n",
      "          -178.5842],\n",
      "         [ 270.7262,  -92.2329, -246.9778,  ..., -173.3764, -182.8991,\n",
      "          -178.5661],\n",
      "         [ 270.7689,  -92.1839, -246.9684,  ..., -173.3573, -182.8825,\n",
      "          -178.5494]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 35:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.7262,  -92.2329, -246.9778,  ..., -173.3764, -182.8991,\n",
      "          -178.5661],\n",
      "         [ 270.7689,  -92.1839, -246.9684,  ..., -173.3573, -182.8825,\n",
      "          -178.5494],\n",
      "         [ 270.8090,  -92.1385, -246.9603,  ..., -173.3399, -182.8673,\n",
      "          -178.5341]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 36:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 270.7689,  -92.1839, -246.9684,  ..., -173.3573, -182.8825,\n",
      "          -178.5494],\n",
      "         [ 270.8090,  -92.1385, -246.9603,  ..., -173.3399, -182.8673,\n",
      "          -178.5341],\n",
      "         [ 270.8466,  -92.0965, -246.9534,  ..., -173.3241, -182.8536,\n",
      "          -178.5200]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 37:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.4243,  -92.4043, -247.1706,  ..., -173.7623, -183.2349,\n",
      "          -178.8637],\n",
      "         [ 270.4673,  -92.3531, -247.1594,  ..., -173.7369, -183.2114,\n",
      "          -178.8406],\n",
      "         [ 270.5088,  -92.3052, -247.1491,  ..., -173.7127, -183.1891,\n",
      "          -178.8184]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 38:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.4673,  -92.3531, -247.1594,  ..., -173.7369, -183.2114,\n",
      "          -178.8406],\n",
      "         [ 270.5088,  -92.3052, -247.1491,  ..., -173.7127, -183.1891,\n",
      "          -178.8184],\n",
      "         [ 270.5489,  -92.2603, -247.1398,  ..., -173.6897, -183.1679,\n",
      "          -178.7972]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 39:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.5088,  -92.3052, -247.1491,  ..., -173.7127, -183.1891,\n",
      "          -178.8184],\n",
      "         [ 270.5489,  -92.2603, -247.1398,  ..., -173.6897, -183.1679,\n",
      "          -178.7972],\n",
      "         [ 270.5876,  -92.2184, -247.1315,  ..., -173.6678, -183.1481,\n",
      "          -178.7772]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 40:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.5489,  -92.2603, -247.1398,  ..., -173.6897, -183.1679,\n",
      "          -178.7972],\n",
      "         [ 270.5876,  -92.2184, -247.1315,  ..., -173.6678, -183.1481,\n",
      "          -178.7772],\n",
      "         [ 270.6251,  -92.1791, -247.1240,  ..., -173.6474, -183.1294,\n",
      "          -178.7583]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 41:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.5876,  -92.2184, -247.1315,  ..., -173.6678, -183.1481,\n",
      "          -178.7772],\n",
      "         [ 270.6251,  -92.1791, -247.1240,  ..., -173.6474, -183.1294,\n",
      "          -178.7583],\n",
      "         [ 270.6612,  -92.1422, -247.1175,  ..., -173.6281, -183.1120,\n",
      "          -178.7406]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 42:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.6251,  -92.1791, -247.1240,  ..., -173.6474, -183.1294,\n",
      "          -178.7583],\n",
      "         [ 270.6612,  -92.1422, -247.1175,  ..., -173.6281, -183.1120,\n",
      "          -178.7406],\n",
      "         [ 270.6960,  -92.1076, -247.1119,  ..., -173.6101, -183.0958,\n",
      "          -178.7240]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 43:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.6612,  -92.1422, -247.1175,  ..., -173.6281, -183.1120,\n",
      "          -178.7406],\n",
      "         [ 270.6960,  -92.1076, -247.1119,  ..., -173.6101, -183.0958,\n",
      "          -178.7240],\n",
      "         [ 270.7294,  -92.0752, -247.1071,  ..., -173.5934, -183.0808,\n",
      "          -178.7085]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 44:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.6960,  -92.1076, -247.1119,  ..., -173.6101, -183.0958,\n",
      "          -178.7240],\n",
      "         [ 270.7294,  -92.0752, -247.1071,  ..., -173.5934, -183.0808,\n",
      "          -178.7085],\n",
      "         [ 270.7614,  -92.0447, -247.1030,  ..., -173.5778, -183.0669,\n",
      "          -178.6941]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 45:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 271.0481,  -91.8729, -246.9327,  ..., -173.2500, -182.7907,\n",
      "          -178.4510],\n",
      "         [ 271.0693,  -91.8485, -246.9328,  ..., -173.2434, -182.7853,\n",
      "          -178.4444],\n",
      "         [ 271.0889,  -91.8256, -246.9337,  ..., -173.2377, -182.7806,\n",
      "          -178.4385]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 46:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 261.7547,  -97.8523, -244.6340,  ..., -176.6966, -186.5170,\n",
      "          -181.5113],\n",
      "         ...,\n",
      "         [ 271.0693,  -91.8485, -246.9328,  ..., -173.2434, -182.7853,\n",
      "          -178.4444],\n",
      "         [ 271.0889,  -91.8256, -246.9337,  ..., -173.2377, -182.7806,\n",
      "          -178.4385],\n",
      "         [ 271.1070,  -91.8039, -246.9350,  ..., -173.2327, -182.7766,\n",
      "          -178.4331]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 47:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.7921,  -92.0161, -247.0998,  ..., -173.5634, -183.0540,\n",
      "          -178.6807],\n",
      "         [ 270.8214,  -91.9892, -247.0972,  ..., -173.5499, -183.0421,\n",
      "          -178.6683],\n",
      "         [ 270.8495,  -91.9639, -247.0952,  ..., -173.5375, -183.0312,\n",
      "          -178.6567]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 48:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.8214,  -91.9892, -247.0972,  ..., -173.5499, -183.0421,\n",
      "          -178.6683],\n",
      "         [ 270.8495,  -91.9639, -247.0952,  ..., -173.5375, -183.0312,\n",
      "          -178.6567],\n",
      "         [ 270.8761,  -91.9400, -247.0938,  ..., -173.5260, -183.0210,\n",
      "          -178.6460]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 49:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 260.4632,  -95.1503, -242.5750,  ..., -169.7946, -179.7816,\n",
      "          -174.6684],\n",
      "         [ 262.4226,  -96.1951, -243.8964,  ..., -176.1559, -185.8799,\n",
      "          -181.0663],\n",
      "         ...,\n",
      "         [ 270.8495,  -91.9639, -247.0952,  ..., -173.5375, -183.0312,\n",
      "          -178.6567],\n",
      "         [ 270.8761,  -91.9400, -247.0938,  ..., -173.5260, -183.0210,\n",
      "          -178.6460],\n",
      "         [ 270.9016,  -91.9176, -247.0929,  ..., -173.5154, -183.0117,\n",
      "          -178.6359]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Original: Мне пора идти спать.\n",
      "Translation: !JustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJustJust\n",
      "Step 0:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Step 1:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 2:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 3:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         [ 191.1423,  -59.0902, -169.9259,  ..., -126.4926, -127.1484,\n",
      "          -123.2633]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 4:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         [ 190.8015,  -59.6763, -169.2597,  ..., -125.4292, -126.1325,\n",
      "          -122.3435],\n",
      "         [ 191.8430,  -58.5386, -170.2992,  ..., -126.4421, -127.0110,\n",
      "          -123.2133]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 5:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         [ 191.1423,  -59.0902, -169.9259,  ..., -126.4926, -127.1484,\n",
      "          -123.2633],\n",
      "         [ 191.6785,  -58.0851, -170.2037,  ..., -126.5202, -127.1018,\n",
      "          -123.2120],\n",
      "         [ 192.9835,  -57.7329, -170.3488,  ..., -126.8242, -127.4148,\n",
      "          -123.4709]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 6:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.8430,  -58.5386, -170.2992,  ..., -126.4421, -127.0110,\n",
      "          -123.2133],\n",
      "         [ 192.9737,  -58.0869, -170.5007,  ..., -126.8016, -127.3713,\n",
      "          -123.5170],\n",
      "         [ 193.9040,  -57.8161, -170.4827,  ..., -126.8505, -127.3924,\n",
      "          -123.4825]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 7:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.9737,  -58.0869, -170.5007,  ..., -126.8016, -127.3713,\n",
      "          -123.5170],\n",
      "         [ 193.9040,  -57.8161, -170.4827,  ..., -126.8505, -127.3924,\n",
      "          -123.4825],\n",
      "         [ 194.3900,  -57.7214, -170.5382,  ..., -126.9210, -127.4707,\n",
      "          -123.4958]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 8:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.9040,  -57.8161, -170.4827,  ..., -126.8505, -127.3924,\n",
      "          -123.4825],\n",
      "         [ 194.3900,  -57.7214, -170.5382,  ..., -126.9210, -127.4707,\n",
      "          -123.4958],\n",
      "         [ 194.3396,  -57.6653, -170.5544,  ..., -126.8847, -127.4833,\n",
      "          -123.4732]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 9:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 194.5046,  -57.5350, -170.5514,  ..., -126.9566, -127.5499,\n",
      "          -123.5254],\n",
      "         [ 194.3817,  -57.4829, -170.6124,  ..., -126.9602, -127.6085,\n",
      "          -123.5847],\n",
      "         [ 194.1025,  -57.3071, -170.6010,  ..., -126.8617, -127.5678,\n",
      "          -123.5542]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 10:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 194.3817,  -57.4829, -170.6124,  ..., -126.9602, -127.6085,\n",
      "          -123.5847],\n",
      "         [ 194.1025,  -57.3071, -170.6010,  ..., -126.8617, -127.5678,\n",
      "          -123.5542],\n",
      "         [ 193.8372,  -57.0746, -170.5878,  ..., -126.7386, -127.4983,\n",
      "          -123.4904]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 11:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 194.1025,  -57.3071, -170.6010,  ..., -126.8617, -127.5678,\n",
      "          -123.5542],\n",
      "         [ 193.8372,  -57.0746, -170.5878,  ..., -126.7386, -127.4983,\n",
      "          -123.4904],\n",
      "         [ 193.6005,  -56.8588, -170.5799,  ..., -126.6147, -127.4249,\n",
      "          -123.4190]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 12:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 193.8372,  -57.0746, -170.5878,  ..., -126.7386, -127.4983,\n",
      "          -123.4904],\n",
      "         [ 193.6005,  -56.8588, -170.5799,  ..., -126.6147, -127.4249,\n",
      "          -123.4190],\n",
      "         [ 193.3934,  -56.6826, -170.5668,  ..., -126.4994, -127.3552,\n",
      "          -123.3519]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 13:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 193.6005,  -56.8588, -170.5799,  ..., -126.6147, -127.4249,\n",
      "          -123.4190],\n",
      "         [ 193.3934,  -56.6826, -170.5668,  ..., -126.4994, -127.3552,\n",
      "          -123.3519],\n",
      "         [ 193.2259,  -56.5417, -170.5405,  ..., -126.4002, -127.2937,\n",
      "          -123.2962]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 14:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.4341,  -56.7903, -170.4466,  ..., -126.2945, -127.1385,\n",
      "          -123.0969],\n",
      "         [ 193.2649,  -56.6451, -170.4266,  ..., -126.1912, -127.0800,\n",
      "          -123.0401],\n",
      "         [ 193.1360,  -56.5232, -170.3966,  ..., -126.1185, -127.0423,\n",
      "          -123.0108]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 15:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 193.2259,  -56.5417, -170.5405,  ..., -126.4002, -127.2937,\n",
      "          -123.2962],\n",
      "         [ 193.1045,  -56.4229, -170.5025,  ..., -126.3259, -127.2480,\n",
      "          -123.2595],\n",
      "         [ 193.0218,  -56.3167, -170.4580,  ..., -126.2782, -127.2208,\n",
      "          -123.2428]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 16:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.1360,  -56.5232, -170.3966,  ..., -126.1185, -127.0423,\n",
      "          -123.0108],\n",
      "         [ 193.0458,  -56.4090, -170.3597,  ..., -126.0751, -127.0257,\n",
      "          -123.0070],\n",
      "         [ 192.9812,  -56.2972, -170.3190,  ..., -126.0520, -127.0229,\n",
      "          -123.0189]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 17:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 193.0218,  -56.3167, -170.4580,  ..., -126.2782, -127.2208,\n",
      "          -123.2428],\n",
      "         [ 192.9626,  -56.2203, -170.4108,  ..., -126.2491, -127.2063,\n",
      "          -123.2388],\n",
      "         [ 192.9135,  -56.1336, -170.3620,  ..., -126.2259, -127.1945,\n",
      "          -123.2366]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 18:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.9812,  -56.2972, -170.3190,  ..., -126.0520, -127.0229,\n",
      "          -123.0189],\n",
      "         [ 192.9284,  -56.1895, -170.2762,  ..., -126.0373, -127.0233,\n",
      "          -123.0339],\n",
      "         [ 192.8779,  -56.0884, -170.2324,  ..., -126.0204, -127.0180,\n",
      "          -123.0418]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 19:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.9135,  -56.1336, -170.3620,  ..., -126.2259, -127.1945,\n",
      "          -123.2366],\n",
      "         [ 192.8653,  -56.0560, -170.3121,  ..., -126.1980, -127.1762,\n",
      "          -123.2267],\n",
      "         [ 192.8126,  -55.9856, -170.2615,  ..., -126.1586, -127.1460,\n",
      "          -123.2036]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 20:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.8779,  -56.0884, -170.2324,  ..., -126.0204, -127.0180,\n",
      "          -123.0418],\n",
      "         [ 192.8229,  -55.9942, -170.1877,  ..., -125.9939, -127.0006,\n",
      "          -123.0359],\n",
      "         [ 192.7585,  -55.9049, -170.1426,  ..., -125.9548, -126.9690,\n",
      "          -123.0138]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 21:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.8126,  -55.9856, -170.2615,  ..., -126.1586, -127.1460,\n",
      "          -123.2036],\n",
      "         [ 192.7514,  -55.9200, -170.2105,  ..., -126.1057, -127.1020,\n",
      "          -123.1655],\n",
      "         [ 192.6793,  -55.8565, -170.1597,  ..., -126.0405, -127.0452,\n",
      "          -123.1137]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 22:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.7585,  -55.9049, -170.1426,  ..., -125.9548, -126.9690,\n",
      "          -123.0138],\n",
      "         [ 192.6827,  -55.8185, -170.0975,  ..., -125.9040, -126.9245,\n",
      "          -122.9771],\n",
      "         [ 192.5961,  -55.7346, -170.0531,  ..., -125.8452, -126.8706,\n",
      "          -122.9297]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 23:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.6827,  -55.8185, -170.0975,  ..., -125.9040, -126.9245,\n",
      "          -122.9771],\n",
      "         [ 192.5961,  -55.7346, -170.0531,  ..., -125.8452, -126.8706,\n",
      "          -122.9297],\n",
      "         [ 192.5014,  -55.6541, -170.0099,  ..., -125.7819, -126.8116,\n",
      "          -122.8758]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 24:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.5954,  -55.7936, -170.1098,  ..., -125.9667, -126.9788,\n",
      "          -123.0514],\n",
      "         [ 192.5013,  -55.7311, -170.0615,  ..., -125.8884, -126.9069,\n",
      "          -122.9830],\n",
      "         [ 192.4002,  -55.6701, -170.0153,  ..., -125.8096, -126.8333,\n",
      "          -122.9125]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 25:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.5013,  -55.7311, -170.0615,  ..., -125.8884, -126.9069,\n",
      "          -122.9830],\n",
      "         [ 192.4002,  -55.6701, -170.0153,  ..., -125.8096, -126.8333,\n",
      "          -122.9125],\n",
      "         [ 192.2953,  -55.6119, -169.9717,  ..., -125.7331, -126.7611,\n",
      "          -122.8430]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 26:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.4017,  -55.5783, -169.9687,  ..., -125.7172, -126.7506,\n",
      "          -122.8191],\n",
      "         [ 192.2996,  -55.5086, -169.9298,  ..., -125.6532, -126.6899,\n",
      "          -122.7619],\n",
      "         [ 192.1968,  -55.4461, -169.8931,  ..., -125.5910, -126.6309,\n",
      "          -122.7059]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 27:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.2996,  -55.5086, -169.9298,  ..., -125.6532, -126.6899,\n",
      "          -122.7619],\n",
      "         [ 192.1968,  -55.4461, -169.8931,  ..., -125.5910, -126.6309,\n",
      "          -122.7059],\n",
      "         [ 192.0944,  -55.3910, -169.8586,  ..., -125.5312, -126.5744,\n",
      "          -122.6521]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 28:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.1892,  -55.5580, -169.9310,  ..., -125.6602, -126.6921,\n",
      "          -122.7764],\n",
      "         [ 192.0836,  -55.5092, -169.8930,  ..., -125.5920, -126.6273,\n",
      "          -122.7139],\n",
      "         [ 191.9798,  -55.4662, -169.8575,  ..., -125.5285, -126.5670,\n",
      "          -122.6559]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 29:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.0944,  -55.3910, -169.8586,  ..., -125.5312, -126.5744,\n",
      "          -122.6521],\n",
      "         [ 191.9934,  -55.3435, -169.8262,  ..., -125.4742, -126.5207,\n",
      "          -122.6007],\n",
      "         [ 191.8948,  -55.3033, -169.7955,  ..., -125.4203, -126.4700,\n",
      "          -122.5523]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 30:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.9934,  -55.3435, -169.8262,  ..., -125.4742, -126.5207,\n",
      "          -122.6007],\n",
      "         [ 191.8948,  -55.3033, -169.7955,  ..., -125.4203, -126.4700,\n",
      "          -122.5523],\n",
      "         [ 191.7993,  -55.2699, -169.7664,  ..., -125.3694, -126.4223,\n",
      "          -122.5068]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 31:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.8785,  -55.4289, -169.8243,  ..., -125.4696, -126.5114,\n",
      "          -122.6023],\n",
      "         [ 191.7805,  -55.3972, -169.7931,  ..., -125.4152, -126.4600,\n",
      "          -122.5532],\n",
      "         [ 191.6862,  -55.3707, -169.7637,  ..., -125.3649, -126.4128,\n",
      "          -122.5081]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 32:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.7993,  -55.2699, -169.7664,  ..., -125.3694, -126.4223,\n",
      "          -122.5068],\n",
      "         [ 191.7075,  -55.2429, -169.7387,  ..., -125.3216, -126.3777,\n",
      "          -122.4643],\n",
      "         [ 191.6199,  -55.2216, -169.7124,  ..., -125.2768, -126.3359,\n",
      "          -122.4247]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 33:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.7075,  -55.2429, -169.7387,  ..., -125.3216, -126.3777,\n",
      "          -122.4643],\n",
      "         [ 191.6199,  -55.2216, -169.7124,  ..., -125.2768, -126.3359,\n",
      "          -122.4247],\n",
      "         [ 191.5368,  -55.2053, -169.6874,  ..., -125.2350, -126.2970,\n",
      "          -122.3880]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 34:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.6199,  -55.2216, -169.7124,  ..., -125.2768, -126.3359,\n",
      "          -122.4247],\n",
      "         [ 191.5368,  -55.2053, -169.6874,  ..., -125.2350, -126.2970,\n",
      "          -122.3880],\n",
      "         [ 191.4583,  -55.1935, -169.6638,  ..., -125.1962, -126.2609,\n",
      "          -122.3542]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 35:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.5368,  -55.2053, -169.6874,  ..., -125.2350, -126.2970,\n",
      "          -122.3880],\n",
      "         [ 191.4583,  -55.1935, -169.6638,  ..., -125.1962, -126.2609,\n",
      "          -122.3542],\n",
      "         [ 191.3847,  -55.1854, -169.6416,  ..., -125.1602, -126.2275,\n",
      "          -122.3232]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 36:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.4583,  -55.1935, -169.6638,  ..., -125.1962, -126.2609,\n",
      "          -122.3542],\n",
      "         [ 191.3847,  -55.1854, -169.6416,  ..., -125.1602, -126.2275,\n",
      "          -122.3232],\n",
      "         [ 191.3158,  -55.1806, -169.6210,  ..., -125.1271, -126.1969,\n",
      "          -122.2950]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 37:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.3524,  -55.3082, -169.6605,  ..., -125.1994, -126.2580,\n",
      "          -122.3631],\n",
      "         [ 191.2805,  -55.3010, -169.6384,  ..., -125.1659, -126.2269,\n",
      "          -122.3346],\n",
      "         [ 191.2132,  -55.2961, -169.6179,  ..., -125.1353, -126.1985,\n",
      "          -122.3089]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 38:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.3158,  -55.1806, -169.6210,  ..., -125.1271, -126.1969,\n",
      "          -122.2950],\n",
      "         [ 191.2518,  -55.1783, -169.6020,  ..., -125.0969, -126.1689,\n",
      "          -122.2696],\n",
      "         [ 191.1923,  -55.1781, -169.5848,  ..., -125.0695, -126.1437,\n",
      "          -122.2469]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 39:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.2132,  -55.2961, -169.6179,  ..., -125.1353, -126.1985,\n",
      "          -122.3089],\n",
      "         [ 191.1506,  -55.2932, -169.5991,  ..., -125.1076, -126.1729,\n",
      "          -122.2859],\n",
      "         [ 191.0924,  -55.2918, -169.5821,  ..., -125.0827, -126.1500,\n",
      "          -122.2656]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 40:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.1506,  -55.2932, -169.5991,  ..., -125.1076, -126.1729,\n",
      "          -122.2859],\n",
      "         [ 191.0924,  -55.2918, -169.5821,  ..., -125.0827, -126.1500,\n",
      "          -122.2656],\n",
      "         [ 191.0386,  -55.2917, -169.5668,  ..., -125.0606, -126.1298,\n",
      "          -122.2478]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 41:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.0924,  -55.2918, -169.5821,  ..., -125.0827, -126.1500,\n",
      "          -122.2656],\n",
      "         [ 191.0386,  -55.2917, -169.5668,  ..., -125.0606, -126.1298,\n",
      "          -122.2478],\n",
      "         [ 190.9889,  -55.2924, -169.5534,  ..., -125.0411, -126.1122,\n",
      "          -122.2326]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 42:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.0866,  -55.1821, -169.5557,  ..., -125.0232, -126.1013,\n",
      "          -122.2093],\n",
      "         [ 191.0399,  -55.1857, -169.5439,  ..., -125.0041, -126.0840,\n",
      "          -122.1944],\n",
      "         [ 190.9970,  -55.1899, -169.5338,  ..., -124.9876, -126.0693,\n",
      "          -122.1819]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 43:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.0399,  -55.1857, -169.5439,  ..., -125.0041, -126.0840,\n",
      "          -122.1944],\n",
      "         [ 190.9970,  -55.1899, -169.5338,  ..., -124.9876, -126.0693,\n",
      "          -122.1819],\n",
      "         [ 190.9575,  -55.1946, -169.5253,  ..., -124.9735, -126.0569,\n",
      "          -122.1716]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 44:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 190.9970,  -55.1899, -169.5338,  ..., -124.9876, -126.0693,\n",
      "          -122.1819],\n",
      "         [ 190.9575,  -55.1946, -169.5253,  ..., -124.9735, -126.0569,\n",
      "          -122.1716],\n",
      "         [ 190.9212,  -55.1995, -169.5184,  ..., -124.9618, -126.0468,\n",
      "          -122.1635]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 45:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 190.9012,  -55.2959, -169.5319,  ..., -125.0096, -126.0843,\n",
      "          -122.2090],\n",
      "         [ 190.8626,  -55.2983, -169.5236,  ..., -124.9974, -126.0738,\n",
      "          -122.2004],\n",
      "         [ 190.8271,  -55.3010, -169.5167,  ..., -124.9872, -126.0653,\n",
      "          -122.1938]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 46:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 190.9212,  -55.1995, -169.5184,  ..., -124.9618, -126.0468,\n",
      "          -122.1635],\n",
      "         [ 190.8879,  -55.2046, -169.5129,  ..., -124.9521, -126.0387,\n",
      "          -122.1574],\n",
      "         [ 190.8572,  -55.2099, -169.5088,  ..., -124.9444, -126.0325,\n",
      "          -122.1530]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 47:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 190.8879,  -55.2046, -169.5129,  ..., -124.9521, -126.0387,\n",
      "          -122.1574],\n",
      "         [ 190.8572,  -55.2099, -169.5088,  ..., -124.9444, -126.0325,\n",
      "          -122.1530],\n",
      "         [ 190.8288,  -55.2152, -169.5057,  ..., -124.9385, -126.0280,\n",
      "          -122.1502]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 48:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 190.8572,  -55.2099, -169.5088,  ..., -124.9444, -126.0325,\n",
      "          -122.1530],\n",
      "         [ 190.8288,  -55.2152, -169.5057,  ..., -124.9385, -126.0280,\n",
      "          -122.1502],\n",
      "         [ 190.8027,  -55.2205, -169.5037,  ..., -124.9341, -126.0251,\n",
      "          -122.1489]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 49:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 190.7646,  -55.3070, -169.5069,  ..., -124.9724, -126.0537,\n",
      "          -122.1856],\n",
      "         [ 190.7371,  -55.3103, -169.5037,  ..., -124.9674, -126.0503,\n",
      "          -122.1837],\n",
      "         [ 190.7117,  -55.3137, -169.5014,  ..., -124.9638, -126.0483,\n",
      "          -122.1831]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Original: Что ты делаешь?\n",
      "Translation: ! glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad\n",
      "Step 0:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Step 1:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 2:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 3:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         [ 191.1423,  -59.0902, -169.9259,  ..., -126.4926, -127.1484,\n",
      "          -123.2633]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 4:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         [ 191.1423,  -59.0902, -169.9259,  ..., -126.4926, -127.1484,\n",
      "          -123.2633],\n",
      "         [ 191.6785,  -58.0851, -170.2037,  ..., -126.5202, -127.1018,\n",
      "          -123.2120]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 5:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         [ 190.8015,  -59.6763, -169.2597,  ..., -125.4292, -126.1325,\n",
      "          -122.3435],\n",
      "         [ 191.8430,  -58.5386, -170.2992,  ..., -126.4421, -127.0110,\n",
      "          -123.2133],\n",
      "         [ 192.9737,  -58.0869, -170.5007,  ..., -126.8016, -127.3713,\n",
      "          -123.5170]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 6:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.8430,  -58.5386, -170.2992,  ..., -126.4421, -127.0110,\n",
      "          -123.2133],\n",
      "         [ 192.9737,  -58.0869, -170.5007,  ..., -126.8016, -127.3713,\n",
      "          -123.5170],\n",
      "         [ 193.9040,  -57.8161, -170.4827,  ..., -126.8505, -127.3924,\n",
      "          -123.4825]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 7:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.9737,  -58.0869, -170.5007,  ..., -126.8016, -127.3713,\n",
      "          -123.5170],\n",
      "         [ 193.9040,  -57.8161, -170.4827,  ..., -126.8505, -127.3924,\n",
      "          -123.4825],\n",
      "         [ 194.3900,  -57.7214, -170.5382,  ..., -126.9210, -127.4707,\n",
      "          -123.4958]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 8:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 194.0331,  -57.5553, -170.4128,  ..., -126.8515, -127.4197,\n",
      "          -123.4287],\n",
      "         [ 194.5046,  -57.5350, -170.5514,  ..., -126.9566, -127.5499,\n",
      "          -123.5254],\n",
      "         [ 194.3817,  -57.4829, -170.6124,  ..., -126.9602, -127.6085,\n",
      "          -123.5847]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 9:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 194.3900,  -57.7214, -170.5382,  ..., -126.9210, -127.4707,\n",
      "          -123.4958],\n",
      "         [ 194.3396,  -57.6653, -170.5544,  ..., -126.8847, -127.4833,\n",
      "          -123.4732],\n",
      "         [ 194.0908,  -57.4797, -170.5074,  ..., -126.7372, -127.4005,\n",
      "          -123.3805]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 10:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 194.3396,  -57.6653, -170.5544,  ..., -126.8847, -127.4833,\n",
      "          -123.4732],\n",
      "         [ 194.0908,  -57.4797, -170.5074,  ..., -126.7372, -127.4005,\n",
      "          -123.3805],\n",
      "         [ 193.8493,  -57.2183, -170.4730,  ..., -126.5748, -127.3030,\n",
      "          -123.2756]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 11:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 194.0908,  -57.4797, -170.5074,  ..., -126.7372, -127.4005,\n",
      "          -123.3805],\n",
      "         [ 193.8493,  -57.2183, -170.4730,  ..., -126.5748, -127.3030,\n",
      "          -123.2756],\n",
      "         [ 193.6320,  -56.9785, -170.4582,  ..., -126.4241, -127.2136,\n",
      "          -123.1773]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 12:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.8493,  -57.2183, -170.4730,  ..., -126.5748, -127.3030,\n",
      "          -123.2756],\n",
      "         [ 193.6320,  -56.9785, -170.4582,  ..., -126.4241, -127.2136,\n",
      "          -123.1773],\n",
      "         [ 193.4341,  -56.7903, -170.4466,  ..., -126.2945, -127.1385,\n",
      "          -123.0969]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 13:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 193.6005,  -56.8588, -170.5799,  ..., -126.6147, -127.4249,\n",
      "          -123.4190],\n",
      "         [ 193.3934,  -56.6826, -170.5668,  ..., -126.4994, -127.3552,\n",
      "          -123.3519],\n",
      "         [ 193.2259,  -56.5417, -170.5405,  ..., -126.4002, -127.2937,\n",
      "          -123.2962]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 14:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.4341,  -56.7903, -170.4466,  ..., -126.2945, -127.1385,\n",
      "          -123.0969],\n",
      "         [ 193.2649,  -56.6451, -170.4266,  ..., -126.1912, -127.0800,\n",
      "          -123.0401],\n",
      "         [ 193.1360,  -56.5232, -170.3966,  ..., -126.1185, -127.0423,\n",
      "          -123.0108]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 15:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.2649,  -56.6451, -170.4266,  ..., -126.1912, -127.0800,\n",
      "          -123.0401],\n",
      "         [ 193.1360,  -56.5232, -170.3966,  ..., -126.1185, -127.0423,\n",
      "          -123.0108],\n",
      "         [ 193.0458,  -56.4090, -170.3597,  ..., -126.0751, -127.0257,\n",
      "          -123.0070]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 16:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 193.1360,  -56.5232, -170.3966,  ..., -126.1185, -127.0423,\n",
      "          -123.0108],\n",
      "         [ 193.0458,  -56.4090, -170.3597,  ..., -126.0751, -127.0257,\n",
      "          -123.0070],\n",
      "         [ 192.9812,  -56.2972, -170.3190,  ..., -126.0520, -127.0229,\n",
      "          -123.0189]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 17:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 193.0218,  -56.3167, -170.4580,  ..., -126.2782, -127.2208,\n",
      "          -123.2428],\n",
      "         [ 192.9626,  -56.2203, -170.4108,  ..., -126.2491, -127.2063,\n",
      "          -123.2388],\n",
      "         [ 192.9135,  -56.1336, -170.3620,  ..., -126.2259, -127.1945,\n",
      "          -123.2366]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 18:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.9812,  -56.2972, -170.3190,  ..., -126.0520, -127.0229,\n",
      "          -123.0189],\n",
      "         [ 192.9284,  -56.1895, -170.2762,  ..., -126.0373, -127.0233,\n",
      "          -123.0339],\n",
      "         [ 192.8779,  -56.0884, -170.2324,  ..., -126.0204, -127.0180,\n",
      "          -123.0418]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 19:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.9135,  -56.1336, -170.3620,  ..., -126.2259, -127.1945,\n",
      "          -123.2366],\n",
      "         [ 192.8653,  -56.0560, -170.3121,  ..., -126.1980, -127.1762,\n",
      "          -123.2267],\n",
      "         [ 192.8126,  -55.9856, -170.2615,  ..., -126.1586, -127.1460,\n",
      "          -123.2036]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 20:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.8653,  -56.0560, -170.3121,  ..., -126.1980, -127.1762,\n",
      "          -123.2267],\n",
      "         [ 192.8126,  -55.9856, -170.2615,  ..., -126.1586, -127.1460,\n",
      "          -123.2036],\n",
      "         [ 192.7514,  -55.9200, -170.2105,  ..., -126.1057, -127.1020,\n",
      "          -123.1655]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 21:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.8229,  -55.9942, -170.1877,  ..., -125.9939, -127.0006,\n",
      "          -123.0359],\n",
      "         [ 192.7585,  -55.9049, -170.1426,  ..., -125.9548, -126.9690,\n",
      "          -123.0138],\n",
      "         [ 192.6827,  -55.8185, -170.0975,  ..., -125.9040, -126.9245,\n",
      "          -122.9771]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 22:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.7514,  -55.9200, -170.2105,  ..., -126.1057, -127.1020,\n",
      "          -123.1655],\n",
      "         [ 192.6793,  -55.8565, -170.1597,  ..., -126.0405, -127.0452,\n",
      "          -123.1137],\n",
      "         [ 192.5954,  -55.7936, -170.1098,  ..., -125.9667, -126.9788,\n",
      "          -123.0514]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 23:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.6827,  -55.8185, -170.0975,  ..., -125.9040, -126.9245,\n",
      "          -122.9771],\n",
      "         [ 192.5961,  -55.7346, -170.0531,  ..., -125.8452, -126.8706,\n",
      "          -122.9297],\n",
      "         [ 192.5014,  -55.6541, -170.0099,  ..., -125.7819, -126.8116,\n",
      "          -122.8758]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 24:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.5954,  -55.7936, -170.1098,  ..., -125.9667, -126.9788,\n",
      "          -123.0514],\n",
      "         [ 192.5013,  -55.7311, -170.0615,  ..., -125.8884, -126.9069,\n",
      "          -122.9830],\n",
      "         [ 192.4002,  -55.6701, -170.0153,  ..., -125.8096, -126.8333,\n",
      "          -122.9125]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 25:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.5014,  -55.6541, -170.0099,  ..., -125.7819, -126.8116,\n",
      "          -122.8758],\n",
      "         [ 192.4017,  -55.5783, -169.9687,  ..., -125.7172, -126.7506,\n",
      "          -122.8191],\n",
      "         [ 192.2996,  -55.5086, -169.9298,  ..., -125.6532, -126.6899,\n",
      "          -122.7619]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 26:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.4017,  -55.5783, -169.9687,  ..., -125.7172, -126.7506,\n",
      "          -122.8191],\n",
      "         [ 192.2996,  -55.5086, -169.9298,  ..., -125.6532, -126.6899,\n",
      "          -122.7619],\n",
      "         [ 192.1968,  -55.4461, -169.8931,  ..., -125.5910, -126.6309,\n",
      "          -122.7059]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 27:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.2996,  -55.5086, -169.9298,  ..., -125.6532, -126.6899,\n",
      "          -122.7619],\n",
      "         [ 192.1968,  -55.4461, -169.8931,  ..., -125.5910, -126.6309,\n",
      "          -122.7059],\n",
      "         [ 192.0944,  -55.3910, -169.8586,  ..., -125.5312, -126.5744,\n",
      "          -122.6521]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 28:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 192.1892,  -55.5580, -169.9310,  ..., -125.6602, -126.6921,\n",
      "          -122.7764],\n",
      "         [ 192.0836,  -55.5092, -169.8930,  ..., -125.5920, -126.6273,\n",
      "          -122.7139],\n",
      "         [ 191.9798,  -55.4662, -169.8575,  ..., -125.5285, -126.5670,\n",
      "          -122.6559]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 29:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 192.0944,  -55.3910, -169.8586,  ..., -125.5312, -126.5744,\n",
      "          -122.6521],\n",
      "         [ 191.9934,  -55.3435, -169.8262,  ..., -125.4742, -126.5207,\n",
      "          -122.6007],\n",
      "         [ 191.8948,  -55.3033, -169.7955,  ..., -125.4203, -126.4700,\n",
      "          -122.5523]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 30:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.9798,  -55.4662, -169.8575,  ..., -125.5285, -126.5670,\n",
      "          -122.6559],\n",
      "         [ 191.8785,  -55.4289, -169.8243,  ..., -125.4696, -126.5114,\n",
      "          -122.6023],\n",
      "         [ 191.7805,  -55.3972, -169.7931,  ..., -125.4152, -126.4600,\n",
      "          -122.5532]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 31:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.8948,  -55.3033, -169.7955,  ..., -125.4203, -126.4700,\n",
      "          -122.5523],\n",
      "         [ 191.7993,  -55.2699, -169.7664,  ..., -125.3694, -126.4223,\n",
      "          -122.5068],\n",
      "         [ 191.7075,  -55.2429, -169.7387,  ..., -125.3216, -126.3777,\n",
      "          -122.4643]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 32:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.7805,  -55.3972, -169.7931,  ..., -125.4152, -126.4600,\n",
      "          -122.5532],\n",
      "         [ 191.6862,  -55.3707, -169.7637,  ..., -125.3649, -126.4128,\n",
      "          -122.5081],\n",
      "         [ 191.5961,  -55.3491, -169.7357,  ..., -125.3184, -126.3692,\n",
      "          -122.4668]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 33:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.7075,  -55.2429, -169.7387,  ..., -125.3216, -126.3777,\n",
      "          -122.4643],\n",
      "         [ 191.6199,  -55.2216, -169.7124,  ..., -125.2768, -126.3359,\n",
      "          -122.4247],\n",
      "         [ 191.5368,  -55.2053, -169.6874,  ..., -125.2350, -126.2970,\n",
      "          -122.3880]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 34:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.6199,  -55.2216, -169.7124,  ..., -125.2768, -126.3359,\n",
      "          -122.4247],\n",
      "         [ 191.5368,  -55.2053, -169.6874,  ..., -125.2350, -126.2970,\n",
      "          -122.3880],\n",
      "         [ 191.4583,  -55.1935, -169.6638,  ..., -125.1962, -126.2609,\n",
      "          -122.3542]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 35:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.5368,  -55.2053, -169.6874,  ..., -125.2350, -126.2970,\n",
      "          -122.3880],\n",
      "         [ 191.4583,  -55.1935, -169.6638,  ..., -125.1962, -126.2609,\n",
      "          -122.3542],\n",
      "         [ 191.3847,  -55.1854, -169.6416,  ..., -125.1602, -126.2275,\n",
      "          -122.3232]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 36:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.4583,  -55.1935, -169.6638,  ..., -125.1962, -126.2609,\n",
      "          -122.3542],\n",
      "         [ 191.3847,  -55.1854, -169.6416,  ..., -125.1602, -126.2275,\n",
      "          -122.3232],\n",
      "         [ 191.3158,  -55.1806, -169.6210,  ..., -125.1271, -126.1969,\n",
      "          -122.2950]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 37:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.3847,  -55.1854, -169.6416,  ..., -125.1602, -126.2275,\n",
      "          -122.3232],\n",
      "         [ 191.3158,  -55.1806, -169.6210,  ..., -125.1271, -126.1969,\n",
      "          -122.2950],\n",
      "         [ 191.2518,  -55.1783, -169.6020,  ..., -125.0969, -126.1689,\n",
      "          -122.2696]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 38:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.2805,  -55.3010, -169.6384,  ..., -125.1659, -126.2269,\n",
      "          -122.3346],\n",
      "         [ 191.2132,  -55.2961, -169.6179,  ..., -125.1353, -126.1985,\n",
      "          -122.3089],\n",
      "         [ 191.1506,  -55.2932, -169.5991,  ..., -125.1076, -126.1729,\n",
      "          -122.2859]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 39:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.2518,  -55.1783, -169.6020,  ..., -125.0969, -126.1689,\n",
      "          -122.2696],\n",
      "         [ 191.1923,  -55.1781, -169.5848,  ..., -125.0695, -126.1437,\n",
      "          -122.2469],\n",
      "         [ 191.1373,  -55.1795, -169.5694,  ..., -125.0450, -126.1212,\n",
      "          -122.2268]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 40:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 191.1506,  -55.2932, -169.5991,  ..., -125.1076, -126.1729,\n",
      "          -122.2859],\n",
      "         [ 191.0924,  -55.2918, -169.5821,  ..., -125.0827, -126.1500,\n",
      "          -122.2656],\n",
      "         [ 191.0386,  -55.2917, -169.5668,  ..., -125.0606, -126.1298,\n",
      "          -122.2478]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 41:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.1373,  -55.1795, -169.5694,  ..., -125.0450, -126.1212,\n",
      "          -122.2268],\n",
      "         [ 191.0866,  -55.1821, -169.5557,  ..., -125.0232, -126.1013,\n",
      "          -122.2093],\n",
      "         [ 191.0399,  -55.1857, -169.5439,  ..., -125.0041, -126.0840,\n",
      "          -122.1944]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 42:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.0866,  -55.1821, -169.5557,  ..., -125.0232, -126.1013,\n",
      "          -122.2093],\n",
      "         [ 191.0399,  -55.1857, -169.5439,  ..., -125.0041, -126.0840,\n",
      "          -122.1944],\n",
      "         [ 190.9970,  -55.1899, -169.5338,  ..., -124.9876, -126.0693,\n",
      "          -122.1819]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 43:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 191.0399,  -55.1857, -169.5439,  ..., -125.0041, -126.0840,\n",
      "          -122.1944],\n",
      "         [ 190.9970,  -55.1899, -169.5338,  ..., -124.9876, -126.0693,\n",
      "          -122.1819],\n",
      "         [ 190.9575,  -55.1946, -169.5253,  ..., -124.9735, -126.0569,\n",
      "          -122.1716]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 44:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 190.9432,  -55.2939, -169.5418,  ..., -125.0242, -126.0970,\n",
      "          -122.2197],\n",
      "         [ 190.9012,  -55.2959, -169.5319,  ..., -125.0096, -126.0843,\n",
      "          -122.2090],\n",
      "         [ 190.8626,  -55.2983, -169.5236,  ..., -124.9974, -126.0738,\n",
      "          -122.2004]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 45:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 190.9012,  -55.2959, -169.5319,  ..., -125.0096, -126.0843,\n",
      "          -122.2090],\n",
      "         [ 190.8626,  -55.2983, -169.5236,  ..., -124.9974, -126.0738,\n",
      "          -122.2004],\n",
      "         [ 190.8271,  -55.3010, -169.5167,  ..., -124.9872, -126.0653,\n",
      "          -122.1938]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 46:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 190.8626,  -55.2983, -169.5236,  ..., -124.9974, -126.0738,\n",
      "          -122.2004],\n",
      "         [ 190.8271,  -55.3010, -169.5167,  ..., -124.9872, -126.0653,\n",
      "          -122.1938],\n",
      "         [ 190.7946,  -55.3039, -169.5112,  ..., -124.9789, -126.0587,\n",
      "          -122.1889]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 47:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 190.8879,  -55.2046, -169.5129,  ..., -124.9521, -126.0387,\n",
      "          -122.1574],\n",
      "         [ 190.8572,  -55.2099, -169.5088,  ..., -124.9444, -126.0325,\n",
      "          -122.1530],\n",
      "         [ 190.8288,  -55.2152, -169.5057,  ..., -124.9385, -126.0280,\n",
      "          -122.1502]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 48:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 190.7715,  -59.1863, -166.5074,  ..., -123.3333, -124.0132,\n",
      "          -120.2488],\n",
      "         ...,\n",
      "         [ 190.8572,  -55.2099, -169.5088,  ..., -124.9444, -126.0325,\n",
      "          -122.1530],\n",
      "         [ 190.8288,  -55.2152, -169.5057,  ..., -124.9385, -126.0280,\n",
      "          -122.1502],\n",
      "         [ 190.8027,  -55.2205, -169.5037,  ..., -124.9341, -126.0251,\n",
      "          -122.1489]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 49:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 188.9534,  -57.9873, -166.1927,  ..., -119.9333, -120.8623,\n",
      "          -116.5510],\n",
      "         [ 192.4722,  -58.6226, -167.7112,  ..., -124.1003, -124.6739,\n",
      "          -121.1076],\n",
      "         ...,\n",
      "         [ 190.7646,  -55.3070, -169.5069,  ..., -124.9724, -126.0537,\n",
      "          -122.1856],\n",
      "         [ 190.7371,  -55.3103, -169.5037,  ..., -124.9674, -126.0503,\n",
      "          -122.1837],\n",
      "         [ 190.7117,  -55.3137, -169.5014,  ..., -124.9638, -126.0483,\n",
      "          -122.1831]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Original: Что ты делаешь?\n",
      "Translation: ! glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad glad\n",
      "Step 0:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Step 1:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 2:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 3:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         [ 292.3787,  -73.3648, -240.1864,  ..., -175.9293, -182.7209,\n",
      "          -175.6470]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 4:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         [ 292.9980,  -72.8019, -240.4743,  ..., -176.4452, -182.9280,\n",
      "          -175.9448],\n",
      "         [ 291.7451,  -73.7555, -241.6272,  ..., -175.8741, -182.5232,\n",
      "          -175.3526]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 5:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         [ 292.3787,  -73.3648, -240.1864,  ..., -175.9293, -182.7209,\n",
      "          -175.6470],\n",
      "         [ 292.4127,  -72.1491, -241.3275,  ..., -176.8064, -183.5142,\n",
      "          -176.2904],\n",
      "         [ 292.8626,  -71.5243, -242.0047,  ..., -177.4413, -184.2446,\n",
      "          -176.8254]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 6:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 292.4127,  -72.1491, -241.3275,  ..., -176.8064, -183.5142,\n",
      "          -176.2904],\n",
      "         [ 292.8626,  -71.5243, -242.0047,  ..., -177.4413, -184.2446,\n",
      "          -176.8254],\n",
      "         [ 292.9226,  -71.2555, -242.2799,  ..., -177.4516, -184.2491,\n",
      "          -176.7764]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 7:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 293.2303,  -71.9982, -242.1274,  ..., -177.6399, -184.3871,\n",
      "          -177.0132],\n",
      "         [ 293.1151,  -71.3129, -242.4697,  ..., -177.8027, -184.5818,\n",
      "          -177.0983],\n",
      "         [ 292.8956,  -71.1979, -242.6154,  ..., -177.7059, -184.4850,\n",
      "          -176.9730]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 8:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 293.1151,  -71.3129, -242.4697,  ..., -177.8027, -184.5818,\n",
      "          -177.0983],\n",
      "         [ 292.8956,  -71.1979, -242.6154,  ..., -177.7059, -184.4850,\n",
      "          -176.9730],\n",
      "         [ 292.9666,  -71.1519, -242.7042,  ..., -177.4751, -184.2484,\n",
      "          -176.7185]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 9:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 293.1231,  -71.1369, -242.4772,  ..., -177.3586, -184.1680,\n",
      "          -176.6665],\n",
      "         [ 293.2722,  -71.1606, -242.5671,  ..., -177.1399, -183.9468,\n",
      "          -176.4328],\n",
      "         [ 293.5550,  -71.2010, -242.6650,  ..., -176.9196, -183.7174,\n",
      "          -176.2070]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 10:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 293.2722,  -71.1606, -242.5671,  ..., -177.1399, -183.9468,\n",
      "          -176.4328],\n",
      "         [ 293.5550,  -71.2010, -242.6650,  ..., -176.9196, -183.7174,\n",
      "          -176.2070],\n",
      "         [ 294.0605,  -71.1750, -242.7801,  ..., -176.7126, -183.4950,\n",
      "          -176.0110]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 11:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 293.9468,  -71.9269, -242.6785,  ..., -176.8784, -183.6207,\n",
      "          -176.0799],\n",
      "         [ 294.5895,  -71.8693, -242.8350,  ..., -176.7331, -183.4617,\n",
      "          -175.9460],\n",
      "         [ 295.2943,  -71.7510, -242.9775,  ..., -176.5801, -183.2888,\n",
      "          -175.8204]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 12:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 294.5895,  -71.8693, -242.8350,  ..., -176.7331, -183.4617,\n",
      "          -175.9460],\n",
      "         [ 295.2943,  -71.7510, -242.9775,  ..., -176.5801, -183.2888,\n",
      "          -175.8204],\n",
      "         [ 295.9349,  -71.6034, -243.0620,  ..., -176.4050, -183.0899,\n",
      "          -175.6740]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 13:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 294.7086,  -71.0838, -242.8692,  ..., -176.4978, -183.2591,\n",
      "          -175.8190],\n",
      "         [ 295.3542,  -70.9508, -242.8989,  ..., -176.2751, -183.0105,\n",
      "          -175.6179],\n",
      "         [ 295.9103,  -70.8074, -242.8926,  ..., -176.0814, -182.7884,\n",
      "          -175.4388]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 14:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 295.4371,  -71.5410, -243.0395,  ..., -176.4143, -183.1325,\n",
      "          -175.7317],\n",
      "         [ 295.9858,  -71.3619, -243.0365,  ..., -176.2117, -182.9007,\n",
      "          -175.5488],\n",
      "         [ 296.4050,  -71.1922, -243.0187,  ..., -176.0479, -182.7093,\n",
      "          -175.3951]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 15:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 295.5176,  -70.7481, -242.9863,  ..., -176.2433, -182.9250,\n",
      "          -175.5945],\n",
      "         [ 296.0041,  -70.5988, -242.9485,  ..., -176.0619, -182.7154,\n",
      "          -175.4230],\n",
      "         [ 296.3945,  -70.4704, -242.9262,  ..., -175.9317, -182.5579,\n",
      "          -175.2963]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 16:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 296.8014,  -71.3048, -243.0867,  ..., -176.0759, -182.7134,\n",
      "          -175.3787],\n",
      "         [ 297.0559,  -71.1698, -243.0797,  ..., -175.9586, -182.5761,\n",
      "          -175.2676],\n",
      "         [ 297.2373,  -71.0452, -243.0772,  ..., -175.8723, -182.4726,\n",
      "          -175.1839]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 17:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 296.7153,  -71.0411, -243.0061,  ..., -175.9272, -182.5641,\n",
      "          -175.2779],\n",
      "         [ 296.9456,  -70.9100, -243.0046,  ..., -175.8428, -182.4591,\n",
      "          -175.1938],\n",
      "         [ 297.1212,  -70.7959, -243.0125,  ..., -175.7858, -182.3849,\n",
      "          -175.1355]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 18:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 296.9308,  -70.4681, -242.8838,  ..., -175.7444, -182.3701,\n",
      "          -175.1137],\n",
      "         [ 297.1187,  -70.3874, -242.8986,  ..., -175.6871, -182.2905,\n",
      "          -175.0554],\n",
      "         [ 297.2657,  -70.3178, -242.9194,  ..., -175.6480, -182.2316,\n",
      "          -175.0135]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 19:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.1187,  -70.3874, -242.8986,  ..., -175.6871, -182.2905,\n",
      "          -175.0554],\n",
      "         [ 297.2657,  -70.3178, -242.9194,  ..., -175.6480, -182.2316,\n",
      "          -175.0135],\n",
      "         [ 297.3841,  -70.2571, -242.9449,  ..., -175.6235, -182.1892,\n",
      "          -174.9839]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 20:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 296.9994,  -70.1337, -243.0064,  ..., -175.7316, -182.2616,\n",
      "          -175.0915],\n",
      "         [ 297.1530,  -70.0785, -243.0257,  ..., -175.7075, -182.2240,\n",
      "          -175.0649],\n",
      "         [ 297.2797,  -70.0333, -243.0531,  ..., -175.6985, -182.2025,\n",
      "          -175.0508]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 21:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.3841,  -70.2571, -242.9449,  ..., -175.6235, -182.1892,\n",
      "          -174.9839],\n",
      "         [ 297.4815,  -70.2041, -242.9740,  ..., -175.6098, -182.1592,\n",
      "          -174.9628],\n",
      "         [ 297.5629,  -70.1576, -243.0056,  ..., -175.6035, -182.1374,\n",
      "          -174.9469]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 22:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.4815,  -70.2041, -242.9740,  ..., -175.6098, -182.1592,\n",
      "          -174.9628],\n",
      "         [ 297.5629,  -70.1576, -243.0056,  ..., -175.6035, -182.1374,\n",
      "          -174.9469],\n",
      "         [ 297.6317,  -70.1166, -243.0385,  ..., -175.6011, -182.1203,\n",
      "          -174.9337]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 23:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.5373,  -70.4559, -243.0986,  ..., -175.7052, -182.2524,\n",
      "          -175.0319],\n",
      "         [ 297.6005,  -70.3930, -243.1281,  ..., -175.7016, -182.2372,\n",
      "          -175.0181],\n",
      "         [ 297.6537,  -70.3371, -243.1583,  ..., -175.6990, -182.2230,\n",
      "          -175.0045]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 24:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.4772,  -69.9632, -243.1214,  ..., -175.7054, -182.1859,\n",
      "          -175.0407],\n",
      "         [ 297.5560,  -69.9353, -243.1577,  ..., -175.7134, -182.1825,\n",
      "          -175.0380],\n",
      "         [ 297.6251,  -69.9108, -243.1930,  ..., -175.7208, -182.1786,\n",
      "          -175.0343]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 25:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.5560,  -69.9353, -243.1577,  ..., -175.7134, -182.1825,\n",
      "          -175.0380],\n",
      "         [ 297.6251,  -69.9108, -243.1930,  ..., -175.7208, -182.1786,\n",
      "          -175.0343],\n",
      "         [ 297.6864,  -69.8887, -243.2263,  ..., -175.7262, -182.1730,\n",
      "          -175.0286]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 26:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.6873,  -69.9777, -243.1731,  ..., -175.7216, -182.1981,\n",
      "          -175.0290],\n",
      "         [ 297.7429,  -69.9526, -243.2077,  ..., -175.7243, -182.1883,\n",
      "          -175.0204],\n",
      "         [ 297.7924,  -69.9295, -243.2393,  ..., -175.7242, -182.1763,\n",
      "          -175.0097]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 27:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.7429,  -69.9526, -243.2077,  ..., -175.7243, -182.1883,\n",
      "          -175.0204],\n",
      "         [ 297.7924,  -69.9295, -243.2393,  ..., -175.7242, -182.1763,\n",
      "          -175.0097],\n",
      "         [ 297.8367,  -69.9076, -243.2678,  ..., -175.7214, -182.1622,\n",
      "          -174.9970]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 28:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.7734,  -70.2026, -243.2424,  ..., -175.6842, -182.1741,\n",
      "          -174.9559],\n",
      "         [ 297.8043,  -70.1659, -243.2662,  ..., -175.6750, -182.1541,\n",
      "          -174.9364],\n",
      "         [ 297.8322,  -70.1322, -243.2876,  ..., -175.6638, -182.1322,\n",
      "          -174.9156]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 29:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.8043,  -70.1659, -243.2662,  ..., -175.6750, -182.1541,\n",
      "          -174.9364],\n",
      "         [ 297.8322,  -70.1322, -243.2876,  ..., -175.6638, -182.1322,\n",
      "          -174.9156],\n",
      "         [ 297.8577,  -70.1009, -243.3068,  ..., -175.6507, -182.1088,\n",
      "          -174.8936]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 30:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.8765,  -69.8867, -243.2934,  ..., -175.7160, -182.1460,\n",
      "          -174.9827],\n",
      "         [ 297.9123,  -69.8664, -243.3163,  ..., -175.7083, -182.1281,\n",
      "          -174.9669],\n",
      "         [ 297.9444,  -69.8466, -243.3369,  ..., -175.6986, -182.1089,\n",
      "          -174.9502]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 31:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.8577,  -70.1009, -243.3068,  ..., -175.6507, -182.1088,\n",
      "          -174.8936],\n",
      "         [ 297.8810,  -70.0716, -243.3240,  ..., -175.6361, -182.0844,\n",
      "          -174.8710],\n",
      "         [ 297.9024,  -70.0440, -243.3396,  ..., -175.6202, -182.0593,\n",
      "          -174.8479]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 32:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.9444,  -69.8466, -243.3369,  ..., -175.6986, -182.1089,\n",
      "          -174.9502],\n",
      "         [ 297.9734,  -69.8273, -243.3556,  ..., -175.6873, -182.0887,\n",
      "          -174.9328],\n",
      "         [ 297.9993,  -69.8086, -243.3727,  ..., -175.6749, -182.0680,\n",
      "          -174.9152]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 33:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 297.9640,  -69.8619, -243.2816,  ..., -175.5383, -181.9546,\n",
      "          -174.7903],\n",
      "         [ 297.9836,  -69.8391, -243.3011,  ..., -175.5239, -181.9320,\n",
      "          -174.7713],\n",
      "         [ 298.0009,  -69.8170, -243.3194,  ..., -175.5088, -181.9094,\n",
      "          -174.7524]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 34:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.9221,  -70.0178, -243.3539,  ..., -175.6035, -182.0338,\n",
      "          -174.8248],\n",
      "         [ 297.9402,  -69.9930, -243.3672,  ..., -175.5862, -182.0082,\n",
      "          -174.8019],\n",
      "         [ 297.9566,  -69.9694, -243.3798,  ..., -175.5686, -181.9829,\n",
      "          -174.7793]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 35:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.0009,  -69.8170, -243.3194,  ..., -175.5088, -181.9094,\n",
      "          -174.7524],\n",
      "         [ 298.0164,  -69.7960, -243.3367,  ..., -175.4933, -181.8871,\n",
      "          -174.7339],\n",
      "         [ 298.0301,  -69.7760, -243.3531,  ..., -175.4778, -181.8653,\n",
      "          -174.7158]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 36:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.0432,  -69.7729, -243.4036,  ..., -175.6476, -182.0261,\n",
      "          -174.8798],\n",
      "         [ 298.0617,  -69.7562, -243.4177,  ..., -175.6332, -182.0055,\n",
      "          -174.8625],\n",
      "         [ 298.0781,  -69.7403, -243.4312,  ..., -175.6189, -181.9853,\n",
      "          -174.8457]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 37:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.9787,  -69.9788, -243.3648,  ..., -175.4347, -181.8433,\n",
      "          -174.6360],\n",
      "         [ 297.9883,  -69.9552, -243.3762,  ..., -175.4141, -181.8169,\n",
      "          -174.6129],\n",
      "         [ 297.9969,  -69.9330, -243.3874,  ..., -175.3941, -181.7914,\n",
      "          -174.5907]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 38:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 297.9883,  -69.9552, -243.3762,  ..., -175.4141, -181.8169,\n",
      "          -174.6129],\n",
      "         [ 297.9969,  -69.9330, -243.3874,  ..., -175.3941, -181.7914,\n",
      "          -174.5907],\n",
      "         [ 298.0046,  -69.9120, -243.3984,  ..., -175.3746, -181.7670,\n",
      "          -174.5695]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 39:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.0532,  -69.7392, -243.3836,  ..., -175.4469, -181.8235,\n",
      "          -174.6813],\n",
      "         [ 298.0630,  -69.7224, -243.3977,  ..., -175.4318, -181.8037,\n",
      "          -174.6649],\n",
      "         [ 298.0717,  -69.7067, -243.4112,  ..., -175.4171, -181.7847,\n",
      "          -174.6493]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 40:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 298.0088,  -69.8865, -243.4259,  ..., -175.4985, -181.8876,\n",
      "          -174.6954],\n",
      "         [ 298.0188,  -69.8685, -243.4369,  ..., -175.4817, -181.8659,\n",
      "          -174.6764],\n",
      "         [ 298.0277,  -69.8515, -243.4477,  ..., -175.4654, -181.8452,\n",
      "          -174.6583]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 41:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.0717,  -69.7067, -243.4112,  ..., -175.4171, -181.7847,\n",
      "          -174.6493],\n",
      "         [ 298.0797,  -69.6921, -243.4240,  ..., -175.4028, -181.7666,\n",
      "          -174.6343],\n",
      "         [ 298.0868,  -69.6785, -243.4362,  ..., -175.3890, -181.7493,\n",
      "          -174.6200]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 42:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.1339,  -69.6612, -243.4863,  ..., -175.5842, -181.9201,\n",
      "          -174.8065],\n",
      "         [ 298.1453,  -69.6519, -243.4972,  ..., -175.5713, -181.9039,\n",
      "          -174.7927],\n",
      "         [ 298.1555,  -69.6435, -243.5079,  ..., -175.5588, -181.8884,\n",
      "          -174.7796]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 43:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.0868,  -69.6785, -243.4362,  ..., -175.3890, -181.7493,\n",
      "          -174.6200],\n",
      "         [ 298.0934,  -69.6658, -243.4479,  ..., -175.3757, -181.7328,\n",
      "          -174.6064],\n",
      "         [ 298.0996,  -69.6541, -243.4590,  ..., -175.3629, -181.7172,\n",
      "          -174.5934]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 44:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.1440,  -69.6634, -243.5027,  ..., -175.5366, -181.8785,\n",
      "          -174.7570],\n",
      "         [ 298.1512,  -69.6536, -243.5134,  ..., -175.5243, -181.8634,\n",
      "          -174.7444],\n",
      "         [ 298.1575,  -69.6446, -243.5237,  ..., -175.5123, -181.8491,\n",
      "          -174.7324]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 45:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.0996,  -69.6541, -243.4590,  ..., -175.3629, -181.7172,\n",
      "          -174.5934],\n",
      "         [ 298.1052,  -69.6433, -243.4697,  ..., -175.3507, -181.7024,\n",
      "          -174.5811],\n",
      "         [ 298.1106,  -69.6333, -243.4799,  ..., -175.3391, -181.6884,\n",
      "          -174.5694]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 46:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 298.0545,  -69.7935, -243.4889,  ..., -175.4055, -181.7715,\n",
      "          -174.5941],\n",
      "         [ 298.0594,  -69.7812, -243.4987,  ..., -175.3920, -181.7554,\n",
      "          -174.5800],\n",
      "         [ 298.0638,  -69.7697, -243.5083,  ..., -175.3791, -181.7402,\n",
      "          -174.5666]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 47:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.7499,  -73.8559, -238.0974,  ..., -174.3518, -180.8045,\n",
      "          -174.0529],\n",
      "         ...,\n",
      "         [ 298.1632,  -69.6363, -243.5337,  ..., -175.5009, -181.8355,\n",
      "          -174.7209],\n",
      "         [ 298.1684,  -69.6286, -243.5434,  ..., -175.4899, -181.8226,\n",
      "          -174.7100],\n",
      "         [ 298.1731,  -69.6216, -243.5527,  ..., -175.4794, -181.8103,\n",
      "          -174.6997]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 48:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 298.0638,  -69.7697, -243.5083,  ..., -175.3791, -181.7402,\n",
      "          -174.5666],\n",
      "         [ 298.0676,  -69.7589, -243.5176,  ..., -175.3668, -181.7258,\n",
      "          -174.5539],\n",
      "         [ 298.0710,  -69.7489, -243.5266,  ..., -175.3550, -181.7122,\n",
      "          -174.5419]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 49:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 291.1051,  -74.5104, -237.2449,  ..., -168.7849, -175.1242,\n",
      "          -168.1669],\n",
      "         [ 291.8402,  -74.8188, -237.9127,  ..., -173.3391, -179.8802,\n",
      "          -172.9241],\n",
      "         ...,\n",
      "         [ 298.0441,  -69.7723, -243.4852,  ..., -175.2308, -181.5957,\n",
      "          -174.4205],\n",
      "         [ 298.0465,  -69.7613, -243.4934,  ..., -175.2184, -181.5816,\n",
      "          -174.4081],\n",
      "         [ 298.0487,  -69.7510, -243.5014,  ..., -175.2067, -181.5684,\n",
      "          -174.3965]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Original: Собака бегает по траве\n",
      "Translation: ! there................................................\n",
      "Step 0:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "Step 1:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 2:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 3:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         [ 311.6829,  -91.2170, -268.3598,  ..., -190.8451, -199.6570,\n",
      "          -188.9690]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 4:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         [ 311.6829,  -91.2170, -268.3598,  ..., -190.8451, -199.6570,\n",
      "          -188.9690],\n",
      "         [ 311.5793,  -90.9296, -269.3810,  ..., -190.7650, -199.5885,\n",
      "          -189.0465]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 5:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         [ 311.9724,  -91.1889, -268.8852,  ..., -190.9809, -199.7034,\n",
      "          -189.1239],\n",
      "         [ 312.2378,  -90.5309, -269.6061,  ..., -190.6677, -199.3484,\n",
      "          -188.9224],\n",
      "         [ 312.2866,  -90.1185, -270.1262,  ..., -190.6415, -199.3053,\n",
      "          -188.9014]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 6:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.2378,  -90.5309, -269.6061,  ..., -190.6677, -199.3484,\n",
      "          -188.9224],\n",
      "         [ 312.2866,  -90.1185, -270.1262,  ..., -190.6415, -199.3053,\n",
      "          -188.9014],\n",
      "         [ 312.8719,  -89.6656, -270.3955,  ..., -190.6394, -199.2431,\n",
      "          -188.8654]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 7:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.2866,  -90.1185, -270.1262,  ..., -190.6415, -199.3053,\n",
      "          -188.9014],\n",
      "         [ 312.8719,  -89.6656, -270.3955,  ..., -190.6394, -199.2431,\n",
      "          -188.8654],\n",
      "         [ 313.5025,  -89.3181, -270.6058,  ..., -190.5334, -199.1143,\n",
      "          -188.7332]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 8:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.8719,  -89.6656, -270.3955,  ..., -190.6394, -199.2431,\n",
      "          -188.8654],\n",
      "         [ 313.5025,  -89.3181, -270.6058,  ..., -190.5334, -199.1143,\n",
      "          -188.7332],\n",
      "         [ 313.5771,  -89.1708, -270.7559,  ..., -190.2770, -198.8866,\n",
      "          -188.4712]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 9:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 313.0121,  -89.4199, -270.3573,  ..., -190.6020, -199.2701,\n",
      "          -188.9177],\n",
      "         [ 313.1371,  -89.2303, -270.4902,  ..., -190.3513, -199.0317,\n",
      "          -188.6738],\n",
      "         [ 312.9776,  -89.1712, -270.5872,  ..., -190.0878, -198.7943,\n",
      "          -188.4195]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 10:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 313.5771,  -89.1708, -270.7559,  ..., -190.2770, -198.8866,\n",
      "          -188.4712],\n",
      "         [ 313.4361,  -89.1015, -270.8515,  ..., -190.0190, -198.6613,\n",
      "          -188.2189],\n",
      "         [ 313.2824,  -89.0934, -270.9257,  ..., -189.8300, -198.4793,\n",
      "          -188.0330]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 11:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.9776,  -89.1712, -270.5872,  ..., -190.0878, -198.7943,\n",
      "          -188.4195],\n",
      "         [ 312.8055,  -89.1944, -270.6739,  ..., -189.8974, -198.6068,\n",
      "          -188.2259],\n",
      "         [ 312.6305,  -89.2640, -270.7531,  ..., -189.7610, -198.4624,\n",
      "          -188.0803]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 12:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.8055,  -89.1944, -270.6739,  ..., -189.8974, -198.6068,\n",
      "          -188.2259],\n",
      "         [ 312.6305,  -89.2640, -270.7531,  ..., -189.7610, -198.4624,\n",
      "          -188.0803],\n",
      "         [ 312.4637,  -89.3491, -270.8231,  ..., -189.6723, -198.3653,\n",
      "          -187.9823]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 13:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 313.1234,  -89.1320, -270.9801,  ..., -189.6922, -198.3402,\n",
      "          -187.8982],\n",
      "         [ 312.9704,  -89.2056, -271.0215,  ..., -189.6046, -198.2511,\n",
      "          -187.8128],\n",
      "         [ 312.8353,  -89.2906, -271.0579,  ..., -189.5592, -198.2046,\n",
      "          -187.7691]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 14:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.9704,  -89.2056, -271.0215,  ..., -189.6046, -198.2511,\n",
      "          -187.8128],\n",
      "         [ 312.8353,  -89.2906, -271.0579,  ..., -189.5592, -198.2046,\n",
      "          -187.7691],\n",
      "         [ 312.7242,  -89.3684, -271.0914,  ..., -189.5389, -198.1827,\n",
      "          -187.7503]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 15:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.3178,  -89.4321, -270.8846,  ..., -189.6219, -198.3078,\n",
      "          -187.9243],\n",
      "         [ 312.1979,  -89.5047, -270.9390,  ..., -189.5951, -198.2748,\n",
      "          -187.8914],\n",
      "         [ 312.1022,  -89.5653, -270.9868,  ..., -189.5797, -198.2540,\n",
      "          -187.8711]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 16:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.7242,  -89.3684, -271.0914,  ..., -189.5389, -198.1827,\n",
      "          -187.7503],\n",
      "         [ 312.6360,  -89.4331, -271.1217,  ..., -189.5294, -198.1714,\n",
      "          -187.7421],\n",
      "         [ 312.5661,  -89.4855, -271.1486,  ..., -189.5233, -198.1635,\n",
      "          -187.7369]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 17:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.6360,  -89.4331, -271.1217,  ..., -189.5294, -198.1714,\n",
      "          -187.7421],\n",
      "         [ 312.5661,  -89.4855, -271.1486,  ..., -189.5233, -198.1635,\n",
      "          -187.7369],\n",
      "         [ 312.5098,  -89.5281, -271.1724,  ..., -189.5173, -198.1563,\n",
      "          -187.7315]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 18:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.5661,  -89.4855, -271.1486,  ..., -189.5233, -198.1635,\n",
      "          -187.7369],\n",
      "         [ 312.5098,  -89.5281, -271.1724,  ..., -189.5173, -198.1563,\n",
      "          -187.7315],\n",
      "         [ 312.4642,  -89.5629, -271.1936,  ..., -189.5105, -198.1490,\n",
      "          -187.7251]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 19:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.9670,  -89.6579, -271.0644,  ..., -189.5578, -198.2249,\n",
      "          -187.8415],\n",
      "         [ 311.9197,  -89.6936, -271.0960,  ..., -189.5465, -198.2117,\n",
      "          -187.8271],\n",
      "         [ 311.8824,  -89.7240, -271.1238,  ..., -189.5342, -198.1984,\n",
      "          -187.8119]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 20:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.4642,  -89.5629, -271.1936,  ..., -189.5105, -198.1490,\n",
      "          -187.7251],\n",
      "         [ 312.4272,  -89.5912, -271.2129,  ..., -189.5027, -198.1411,\n",
      "          -187.7174],\n",
      "         [ 312.3974,  -89.6141, -271.2307,  ..., -189.4939, -198.1327,\n",
      "          -187.7085]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 21:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.4272,  -89.5912, -271.2129,  ..., -189.5027, -198.1411,\n",
      "          -187.7174],\n",
      "         [ 312.3974,  -89.6141, -271.2307,  ..., -189.4939, -198.1327,\n",
      "          -187.7085],\n",
      "         [ 312.3741,  -89.6323, -271.2473,  ..., -189.4844, -198.1239,\n",
      "          -187.6986]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 22:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8537,  -89.7498, -271.1488,  ..., -189.5209, -198.1847,\n",
      "          -187.7959],\n",
      "         [ 311.8322,  -89.7714, -271.1715,  ..., -189.5069, -198.1709,\n",
      "          -187.7794],\n",
      "         [ 311.8173,  -89.7892, -271.1926,  ..., -189.4925, -198.1569,\n",
      "          -187.7625]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 23:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8322,  -89.7714, -271.1715,  ..., -189.5069, -198.1709,\n",
      "          -187.7794],\n",
      "         [ 311.8173,  -89.7892, -271.1926,  ..., -189.4925, -198.1569,\n",
      "          -187.7625],\n",
      "         [ 311.8081,  -89.8035, -271.2125,  ..., -189.4781, -198.1430,\n",
      "          -187.7455]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 24:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.3564,  -89.6463, -271.2631,  ..., -189.4745, -198.1148,\n",
      "          -187.6881],\n",
      "         [ 312.3439,  -89.6566, -271.2784,  ..., -189.4644, -198.1056,\n",
      "          -187.6771],\n",
      "         [ 312.3358,  -89.6636, -271.2933,  ..., -189.4543, -198.0964,\n",
      "          -187.6658]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 25:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8081,  -89.8035, -271.2125,  ..., -189.4781, -198.1430,\n",
      "          -187.7455],\n",
      "         [ 311.8041,  -89.8143, -271.2313,  ..., -189.4639, -198.1292,\n",
      "          -187.7285],\n",
      "         [ 311.8047,  -89.8220, -271.2493,  ..., -189.4500, -198.1158,\n",
      "          -187.7118]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 26:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8041,  -89.8143, -271.2313,  ..., -189.4639, -198.1292,\n",
      "          -187.7285],\n",
      "         [ 311.8047,  -89.8220, -271.2493,  ..., -189.4500, -198.1158,\n",
      "          -187.7118],\n",
      "         [ 311.8092,  -89.8267, -271.2668,  ..., -189.4366, -198.1028,\n",
      "          -187.6956]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 27:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.3319,  -89.6675, -271.3079,  ..., -189.4443, -198.0872,\n",
      "          -187.6546],\n",
      "         [ 312.3315,  -89.6686, -271.3223,  ..., -189.4346, -198.0782,\n",
      "          -187.6433],\n",
      "         [ 312.3342,  -89.6670, -271.3366,  ..., -189.4252, -198.0695,\n",
      "          -187.6323]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 28:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8092,  -89.8267, -271.2668,  ..., -189.4366, -198.1028,\n",
      "          -187.6956],\n",
      "         [ 311.8175,  -89.8285, -271.2837,  ..., -189.4238, -198.0904,\n",
      "          -187.6798],\n",
      "         [ 311.8288,  -89.8276, -271.3001,  ..., -189.4116, -198.0786,\n",
      "          -187.6645]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 29:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8175,  -89.8285, -271.2837,  ..., -189.4238, -198.0904,\n",
      "          -187.6798],\n",
      "         [ 311.8288,  -89.8276, -271.3001,  ..., -189.4116, -198.0786,\n",
      "          -187.6645],\n",
      "         [ 311.8428,  -89.8241, -271.3162,  ..., -189.4002, -198.0673,\n",
      "          -187.6499]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 30:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8288,  -89.8276, -271.3001,  ..., -189.4116, -198.0786,\n",
      "          -187.6645],\n",
      "         [ 311.8428,  -89.8241, -271.3162,  ..., -189.4002, -198.0673,\n",
      "          -187.6499],\n",
      "         [ 311.8593,  -89.8182, -271.3319,  ..., -189.3895, -198.0566,\n",
      "          -187.6359]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 31:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.3478,  -89.6566, -271.3647,  ..., -189.4077, -198.0527,\n",
      "          -187.6109],\n",
      "         [ 312.3579,  -89.6481, -271.3786,  ..., -189.3995, -198.0447,\n",
      "          -187.6007],\n",
      "         [ 312.3699,  -89.6376, -271.3923,  ..., -189.3918, -198.0370,\n",
      "          -187.5908]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 32:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8593,  -89.8182, -271.3319,  ..., -189.3895, -198.0566,\n",
      "          -187.6359],\n",
      "         [ 311.8778,  -89.8100, -271.3472,  ..., -189.3794, -198.0465,\n",
      "          -187.6225],\n",
      "         [ 311.8981,  -89.7996, -271.3622,  ..., -189.3701, -198.0370,\n",
      "          -187.6097]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 33:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8778,  -89.8100, -271.3472,  ..., -189.3794, -198.0465,\n",
      "          -187.6225],\n",
      "         [ 311.8981,  -89.7996, -271.3622,  ..., -189.3701, -198.0370,\n",
      "          -187.6097],\n",
      "         [ 311.9200,  -89.7872, -271.3768,  ..., -189.3613, -198.0280,\n",
      "          -187.5974]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 34:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.8981,  -89.7996, -271.3622,  ..., -189.3701, -198.0370,\n",
      "          -187.6097],\n",
      "         [ 311.9200,  -89.7872, -271.3768,  ..., -189.3613, -198.0280,\n",
      "          -187.5974],\n",
      "         [ 311.9432,  -89.7729, -271.3911,  ..., -189.3531, -198.0195,\n",
      "          -187.5857]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 35:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.3986,  -89.6110, -271.4193,  ..., -189.3775, -198.0224,\n",
      "          -187.5719],\n",
      "         [ 312.4148,  -89.5954, -271.4325,  ..., -189.3709, -198.0154,\n",
      "          -187.5629],\n",
      "         [ 312.4320,  -89.5782, -271.4454,  ..., -189.3646, -198.0087,\n",
      "          -187.5542]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 36:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.4148,  -89.5954, -271.4325,  ..., -189.3709, -198.0154,\n",
      "          -187.5629],\n",
      "         [ 312.4320,  -89.5782, -271.4454,  ..., -189.3646, -198.0087,\n",
      "          -187.5542],\n",
      "         [ 312.4502,  -89.5598, -271.4581,  ..., -189.3586, -198.0022,\n",
      "          -187.5457]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 37:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.4320,  -89.5782, -271.4454,  ..., -189.3646, -198.0087,\n",
      "          -187.5542],\n",
      "         [ 312.4502,  -89.5598, -271.4581,  ..., -189.3586, -198.0022,\n",
      "          -187.5457],\n",
      "         [ 312.4690,  -89.5402, -271.4706,  ..., -189.3529, -197.9959,\n",
      "          -187.5375]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 38:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 311.9931,  -89.7390, -271.4186,  ..., -189.3384, -198.0040,\n",
      "          -187.5638],\n",
      "         [ 312.0193,  -89.7198, -271.4319,  ..., -189.3317, -197.9968,\n",
      "          -187.5535],\n",
      "         [ 312.0464,  -89.6991, -271.4449,  ..., -189.3255, -197.9901,\n",
      "          -187.5436]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 39:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.4690,  -89.5402, -271.4706,  ..., -189.3529, -197.9959,\n",
      "          -187.5375],\n",
      "         [ 312.4885,  -89.5196, -271.4829,  ..., -189.3475, -197.9898,\n",
      "          -187.5295],\n",
      "         [ 312.5085,  -89.4980, -271.4948,  ..., -189.3423, -197.9840,\n",
      "          -187.5218]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 40:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.0464,  -89.6991, -271.4449,  ..., -189.3255, -197.9901,\n",
      "          -187.5436],\n",
      "         [ 312.0739,  -89.6770, -271.4576,  ..., -189.3196, -197.9836,\n",
      "          -187.5341],\n",
      "         [ 312.1021,  -89.6538, -271.4700,  ..., -189.3141, -197.9775,\n",
      "          -187.5250]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 41:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.0739,  -89.6770, -271.4576,  ..., -189.3196, -197.9836,\n",
      "          -187.5341],\n",
      "         [ 312.1021,  -89.6538, -271.4700,  ..., -189.3141, -197.9775,\n",
      "          -187.5250],\n",
      "         [ 312.1306,  -89.6295, -271.4822,  ..., -189.3090, -197.9717,\n",
      "          -187.5162]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 42:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.1021,  -89.6538, -271.4700,  ..., -189.3141, -197.9775,\n",
      "          -187.5250],\n",
      "         [ 312.1306,  -89.6295, -271.4822,  ..., -189.3090, -197.9717,\n",
      "          -187.5162],\n",
      "         [ 312.1596,  -89.6041, -271.4941,  ..., -189.3041, -197.9661,\n",
      "          -187.5076]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 43:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.5495,  -89.4527, -271.5180,  ..., -189.3325, -197.9727,\n",
      "          -187.5070],\n",
      "         [ 312.5704,  -89.4291, -271.5292,  ..., -189.3278, -197.9673,\n",
      "          -187.4999],\n",
      "         [ 312.5914,  -89.4050, -271.5401,  ..., -189.3234, -197.9620,\n",
      "          -187.4930]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 44:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.5704,  -89.4291, -271.5292,  ..., -189.3278, -197.9673,\n",
      "          -187.4999],\n",
      "         [ 312.5914,  -89.4050, -271.5401,  ..., -189.3234, -197.9620,\n",
      "          -187.4930],\n",
      "         [ 312.6126,  -89.3805, -271.5508,  ..., -189.3190, -197.9569,\n",
      "          -187.4863]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 45:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.5914,  -89.4050, -271.5401,  ..., -189.3234, -197.9620,\n",
      "          -187.4930],\n",
      "         [ 312.6126,  -89.3805, -271.5508,  ..., -189.3190, -197.9569,\n",
      "          -187.4863],\n",
      "         [ 312.6337,  -89.3557, -271.5612,  ..., -189.3149, -197.9519,\n",
      "          -187.4797]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 46:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.2180,  -89.5509, -271.5173,  ..., -189.2952, -197.9557,\n",
      "          -187.4915],\n",
      "         [ 312.2475,  -89.5232, -271.5285,  ..., -189.2910, -197.9508,\n",
      "          -187.4838],\n",
      "         [ 312.2769,  -89.4949, -271.5396,  ..., -189.2872, -197.9461,\n",
      "          -187.4763]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 47:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.2475,  -89.5232, -271.5285,  ..., -189.2910, -197.9508,\n",
      "          -187.4838],\n",
      "         [ 312.2769,  -89.4949, -271.5396,  ..., -189.2872, -197.9461,\n",
      "          -187.4763],\n",
      "         [ 312.3063,  -89.4662, -271.5504,  ..., -189.2835, -197.9416,\n",
      "          -187.4692]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 48:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.3150,  -89.9639, -265.7579,  ..., -190.4266, -199.3746,\n",
      "          -188.5738],\n",
      "         ...,\n",
      "         [ 312.2769,  -89.4949, -271.5396,  ..., -189.2872, -197.9461,\n",
      "          -187.4763],\n",
      "         [ 312.3063,  -89.4662, -271.5504,  ..., -189.2835, -197.9416,\n",
      "          -187.4692],\n",
      "         [ 312.3356,  -89.4371, -271.5611,  ..., -189.2800, -197.9373,\n",
      "          -187.4622]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Step 49:\n",
      "Attention Weights: None\n",
      "Outputs: tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [ 308.6549,  -89.1192, -264.4236,  ..., -184.7745, -193.6636,\n",
      "          -182.8016],\n",
      "         [ 310.9576,  -91.6797, -267.0257,  ..., -190.9006, -199.6145,\n",
      "          -188.9787],\n",
      "         ...,\n",
      "         [ 312.6760,  -89.3054, -271.5812,  ..., -189.3068, -197.9423,\n",
      "          -187.4671],\n",
      "         [ 312.6970,  -89.2801, -271.5909,  ..., -189.3030, -197.9376,\n",
      "          -187.4611],\n",
      "         [ 312.7178,  -89.2548, -271.6004,  ..., -189.2993, -197.9331,\n",
      "          -187.4552]]], device='cuda:0', grad_fn=<CopySlices>)\n",
      "Original: Небо и море - все синее.\n",
      "Translation: ! should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should should\n"
     ]
    }
   ],
   "source": [
    "# 对每个文本进行翻译并打印结果 - Переводим каждый текст и выводим результат\n",
    "for text in sample_texts:\n",
    "    translation = translateAtt(text, tokenizer, seq2seq_attention_model)  # 使用模型进行翻译 - Переводим текст с помощью модели\n",
    "    print(f\"Original: {text}\")  # 原始文本 - Оригинальный текст\n",
    "    print(f\"Translation: {translation}\")  # 翻译文本 - Перевод текста"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
